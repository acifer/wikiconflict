{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "\n",
    "\n",
    "change_vector_dir = \"../data/change_vector_optimised/\"\n",
    "change_vec_filename = f\"{article_name}_comp_vec.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "    \n",
    "# change_object_dataframe[\"edited_tokens\"] = change_object_dataframe[\"ins_tokens\"]  + change_object_dataframe[\"del_tokens\"]\n",
    "# rev_len_df = pd.read_hdf(len_file_path, key = \"rev_len\")\n",
    "vectors ={}\n",
    "\n",
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    vectors[\"clean_4\"] = arrays_dict[\"4_clean_not_weighted\"]\n",
    "    vectors[\"clean_10\"] = arrays_dict[\"10_clean_not_weighted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_dir = \"../data/annotation/\"\n",
    "annotation_file_name = f\"{article_name}.csv\"\n",
    "annotation_file_path = os.path.join(annotation_dir, annotation_file_name)\n",
    "\n",
    "annotation_df = pd.read_csv(annotation_file_path)\n",
    "annotation_df.columns =[\"context_id\", \"token_id\", \"rev_id\", \"cluster_4\", \"cluster_10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.73 s, sys: 1.83 s, total: 11.6 s\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "vector_names = [\"clean_4\", \"clean_10\"]\n",
    "cluster_df = pd.DataFrame(columns=vector_names)\n",
    "# cluster_by= vector_names[0]\n",
    "dbscan_param = { \"eps\": 1.5, \"min_samples\": 4 }\n",
    "for cluster_by in vector_names:\n",
    "    distances = pairwise_distances(vectors[cluster_by])\n",
    "    cluster_df[cluster_by] = DBSCAN(**dbscan_param, metric=\"precomputed\").fit(distances).labels_\n",
    "cluster_df.index = change_object_dataframe.index\n",
    "change_object_dataframe[vector_names] = cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = \"../data/clusters/\"\n",
    "\n",
    "file_name = article_name + \"_dbscan_cluster_4and10.h5\"\n",
    "full_file_path = os.path.join(cluster_dir, file_name)\n",
    "with pd.HDFStore(full_file_path, 'w') as store:\n",
    "    store.put(\"cluster\", change_object_dataframe[[\"clean_4\", \"clean_10\"]], table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id',\"clean_4\", \"clean_10\"]].values\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id',\"clean_4\", \"clean_10\"]].values\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=annotation_df.columns)\n",
    "\n",
    "gap_df = gap_df.set_index(['context_id', 'rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap):\n",
    "    context_gap = gap.loc[ann[['context_id', 'rev_id']]]\n",
    "    clusters = context_gap.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x), [\"cluster_4\",\"cluster_10\"]].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=[\"cluster_4\",\"cluster_10\"])\n",
    "    else:\n",
    "        clusters = pd.Series([-10,-10], index=[\"cluster_4\",\"cluster_10\"])\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tokens who were in the gap.\n",
    "annotation_df[[\"cluster_4\",\"cluster_10\"]] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = f\"{article_name}_evaluation.csv\"\n",
    "result_file_path = os.path.join(annotation_dir, result_file_name)\n",
    "annotation_df.to_csv(result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1279\n",
       "1.0     496\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_name = article_name + \"_FULL.csv\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "full_df = pd.read_csv(full_file_path)\n",
    "true_lable_df = full_df[[\"nationality\", \"birth_place\"]]\n",
    "true_lable_df.head()\n",
    "\n",
    "true_labels = np.zeros((true_lable_df.shape[0]))\n",
    "true_labels[true_lable_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n",
    "full_df['true_labels']=true_labels\n",
    "pd.Series(true_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[(\n",
    "#                             true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | \n",
    "#                             (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[ (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "                  \"cluster_10\"].unique())\n",
    "\n",
    "annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "annotation_df.loc[annotation_df['cluster_10'] != -999,\"cluster_10\"] = 999\n",
    "\n",
    "nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[\n",
    "                            (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999\n",
    "#annotation_df.loc[~np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =999\n",
    "annotation_df.loc[annotation_df['cluster_4'] != -999,\"cluster_4\"] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "# _tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]\n",
    "# #_tdf['cluster_4_x'] - _tdf['cluster_10_y']\n",
    "# #_tdf.shape\n",
    "# #_tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "#_tdf = _tdf[_tdf['Bulk'].str.strip() == 'N']\n",
    "_tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand_4            0.306269\n",
       "rand_10           0.377451\n",
       "mutual_info_4     0.178438\n",
       "mutual_info_10    0.241638\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "evaluation_score[\"rand_4\"] = adjusted_rand_score( _tdf[\"cluster_4\"], _tdf['true_labels'])\n",
    "evaluation_score[\"rand_10\"] = adjusted_rand_score( _tdf[\"cluster_10\"], _tdf['true_labels'])\n",
    "evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(_tdf['true_labels'], \n",
    "                                            _tdf[\"cluster_4\"], average_method=\"max\"  )\n",
    "evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(_tdf['true_labels'],\n",
    "                                            _tdf[\"cluster_10\"], average_method=\"max\" )\n",
    "evaluation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rand_4            0.306269\n",
       "rand_10           0.377451\n",
       "mutual_info_4     0.178438\n",
       "mutual_info_10    0.241638\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "evaluation_score[\"rand_4\"] = adjusted_rand_score( annotation_df[\"cluster_4\"], true_labels)\n",
    "evaluation_score[\"rand_10\"] = adjusted_rand_score( annotation_df[\"cluster_10\"], true_labels)\n",
    "evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(true_labels, \n",
    "                                            annotation_df[\"cluster_4\"], average_method=\"max\"  )\n",
    "evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(true_labels,\n",
    "                                            annotation_df[\"cluster_10\"], average_method=\"max\" )\n",
    "evaluation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17923641305186078"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_mutual_info_score(true_labels, annotation_df[\"cluster_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"birth_place\"].str.strip().values == \"Y\") , \n",
    "                  \"cluster_10\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df.loc[annotation_df[\"cluster_10\"] == -1,\"cluster_10\"] =-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
