{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    token_string_df = store.get(\"all_tokens\")\n",
    "    \n",
    "token_string_df = token_string_df.set_index(\"token_id\")[\"str\"]\n",
    "token_string_df[-1] = \"St@rt\"\n",
    "token_string_df[-2] = \"$nd\"\n",
    "change_vector_dir = \"../data/change_vector/\"\n",
    "change_vec_filename = f\"{article_name}.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "change_object_dataframe[\"del_string_tokens\"] = change_object_dataframe[\"del_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"ins_string_tokens\"] = change_object_dataframe[\"ins_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"edit_string_tokens\"] = change_object_dataframe[\"ins_string_tokens\"] + change_object_dataframe[\"del_string_tokens\"]\n",
    "\n",
    "\n",
    "# rev_len_df = pd.read_hdf(len_file_path, key = \"rev_len\")\n",
    "vectors ={}\n",
    "\n",
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    vectors[2] = arrays_dict[\"2_clean_not_weighted\"]\n",
    "    vectors[4] = arrays_dict[\"4_clean_not_weighted\"]\n",
    "    vectors[6] = arrays_dict[\"6_clean_not_weighted\"]\n",
    "    vectors[8] = arrays_dict[\"8_clean_not_weighted\"]\n",
    "    vectors[10] = arrays_dict[\"10_clean_not_weighted\"]\n",
    "    vectors[12] = arrays_dict[\"12_clean_not_weighted\"]\n",
    "    vectors[15] = arrays_dict[\"15_clean_not_weighted\"]\n",
    "    vectors[20] = arrays_dict[\"20_clean_not_weighted\"]\n",
    "    vectors[25] = arrays_dict[\"25_clean_not_weighted\"]\n",
    "    vectors[30] = arrays_dict[\"30_clean_not_weighted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = article_name + \"_FULL.csv\"\n",
    "annotation_dir = \"../data/annotation/\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "annotation_df = pd.read_csv(full_file_path)\n",
    "annotation_df = annotation_df[[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\", \"nationality\", \"birth_place\", \"Bulk\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_names = list(vectors.keys())\n",
    "context_array  = vector_names\n",
    "eps_array = [0.00000001, 0.01, 0.1, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0,2.5,3,4]\n",
    "\n",
    "min_samples_array = [2]\n",
    "all_combinations = list(itertools.product(context_array, eps_array,\n",
    "                                          min_samples_array))\n",
    "dbscan_params = list(itertools.product(eps_array,min_samples_array))\n",
    "idx = pd.MultiIndex.from_product([context_array, eps_array,min_samples_array],\n",
    "                                names=[\"context\",\"eps\",\"min_samples\"])\n",
    "cluster_df = pd.DataFrame(columns=idx)\n",
    "\n",
    "evaluation_df = pd.DataFrame(index=idx, columns=[\"rand\", \"entropy\", \"token_entropy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 1min 42s, total: 4min 18s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for cluster_by in vector_names:\n",
    "    distances = pairwise_distances(vectors[cluster_by])\n",
    "    for eps, min_samples in dbscan_params:\n",
    "        cluster_df[cluster_by,eps, min_samples] = DBSCAN(eps=eps, min_samples=min_samples, \n",
    "                                                         metric=\"precomputed\").fit(distances).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.index = change_object_dataframe.index\n",
    "dbscan_results = pd.concat([change_object_dataframe, cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 29s, sys: 132 ms, total: 2min 29s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# entropy_series = pd.Series(index=all_combinations)\n",
    "\n",
    "for context, eps, min_samples in all_combinations:\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"token_entropy\"] = weighted_token_entropy(dbscan_results, (context, eps, min_samples))\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cluster_dir = \"../data/clusters/\"\n",
    "\n",
    "# file_name = article_name + \"_dbscan_cluster_4and10.h5\"\n",
    "# full_file_path = os.path.join(cluster_dir, file_name)\n",
    "# with pd.HDFStore(full_file_path, 'w') as store:\n",
    "#     store.put(\"cluster\", change_object_dataframe[[\"clean_4\", \"clean_10\"]], table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change_object_dataframe[[\"ins_tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting change object to match annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap_df, gap_cluster_df):\n",
    "    context_gap = gap_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    context_cluster = gap_cluster_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    clusters = context_cluster.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x),:].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=gap_cluster_df.columns)\n",
    "    else:\n",
    "        clusters = pd.Series(-10, index=gap_cluster_df.columns)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\", ):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.zeros((annotation_df.shape[0]))\n",
    "true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gap_df.copy()\n",
    "df2=df2.reset_index()\n",
    "df2['nationality'] = 0\n",
    "\n",
    "\n",
    "aci = annotation_clusters.set_index(['revid_ctxt', 'rev_id']).sort_index()\n",
    "aci = aci[aci['Bulk'] =='N']\n",
    "aci_y=aci[aci['nationality'] == 'Y']\n",
    "aci_n=aci[aci['nationality'] == 'N']\n",
    "\n",
    "counter = 0\n",
    "def nat_val(row):\n",
    "    global counter\n",
    "    val = 0\n",
    "    x=0\n",
    "    y=0\n",
    "    try:\n",
    "        x = int(aci_y.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val + (1 if x > 0 else 0)\n",
    "        \n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    try:\n",
    "        y = int(aci_n.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val - (1 if y > 0 else 0)\n",
    "        counter += y\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "#     if (x + y) > 1:\n",
    "#         print((aci.loc[(row[0],row[1]),['token_id','nationality','Bulk']])[aci.loc[(row[0],row[1]), 'token_id'].isin(row[2])])\n",
    "#         print(aci.loc[(row[0],row[1]), ['token_id']].isin(row[2]))\n",
    "        \n",
    "#     if (1 if x > 0 else 0) + (1 if y > 0 else 0) > 1:\n",
    "#         try:\n",
    "#             if not (aci.loc[(row[0],row[1]),['Bulk']] == 'Y').all()[0] or True:\n",
    "#                 print((aci.loc[(row[0],row[1]),['token_id','nationality','Bulk']])[aci.loc[(row[0],row[1]), 'token_id'].isin(row[2])])\n",
    "#                 print(aci.loc[(row[0],row[1]), ['token_id']].isin(row[2]))\n",
    "#         except:\n",
    "#             import pdb; pdb.set_trace()\n",
    "    return val \n",
    "    \n",
    "df2['nationality'] = df2[['revid_ctxt', 'rev_id', 'token_id']].apply(nat_val, axis=1)\n",
    "df3 = pd.concat([df2, gap_cluster_df.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    6241\n",
       " 1     405\n",
       "-1     272\n",
       "Name: nationality, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without bulks\n",
      "(2, 1e-08, 2): 1.9867810900482359\n",
      "(2, 0.01, 2): 1.4858458222106017\n",
      "(2, 0.1, 2): 1.4858458222106017\n",
      "(2, 0.25, 2): 1.4858458222106017\n",
      "(2, 0.5, 2): 1.4858458222106017\n",
      "(2, 0.75, 2): 1.5071841634990952\n",
      "(2, 1.0, 2): 1.513538453097494\n",
      "(2, 1.25, 2): 1.5109643272732556\n",
      "(2, 1.5, 2): 1.5046704314783486\n",
      "(2, 1.75, 2): 1.5034484230771907\n",
      "(2, 2.0, 2): 1.5452693225419276\n",
      "(2, 2.5, 2): 1.7931284839353576\n",
      "(2, 3, 2): 2.6117212958577536\n",
      "(2, 4, 2): 7.789703046152093\n",
      "(4, 1e-08, 2): 1.4214091160218247\n",
      "(4, 0.01, 2): 1.0357292537540328\n",
      "(4, 0.1, 2): 1.0357292537540328\n",
      "(4, 0.25, 2): 1.0357292537540328\n",
      "(4, 0.5, 2): 1.0357292537540328\n",
      "(4, 0.75, 2): 1.0369956392785165\n",
      "(4, 1.0, 2): 1.0395016797096868\n",
      "(4, 1.25, 2): 1.0259857221530067\n",
      "(4, 1.5, 2): 0.995497621989483\n",
      "(4, 1.75, 2): 0.9976439747823428\n",
      "(4, 2.0, 2): 1.0233821584226985\n",
      "(4, 2.5, 2): 1.5667937904280422\n",
      "(4, 3, 2): 2.928653680845209\n",
      "(4, 4, 2): 18.964253900823497\n",
      "(6, 1e-08, 2): 1.428937887669013\n",
      "(6, 0.01, 2): 1.153708824190206\n",
      "(6, 0.1, 2): 1.153708824190206\n",
      "(6, 0.25, 2): 1.153708824190206\n",
      "(6, 0.5, 2): 1.156583986113914\n",
      "(6, 0.75, 2): 1.1493720706044588\n",
      "(6, 1.0, 2): 1.1173793202187854\n",
      "(6, 1.25, 2): 1.0702422685750637\n",
      "(6, 1.5, 2): 1.0495052094944133\n",
      "(6, 1.75, 2): 1.029547347087571\n",
      "(6, 2.0, 2): 1.105644331290468\n",
      "(6, 2.5, 2): 1.9265064705977162\n",
      "(6, 3, 2): 6.228721810122258\n",
      "(6, 4, 2): 50.626431072063085\n",
      "(8, 1e-08, 2): 1.4056552988419573\n",
      "(8, 0.01, 2): 1.2379160207002295\n",
      "(8, 0.1, 2): 1.2379160207002295\n",
      "(8, 0.25, 2): 1.2362535054454542\n",
      "(8, 0.5, 2): 1.2269541740641585\n",
      "(8, 0.75, 2): 1.1714167805966074\n",
      "(8, 1.0, 2): 1.0769187046291169\n",
      "(8, 1.25, 2): 1.0277106708645192\n",
      "(8, 1.5, 2): 1.0066984191703992\n",
      "(8, 1.75, 2): 1.045007861376431\n",
      "(8, 2.0, 2): 1.218972760879676\n",
      "(8, 2.5, 2): 4.056887215698153\n",
      "(8, 3, 2): 14.645545536671198\n",
      "(8, 4, 2): 98.42210776937625\n",
      "(10, 1e-08, 2): 1.5972384914358568\n",
      "(10, 0.01, 2): 1.3196058370381805\n",
      "(10, 0.1, 2): 1.3196058370381805\n",
      "(10, 0.25, 2): 1.3196058370381805\n",
      "(10, 0.5, 2): 1.2974281443302975\n",
      "(10, 0.75, 2): 1.2002923218751251\n",
      "(10, 1.0, 2): 1.0892457334372259\n",
      "(10, 1.25, 2): 1.0261267995440344\n",
      "(10, 1.5, 2): 1.1205490555809041\n",
      "(10, 1.75, 2): 1.3056196260851647\n",
      "(10, 2.0, 2): 2.0025072459866182\n",
      "(10, 2.5, 2): 8.022870367083337\n",
      "(10, 3, 2): 27.159319700815633\n",
      "(10, 4, 2): 166.50905402468734\n",
      "(12, 1e-08, 2): 1.6112947365356285\n",
      "(12, 0.01, 2): 1.3975421914862982\n",
      "(12, 0.1, 2): 1.3975421914862982\n",
      "(12, 0.25, 2): 1.3975421914862982\n",
      "(12, 0.5, 2): 1.332111094027642\n",
      "(12, 0.75, 2): 1.1841178749456536\n",
      "(12, 1.0, 2): 1.0859240124662999\n",
      "(12, 1.25, 2): 1.0805786993782789\n",
      "(12, 1.5, 2): 1.2266706685152324\n",
      "(12, 1.75, 2): 1.718393152047237\n",
      "(12, 2.0, 2): 2.955688683359748\n",
      "(12, 2.5, 2): 13.270847387260593\n",
      "(12, 3, 2): 31.95755884634342\n",
      "(12, 4, 2): 166.51669183987832\n",
      "(15, 1e-08, 2): 1.6847229838570836\n",
      "(15, 0.01, 2): 1.46559056078231\n",
      "(15, 0.1, 2): 1.46559056078231\n",
      "(15, 0.25, 2): 1.465378594312048\n",
      "(15, 0.5, 2): 1.3039436453558537\n",
      "(15, 0.75, 2): 1.122421350347863\n",
      "(15, 1.0, 2): 1.0542990890029529\n",
      "(15, 1.25, 2): 1.1153040527849274\n",
      "(15, 1.5, 2): 1.4620116808834236\n",
      "(15, 1.75, 2): 2.969500099817077\n",
      "(15, 2.0, 2): 5.997220781903144\n",
      "(15, 2.5, 2): 20.709763715260433\n",
      "(15, 3, 2): 64.50409765184226\n",
      "(15, 4, 2): 222.2209157664698\n",
      "(20, 1e-08, 2): 1.75034024552684\n",
      "(20, 0.01, 2): 1.4960231732092586\n",
      "(20, 0.1, 2): 1.4960231732092586\n",
      "(20, 0.25, 2): 1.4745062344054964\n",
      "(20, 0.5, 2): 1.1927022805056378\n",
      "(20, 0.75, 2): 1.0530991867646557\n",
      "(20, 1.0, 2): 1.2091657406018146\n",
      "(20, 1.25, 2): 1.608714266447204\n",
      "(20, 1.5, 2): 2.7855557779061204\n",
      "(20, 1.75, 2): 7.145760754097522\n",
      "(20, 2.0, 2): 15.777077212040883\n",
      "(20, 2.5, 2): 52.76179366709302\n",
      "(20, 3, 2): 106.22718860728104\n",
      "(20, 4, 2): 381.12801633384197\n",
      "(25, 1e-08, 2): 1.7541605664267834\n",
      "(25, 0.01, 2): 1.4945612788539926\n",
      "(25, 0.1, 2): 1.4945612788539926\n",
      "(25, 0.25, 2): 1.436801861556504\n",
      "(25, 0.5, 2): 1.1278103160208002\n",
      "(25, 0.75, 2): 1.1743803864140765\n",
      "(25, 1.0, 2): 1.4673719252962112\n",
      "(25, 1.25, 2): 2.1815224388501955\n",
      "(25, 1.5, 2): 5.32822343346747\n",
      "(25, 1.75, 2): 12.439844467692211\n",
      "(25, 2.0, 2): 35.05552608789583\n",
      "(25, 2.5, 2): 120.76638469716731\n",
      "(25, 3, 2): 204.79037784784393\n",
      "(25, 4, 2): 667.6981309109401\n",
      "(30, 1e-08, 2): 1.8279383538241094\n",
      "(30, 0.01, 2): 1.5841109457394862\n",
      "(30, 0.1, 2): 1.5865261097274561\n",
      "(30, 0.25, 2): 1.4953400758331168\n",
      "(30, 0.5, 2): 1.1146822824333942\n",
      "(30, 0.75, 2): 1.2132146566065956\n",
      "(30, 1.0, 2): 1.7582217797285702\n",
      "(30, 1.25, 2): 3.5584232728874956\n",
      "(30, 1.5, 2): 9.124166162063183\n",
      "(30, 1.75, 2): 26.58321899949543\n",
      "(30, 2.0, 2): 73.09161377120802\n",
      "(30, 2.5, 2): 266.3956279276707\n",
      "(30, 3, 2): 380.5934059337384\n",
      "(30, 4, 2): 890.5048973944099\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "print(\"Without bulks\")\n",
    "for context, eps, min_samples in all_combinations:\n",
    "    print(str((context, eps, min_samples)) + \": \" + str(weighted_entropy(df3, entropy_column=\"nationality\", group_columns=(context, eps, min_samples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bulks\n",
      "(2, 1e-08, 2): 2.772904646693351\n",
      "(2, 0.01, 2): 2.3561279561235495\n",
      "(2, 0.1, 2): 2.3561279561235495\n",
      "(2, 0.25, 2): 2.3561279561235495\n",
      "(2, 0.5, 2): 2.3561279561235495\n",
      "(2, 0.75, 2): 2.397156331522571\n",
      "(2, 1.0, 2): 2.410324952222437\n",
      "(2, 1.25, 2): 2.40610784786207\n",
      "(2, 1.5, 2): 2.3927716261593166\n",
      "(2, 1.75, 2): 2.3916542088137605\n",
      "(2, 2.0, 2): 2.4558325274589827\n",
      "(2, 2.5, 2): 2.885547099996353\n",
      "(2, 3, 2): 3.999872247057809\n",
      "(2, 4, 2): 10.895400517397643\n",
      "(4, 1e-08, 2): 2.050943681386586\n",
      "(4, 0.01, 2): 1.6598129684962035\n",
      "(4, 0.1, 2): 1.6598129684962035\n",
      "(4, 0.25, 2): 1.6598129684962035\n",
      "(4, 0.5, 2): 1.6598129684962035\n",
      "(4, 0.75, 2): 1.6623432804183367\n",
      "(4, 1.0, 2): 1.6673229471947226\n",
      "(4, 1.25, 2): 1.640818986225281\n",
      "(4, 1.5, 2): 1.601013180672444\n",
      "(4, 1.75, 2): 1.6068964144409135\n",
      "(4, 2.0, 2): 1.6329443722131844\n",
      "(4, 2.5, 2): 2.396619406205798\n",
      "(4, 3, 2): 4.415250558394471\n",
      "(4, 4, 2): 24.430395840846845\n",
      "(6, 1e-08, 2): 2.09705170742411\n",
      "(6, 0.01, 2): 1.7726451369657767\n",
      "(6, 0.1, 2): 1.7726451369657767\n",
      "(6, 0.25, 2): 1.7726451369657767\n",
      "(6, 0.5, 2): 1.7774026479012255\n",
      "(6, 0.75, 2): 1.7657953938769737\n",
      "(6, 1.0, 2): 1.7209050629297882\n",
      "(6, 1.25, 2): 1.6470616530994655\n",
      "(6, 1.5, 2): 1.6223994623880886\n",
      "(6, 1.75, 2): 1.6209636849274882\n",
      "(6, 2.0, 2): 1.734160691375407\n",
      "(6, 2.5, 2): 2.8927365771955715\n",
      "(6, 3, 2): 8.46057830272948\n",
      "(6, 4, 2): 64.71021221111245\n",
      "(8, 1e-08, 2): 2.0885794199700243\n",
      "(8, 0.01, 2): 1.8715324948559688\n",
      "(8, 0.1, 2): 1.8715324948559688\n",
      "(8, 0.25, 2): 1.8689632315216116\n",
      "(8, 0.5, 2): 1.8582786223543406\n",
      "(8, 0.75, 2): 1.784088938832231\n",
      "(8, 1.0, 2): 1.6564475379718984\n",
      "(8, 1.25, 2): 1.6026616924610124\n",
      "(8, 1.5, 2): 1.5732598722392632\n",
      "(8, 1.75, 2): 1.6186936954775362\n",
      "(8, 2.0, 2): 1.8923044177700719\n",
      "(8, 2.5, 2): 5.5957873088941605\n",
      "(8, 3, 2): 18.67566244338229\n",
      "(8, 4, 2): 125.76159402012456\n",
      "(10, 1e-08, 2): 2.289079430534011\n",
      "(10, 0.01, 2): 1.9677482947318037\n",
      "(10, 0.1, 2): 1.9677482947318037\n",
      "(10, 0.25, 2): 1.9677482947318037\n",
      "(10, 0.5, 2): 1.9375192634075382\n",
      "(10, 0.75, 2): 1.7963084351764178\n",
      "(10, 1.0, 2): 1.6457955330257807\n",
      "(10, 1.25, 2): 1.581683368356047\n",
      "(10, 1.5, 2): 1.7230530798351824\n",
      "(10, 1.75, 2): 1.9614164761881148\n",
      "(10, 2.0, 2): 2.8153546644680674\n",
      "(10, 2.5, 2): 10.281489814931476\n",
      "(10, 3, 2): 34.702921203332714\n",
      "(10, 4, 2): 213.26215072648316\n",
      "(12, 1e-08, 2): 2.323363752922525\n",
      "(12, 0.01, 2): 2.046664046142785\n",
      "(12, 0.1, 2): 2.046664046142785\n",
      "(12, 0.25, 2): 2.046664046142785\n",
      "(12, 0.5, 2): 1.9630823132508302\n",
      "(12, 0.75, 2): 1.7417233322942614\n",
      "(12, 1.0, 2): 1.6418660968830299\n",
      "(12, 1.25, 2): 1.6563132204073574\n",
      "(12, 1.5, 2): 1.8672538871421873\n",
      "(12, 1.75, 2): 2.500240520059344\n",
      "(12, 2.0, 2): 4.261217002252952\n",
      "(12, 2.5, 2): 16.961786068572813\n",
      "(12, 3, 2): 40.955615031252876\n",
      "(12, 4, 2): 213.18801094612144\n",
      "(15, 1e-08, 2): 2.34330325332989\n",
      "(15, 0.01, 2): 2.110210365297943\n",
      "(15, 0.1, 2): 2.110210365297943\n",
      "(15, 0.25, 2): 2.111633966348122\n",
      "(15, 0.5, 2): 1.906733693899792\n",
      "(15, 0.75, 2): 1.6805816076820215\n",
      "(15, 1.0, 2): 1.62820538474331\n",
      "(15, 1.25, 2): 1.7323618330633088\n",
      "(15, 1.5, 2): 2.2250234156708606\n",
      "(15, 1.75, 2): 4.09741809661444\n",
      "(15, 2.0, 2): 7.9349919662305455\n",
      "(15, 2.5, 2): 26.310702158623382\n",
      "(15, 3, 2): 82.54421697555192\n",
      "(15, 4, 2): 284.612263987119\n",
      "(20, 1e-08, 2): 2.4431406742639727\n",
      "(20, 0.01, 2): 2.172622035061081\n",
      "(20, 0.1, 2): 2.172622035061081\n",
      "(20, 0.25, 2): 2.1442733795424083\n",
      "(20, 0.5, 2): 1.7868080449883923\n",
      "(20, 0.75, 2): 1.594220135238983\n",
      "(20, 1.0, 2): 1.7894313002404836\n",
      "(20, 1.25, 2): 2.3455869237432037\n",
      "(20, 1.5, 2): 3.8940632568092144\n",
      "(20, 1.75, 2): 9.377415872269877\n",
      "(20, 2.0, 2): 20.046560691393683\n",
      "(20, 2.5, 2): 67.44045957976826\n",
      "(20, 3, 2): 135.82535233341443\n",
      "(20, 4, 2): 488.0994255904364\n",
      "(25, 1e-08, 2): 2.424986218977858\n",
      "(25, 0.01, 2): 2.143026627201526\n",
      "(25, 0.1, 2): 2.143026627201526\n",
      "(25, 0.25, 2): 2.0710351043729816\n",
      "(25, 0.5, 2): 1.6845676073337656\n",
      "(25, 0.75, 2): 1.7430535883232299\n",
      "(25, 1.0, 2): 2.13094368586798\n",
      "(25, 1.25, 2): 3.1208359257799785\n",
      "(25, 1.5, 2): 7.233962814456049\n",
      "(25, 1.75, 2): 16.445971903866088\n",
      "(25, 2.0, 2): 44.81081775521425\n",
      "(25, 2.5, 2): 154.71515003632217\n",
      "(25, 3, 2): 262.26753520238674\n",
      "(25, 4, 2): 855.4790970387788\n",
      "(30, 1e-08, 2): 2.506060548830933\n",
      "(30, 0.01, 2): 2.2464437439796745\n",
      "(30, 0.1, 2): 2.2495077506939785\n",
      "(30, 0.25, 2): 2.1208810885609886\n",
      "(30, 0.5, 2): 1.6814581738350287\n",
      "(30, 0.75, 2): 1.788957408438554\n",
      "(30, 1.0, 2): 2.514027031178606\n",
      "(30, 1.25, 2): 4.953325458472039\n",
      "(30, 1.5, 2): 12.3400067463799\n",
      "(30, 1.75, 2): 34.005166360104774\n",
      "(30, 2.0, 2): 93.5132189683366\n",
      "(30, 2.5, 2): 341.31804499585587\n",
      "(30, 3, 2): 487.7647845021714\n",
      "(30, 4, 2): 1140.8306607263098\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "print(\"With bulks\")\n",
    "for context, eps, min_samples in all_combinations:\n",
    "    print(str((context, eps, min_samples)) + \": \" + str(weighted_entropy(df3, entropy_column=\"nationality\", group_columns=(context, eps, min_samples))))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eps   context\n",
       "2.00  4          0.97293\n",
       "1.75  4          0.97415\n",
       "1.50  4          1.05774\n",
       "1.00  4           1.0903\n",
       "1.25  4           1.0903\n",
       "0.75  4           1.0903\n",
       "0.50  4           1.0903\n",
       "0.25  4           1.0903\n",
       "0.10  4           1.0903\n",
       "0.01  4           1.0903\n",
       "1.25  15         1.13612\n",
       "1.75  8           1.1664\n",
       "1.50  6          1.18681\n",
       "1.25  6          1.19074\n",
       "1.75  6          1.20398\n",
       "1.50  8          1.22739\n",
       "2.00  6          1.22824\n",
       "1.00  6          1.23382\n",
       "0.01  6          1.29177\n",
       "0.25  6          1.29177\n",
       "0.50  6          1.29177\n",
       "0.75  6          1.29177\n",
       "0.10  6          1.29177\n",
       "1.00  15         1.29467\n",
       "1.25  8           1.3073\n",
       "2.00  8           1.3122\n",
       "1.00  8          1.34942\n",
       "1.50  12         1.35512\n",
       "0.75  20         1.36965\n",
       "1.25  12         1.37586\n",
       "1.75  2          1.41378\n",
       "1.50  2          1.41378\n",
       "1.25  2          1.41378\n",
       "1.00  2          1.41378\n",
       "0.75  2          1.41378\n",
       "0.50  2          1.41378\n",
       "0.25  2          1.41378\n",
       "0.10  2          1.41378\n",
       "0.01  2          1.41378\n",
       "0.75  25         1.42645\n",
       "1.00  12         1.42676\n",
       "1.75  10         1.43122\n",
       "0.50  30         1.44962\n",
       "1.25  10          1.4499\n",
       "1.50  10         1.46569\n",
       "2.00  2          1.46573\n",
       "1.00  10         1.48208\n",
       "2.50  2          1.50373\n",
       "0.75  15         1.51017\n",
       "      8          1.54113\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = annotation_clusters[annotation_clusters['Bulk']=='N']\n",
    "for context, eps, min_samples in all_combinations:\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"entropy\"] = weighted_entropy(df4, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps, min_samples))\n",
    "evaluation_df.reset_index().set_index([\"min_samples\", \"eps\", \"context\"]).loc[2][\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y    1029\n",
       "N     746\n",
       "Name: Bulk, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps, min_samples in all_combinations:\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps, min_samples))\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps, min_samples)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eps   context\n",
       "0.10  4          1.84256\n",
       "0.25  4          1.84256\n",
       "0.50  4          1.84256\n",
       "0.75  4          1.84256\n",
       "      30         1.89924\n",
       "      20          1.9421\n",
       "      25         1.94935\n",
       "0.50  30         2.10196\n",
       "0.10  6          2.10197\n",
       "0.25  6          2.10197\n",
       "0.50  6          2.10197\n",
       "0.75  6          2.10197\n",
       "0.50  25         2.21462\n",
       "0.75  8          2.23276\n",
       "      15         2.24167\n",
       "      10         2.26895\n",
       "0.50  8          2.27903\n",
       "      20         2.29684\n",
       "0.75  12         2.31711\n",
       "0.10  8           2.3225\n",
       "0.25  8           2.3225\n",
       "      2           2.3735\n",
       "0.50  2           2.3735\n",
       "0.10  2           2.3735\n",
       "0.75  2          2.40557\n",
       "0.50  10         2.49668\n",
       "0.25  10         2.49938\n",
       "0.10  10         2.49938\n",
       "0.50  15         2.54442\n",
       "      12          2.6279\n",
       "0.25  20         2.71612\n",
       "      12         2.74786\n",
       "0.10  12         2.74786\n",
       "0.25  15         2.75932\n",
       "0.10  20         2.77461\n",
       "      15         2.77831\n",
       "0.25  25         2.81524\n",
       "      30         2.87173\n",
       "0.10  25          2.8821\n",
       "      30         2.97156\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.reset_index().set_index([\"min_samples\", \"eps\", \"context\"]).loc[2][\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df[\"entropy\"].sort_values()#.iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_combination_clusters_df.nunique(axis=0).reset_index().set_index([\"min_samples\", \"eps\", \"context\"]).loc[2].sort_values(0, ascending=False)\n",
    "#[0].sort_values().iloc[0:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_combination_clusters_df.nunique(axis=0)[evaluation_df[\"entropy\"].sort_values().index]\n",
    "# al_combination_clusters_df.values.shape\n",
    "# al_combination_clusters_df.values[0]\n",
    "# al_combination_clusters_df.head()\n",
    "al_combination_clusters_df.nunique(axis=0)[evaluation_df[\"entropy\"].sort_values().iloc[0:60].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_file_name = f\"{article_name}_evaluation.csv\"\n",
    "# result_file_path = os.path.join(annotation_dir, result_file_name)\n",
    "# annotation_df.to_csv(result_file_path)\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[(\n",
    "#                             true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | \n",
    "#                             (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[ (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "# annotation_df.loc[annotation_df['cluster_10'] != -999,\"cluster_10\"] = 999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[\n",
    "#                             (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999\n",
    "# #annotation_df.loc[~np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =999\n",
    "# annotation_df.loc[annotation_df['cluster_4'] != -999,\"cluster_4\"] = 999\n",
    "\n",
    "# _tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "# _tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]\n",
    "# #_tdf['cluster_4_x'] - _tdf['cluster_10_y']\n",
    "# #_tdf.shape\n",
    "# #_tdf\n",
    "\n",
    "# _tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "# _tdf = _tdf[_tdf['Bulk'].str.strip() == 'N']\n",
    "# _tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "# evaluation_score[\"rand_4\"] = adjusted_rand_score( _tdf[\"cluster_4\"], _tdf['true_labels'])\n",
    "# evaluation_score[\"rand_10\"] = adjusted_rand_score( _tdf[\"cluster_10\"], _tdf['true_labels'])\n",
    "# evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(_tdf['true_labels'], \n",
    "#                                             _tdf[\"cluster_4\"], average_method=\"max\"  )\n",
    "# evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(_tdf['true_labels'],\n",
    "#                                             _tdf[\"cluster_10\"], average_method=\"max\" )\n",
    "# evaluation_score\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "# evaluation_score[\"rand_4\"] = adjusted_rand_score( annotation_df[\"cluster_4\"], true_labels)\n",
    "# evaluation_score[\"rand_10\"] = adjusted_rand_score( annotation_df[\"cluster_10\"], true_labels)\n",
    "# evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(true_labels, \n",
    "#                                             annotation_df[\"cluster_4\"], average_method=\"max\"  )\n",
    "# evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(true_labels,\n",
    "#                                             annotation_df[\"cluster_10\"], average_method=\"max\" )\n",
    "# evaluation_score\n",
    "\n",
    "# normalized_mutual_info_score(true_labels, annotation_df[\"cluster_4\"])\n",
    "\n",
    "# set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"birth_place\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[annotation_df[\"cluster_10\"] == -1,\"cluster_10\"] =-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
