{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 21s, sys: 2min 17s, total: 13min 38s\n",
      "Wall time: 13min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lenght of vocabulary is 1000000 words'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lenght of vocabulary is 970137 words'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_list = list(wiki_vec.vocab)\n",
    "filtered_vocab = [ t for t in vocab_list[20:] if len(t) > 3]\n",
    "\n",
    "display(f\"lenght of vocabulary is {len(vocab_list)} words\")\n",
    "display(f\"lenght of vocabulary is {len(filtered_vocab)} words\")\n",
    "\n",
    "vocab_list = np.array(vocab_list)\n",
    "filtered_vocab = np.array(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(change_vector_dir\n",
    "# with open(\"../../wordvectors/vocabs.pkl\", \"wb\") as file:\n",
    "# #     print(file)\n",
    "#     pickle.dump(filtered_vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vecs(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[-1] == \"{st@rt}\":\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        print(tokens)\n",
    "        tokens = tokens[:-1]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "#     print(tokens_in_vocab_mask)\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "\n",
    "def weighted_average_word_vec(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[-1] == \"{st@rt}\":\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        tokens = tokens[:-1]\n",
    "    token_len = len(tokens)\n",
    "    if token_len == 0:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "    weights = (token_len-np.arange(token_len))/token_len\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    in_vocab_weight = weights[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], weights=in_vocab_weight, axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"Freddie_Mercury\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "# change_file_name = f\"{article_name}.pkl\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40276, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.7 s, sys: 1.59 s, total: 5.29 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "display(change_object_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "content_dir = \"../data/content/\"\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "unique_str = np.unique(all_rev.str)\n",
    "str_in_vocab_mask = np.isin(unique_str, vocab_list, assume_unique=True)\n",
    "str_in_filtered_vocab_mask = np.isin(unique_str, filtered_vocab, assume_unique=True)\n",
    "\n",
    "vocab_masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":str_in_vocab_mask}).set_index(\"str\")\n",
    "filtered_vocab_masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":str_in_filtered_vocab_mask}).set_index(\"str\")\n",
    "# all_rev.set_index(\"token_id\")[\"str\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vector from change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df,  token_set)).values)\n",
    "neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "# weighted vectors\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "weighted_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "weighted_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'None of [[46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23\\n 22 21 20 19 18 17 16]] are in the [index]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fb70dd804e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_word_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mright_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweighted_average_word_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnot_filtered_not_weighted_neighbour30_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mleft_neighbour_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_neighbour_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mleft_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweighted_average_word_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-fb70dd804e2e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(token_set)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_word_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mright_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweighted_average_word_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnot_filtered_not_weighted_neighbour30_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mleft_neighbour_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_neighbour_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mleft_neighbour_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_object_dataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweighted_average_word_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_masks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cae4462372d1>\u001b[0m in \u001b[0;36mget_word_vecs\u001b[0;34m(wiki_vec, masks, tokens)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mwiki_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens_in_vocab_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     print(tokens_in_vocab_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0min_vocab_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens_in_vocab_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;31m# This is an elided recursive call to iloc/loc/etc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'not applicable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1899\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                 raise KeyError(\n\u001b[1;32m   1205\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1206\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'None of [[46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23\\n 22 21 20 19 18 17 16]] are in the [index]'"
     ]
    }
   ],
   "source": [
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "not_filtered_not_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "not_filtered_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_not_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving change object vector  to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vector_dir = \"../data/change_vector_optimised/\"\n",
    "change_vec_filename = f\"{article_name}_comp_vec.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "arrays_to_save = {\n",
    "    \"4_clean_weighted\": filtered_weighted_neighbour4_matrix,\n",
    "    \"10_clean_weighted\": filtered_weighted_neighbour10_matrix,\n",
    "\n",
    "    \"4_clean_not_weighted\": filtered_neighbour4_matrix,\n",
    "    \"10_clean_not_weighted\": filtered_neighbour10_matrix,\n",
    "\n",
    "    \"4_notclean_weighted\": weighted_neighbour4_matrix,\n",
    "    \"4_notclean_not_weighted\": neighbour4_matrix,\n",
    "    \"10_notclean_weighted\": weighted_neighbour10_matrix,\n",
    "    \"10_notclean_not_weighted\": neighbour10_matrix,\n",
    "    \"not_filtered_not_weighted_neighbour30_matrix\": not_filtered_not_weighted_neighbour30_matrix,\n",
    "    \"not_filtered_weighted_neighbour30_matrix\": not_filtered_weighted_neighbour30_matrix,\n",
    "    \"filtered_weighted_neighbour30_matrix\": filtered_weighted_neighbour30_matrix,\n",
    "    \"filtered_not_weighted_neighbour30_matrix\": filtered_not_weighted_neighbour30_matrix\n",
    "}\n",
    "with open(change_vector_file, \"wb\") as file:\n",
    "    np.savez(file, **arrays_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
