{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT_gensim.load('../wordvectors/wiki-news-300d-1M-subword.vec.zip')\n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../wordvectors/wiki.en.vec', binary=False, limit=5000)\n",
    "vocab = set(wiki_vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki:\n",
    "    def __init__(self,id,title, all_tokens=[]):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.all_tokens = all_tokens\n",
    "        \n",
    "    def init_revisions(self, revisions):\n",
    "          self.revisions = pd.Series( {revision[\"id\"] : \n",
    "                                       Revision(revision[\"id\"],revision[\"timestamp\"], revision[\"editor\"]) for revision in revisions} )\n",
    "\n",
    " \n",
    "    def add_all_token(self, all_tokens):\n",
    "        for token in all_tokens:\n",
    "            self.revisions.loc[token[\"o_rev_id\"]].added.add(token[\"token_id\"])\n",
    "            for in_revision in token[\"in\"]:\n",
    "                self.revisions.loc[in_revision].added.add(token[\"token_id\"])\n",
    "            for out_revision in token[\"out\"]:\n",
    "                self.revisions.loc[out_revision].removed.add(token[\"token_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Revision:\n",
    "    def __init__(self, id, timestamp,editor):\n",
    "        self.id = id\n",
    "        self.timestamp = timestamp\n",
    "        self.editor = editor\n",
    "        self.added = set()\n",
    "        self.removed = set()   \n",
    "        \n",
    "    def deleted(self, to_rev):\n",
    "        self.content[\"removed\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(to_rev.removed), assume_unique= True ))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        start_neighbour = start_pos - 1\n",
    "        end_neighbour = end_pos + 1\n",
    "        self.deleted_object = pd.DataFrame(np.c_[ start_pos, end_pos, start_neighbour, end_neighbour ],\n",
    "                                       columns=[ \"del_start_pos\", \"del_end_pos\", \"left_neigh\", \"right_neigh\",])\n",
    "    \n",
    "    def inserted_continuous_pos(self):\n",
    "        self.content[\"added\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(self.added), assume_unique= True))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        self.added_pos = np.c_[start_pos, end_pos]\n",
    "\n",
    "    def inserted_neighbours(self):\n",
    "        start_token_pos = self.added_pos[:,0] - 1\n",
    "        end_token_pos = self.added_pos[:,1] + 1\n",
    "        self.start_token_id = self.content[\"token_id\"].values[start_token_pos]\n",
    "        self.end_token_id = self.content[\"token_id\"].values[end_token_pos]\n",
    "    \n",
    "    def create_change_object(self, to_rev):\n",
    "        self.ins_left = np.argwhere(np.isin(self.content.token_id.values, to_rev.start_token_id, assume_unique= True))\n",
    "        self.ins_right = np.argwhere(np.isin(self.content.token_id.values, to_rev.end_token_id, assume_unique= True))\n",
    "        self.inserted_object = pd.DataFrame(np.concatenate([to_rev.added_pos, self.ins_left, self.ins_right], axis=1),\n",
    "                                       columns=[\"ins_start_pos\", \"ins_end_pos\", \"left_neigh\", \"right_neigh\", ])\n",
    "\n",
    "        self.change = pd.merge(self.inserted_object, self.deleted_object,how=\"outer\", on=[\"left_neigh\", \"right_neigh\"])\n",
    "        self.change.fillna(0, inplace=True)\n",
    "        \n",
    "    def append_neighbour_vec(self, epsilon_size):\n",
    "        self.vocabs_pos = np.argwhere( self.content[\"invocab\"].values)\n",
    "        self.content_str_vec = self.content.str.values\n",
    "        del self.content\n",
    "        neighbour_df = self.change.apply(find_tokens, axis=1, args=(self, epsilon_size))\n",
    "        neighbour_df.columns= [\"ins_tokens\", \"del_tokens\", \"left_neigh\", \"right_neigh\", \"left_token\", \"right_token\"]\n",
    "        self.neighbour = neighbour_df\n",
    "        self.change_df = pd.concat([self.change, neighbour_df], sort=False, axis=1)\n",
    "        \n",
    "class Change:\n",
    "    def __init__(self, token, start, end, left_context, right_context):\n",
    "        self.token = token\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.left = left_context\n",
    "        self.right = right_context\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def find_tokens(change, revision, epsilon_size):\n",
    "    left_neigh = revision.vocabs_pos[revision.vocabs_pos <= change[\"left_neigh\"]][-epsilon_size:]\n",
    "    right_neigh = revision.vocabs_pos[revision.vocabs_pos >= change[\"right_neigh\"]][:epsilon_size]\n",
    "    ins_slice = slice(int(change[\"ins_start_pos\"]), int(change[\"ins_end_pos\"]+1) )\n",
    "    del_slice = slice(int(change[\"del_start_pos\"]), int(change[\"del_end_pos\"]+1) )\n",
    "    left_token = revision.content_str_vec[left_neigh]\n",
    "    right_token = revision.content_str_vec[right_neigh]\n",
    "    ins_tokens = revision.content_str_vec[ins_slice]\n",
    "    del_tokens = revision.content_str_vec[del_slice]\n",
    "    return pd.Series([tuple(ins_tokens), tuple(del_tokens), tuple(left_neigh), tuple(right_neigh), tuple(left_token), tuple(right_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "content = \"Violence_against_Muslims_in_India\"\n",
    "revisions_url = os.path.join( baseurl, \"rev_ids\", content+\"/\")\n",
    "params = {\"editor\": \"true\", \"timestamp\": \"true\"}\n",
    "response = requests.get(revisions_url, params= params)\n",
    "revisons_list = response.json()[\"revisions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_url = os.path.join(baseurl, \"all_content\", content +\"/\")\n",
    "params = { \"o_rev_id\": \"true\", \"editor\": \"false\", \"token_id\": \"true\", \"in\": \"true\", \"out\": \"true\" }\n",
    "all_rev_data = requests.get(all_content_url, params= params)\n",
    "all_tokens_mama = all_rev_data.json()[\"all_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.12 s, sys: 0 ns, total: 9.12 s\n",
      "Wall time: 9.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_wiki = Wiki(2345, \"a test wiki\", all_tokens=4)\n",
    "test_wiki.init_revisions(revisons_list)\n",
    "test_wiki.add_all_token(all_tokens_mama) \n",
    "epsilon_size = 6\n",
    "del all_tokens_mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(baseurl, content, start_rev_id, end_rev_id=\"\"):\n",
    "    content_url = os.path.join(baseurl, \"rev_content\", content, str(start_rev_id)+\"/\")\n",
    "    if end_rev_id:\n",
    "        content_url = os.path.join(content_url, str(end_rev_id)+\"/\")\n",
    "    params = { \"o_rev_id\": \"false\", \"editor\": \"false\", \"token_id\": \"true\", \"in\": \"false\", \"out\": \"false\" }\n",
    "    rev_contents = requests.get(content_url, params= params).json()[\"revisions\"]\n",
    "    return rev_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_change(wiki, rev_contents, from_index, vocab, epsilon_size):\n",
    "    for rev_content in rev_contents:\n",
    "        try:\n",
    "            f_rev = wiki.revisions.iloc[from_index]\n",
    "            t_rev = wiki.revisions.iloc[from_index+1]\n",
    "            from_index += 1\n",
    "            f_rev.deleted(t_rev)\n",
    "            f_rev.content[\"invocab\"] = f_rev.content[\"str\"].isin(vocab)\n",
    "            tokens = list(rev_content.values())[0][\"tokens\"]\n",
    "            tokens.insert(0, {'token_id':-1, 'str':  \"{st@rt}\"})\n",
    "            tokens.append({'token_id':-2, 'str': \"{$nd}\"})\n",
    "            t_rev.content = pd.DataFrame(tokens)\n",
    "            t_rev.inserted_continuous_pos()\n",
    "            t_rev.inserted_neighbours()\n",
    "            f_rev.create_change_object(t_rev)\n",
    "            f_rev.append_neighbour_vec(epsilon_size)\n",
    "        except:\n",
    "            print(\"exception occurred in calculating change object\",traceback.format_exc())\n",
    "            print(\"problem in \", rev_content.keys() )\n",
    "    return from_index\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran till 1\n",
      "ran till 201\n",
      "ran till 401\n",
      "ran till 601\n",
      "ran till 801\n",
      "CPU times: user 43.6 s, sys: 18.8 s, total: 1min 2s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rev_contents = get_contents(baseurl, content, str(revisons_list[0][\"id\"]), str(revisons_list[1][\"id\"]))\n",
    "epsilon_size = 6\n",
    "tokens = list(rev_contents[0].values())[0][\"tokens\"]\n",
    "tokens.insert(0, {'token_id':-1, 'str':  \"{st@rt}\"})\n",
    "tokens.append({'token_id':-2, 'str': \"{$nd}\"})\n",
    "test_wiki.revisions.iloc[0].content = pd.DataFrame(tokens)\n",
    "from_index = 0\n",
    "if len(revisons_list) > 200:\n",
    "    step = 200\n",
    "else:\n",
    "    step = len(revisons_list)\n",
    "start_index = from_index + 1\n",
    "end_index = len(revisons_list)\n",
    "for to_index in  range(start_index, end_index, step):\n",
    "    try:\n",
    "        rev_contents = get_contents(baseurl, content, str(revisons_list[(from_index+1)][\"id\"]), str(revisons_list[to_index][\"id\"]))\n",
    "        create_change(test_wiki, rev_contents, from_index, vocab, epsilon_size)\n",
    "        print(\"ran till\", to_index)\n",
    "        from_index = to_index - 1\n",
    "    except:\n",
    "        print(\"problem \", from_index)\n",
    "to_index = from_index + (end_index-1)%step\n",
    "rev_contents = get_contents(baseurl, content, str(revisons_list[(from_index+1)][\"id\"]), str(revisons_list[to_index][\"id\"]))\n",
    "create_change(test_wiki, rev_contents, from_index, vocab, epsilon_size)\n",
    "from_index = to_index - 1\n",
    "rev_contents = get_contents(baseurl, content, str(revisons_list[(from_index+1)][\"id\"]), \"\")\n",
    "create_change(test_wiki, rev_contents, from_index, vocab, epsilon_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = content + \".h5\"\n",
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "content = \"Violence_against_Muslims_in_India\"\n",
    "with pd.HDFStore(filename, 'a') as store:\n",
    "    store[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(content+\".pkl\", \"wb\") as file:\n",
    "    pickle.dump(test_wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(content+\".pkl\", \"rb\") as file:\n",
    "    wiki = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_objects = []\n",
    "wiki.revisions[:-1].apply(lambda revision: change_objects.append(revision.change))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=wiki.revisions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vecs(tokens):\n",
    "    in_vocab_tokens = set(tokens) & set(wiki_vec.vocab)\n",
    "    if in_vocab_tokens:\n",
    "        return wiki_vec[in_vocab_tokens].sum(axis=0, keepdims=True)\n",
    "    else:\n",
    "        return np.zeros((1, wiki_vec.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 936 ms, total: 11.8 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "change_vecs_list = []\n",
    "change_token_s = change_df[\"ins_tokens\"] + change_df[\"del_tokens\"]\n",
    "change_token_s.apply(lambda token_set: change_vecs_list.append(get_word_vecs(token_set)))\n",
    "\n",
    "change_matrix = np.concatenate(change_vecs_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 s, sys: 380 ms, total: 2.38 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neigh_vecs_list = []\n",
    "neighbour_s = change_df['left_token'] + change_df['right_token']\n",
    "neighbour_s.apply(lambda token_set: neigh_vecs_list.append(get_word_vecs(token_set)))\n",
    "\n",
    "neighbour_matrix = np.concatenate(neigh_vecs_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters= 30)\n",
    "clusters = km.fit(neighbour_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_s = pd.Series(clusters.labels_, index= change_df.index)\n",
    "change_df[\"cluster\"] = cluster_s\n",
    "change_grouped = change_df.groupby(\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_grouped.get_group(1)[[\"ins_tokens\", \"del_tokens\", \"left_neigh\", \"right_neigh\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
