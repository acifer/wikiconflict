{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq(gap_freq, left_context_freq, right_context_freq, timestamp, relative_position, count = 20):\n",
    "    \n",
    "    fig2, ax = plt.subplots(nrows=1, ncols=1,figsize=(30, 7))\n",
    "    ax.scatter( np.arange(relative_position.shape[0])+1, relative_position, c=\"red\",marker=\"D\", label = \"relative position with respect to timestamp\")\n",
    "    ax.set_title(\"Time scale invariant Plot of timestamp with relative position\")\n",
    "    ax.set_xlabel(\"Position with respect to time\")\n",
    "    ax.set_ylabel(\"relative position \")\n",
    "    ax.set_xticklabels(timestamp)\n",
    "    plt.ylim(0, 1)\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3,figsize=(30, 10))\n",
    "        \n",
    "    axs[0].barh( left_context_freq.index[:count][::-1], left_context_freq.values[:count][::-1])\n",
    "    axs[0].set_title(\" frequency plot of top 100 words in left context\")\n",
    "    axs[0].set_xlabel(\"frequency\")\n",
    "    axs[0].set_ylabel(\"unique words in left context \")\n",
    "    \n",
    "    axs[1].barh( gap_freq.index[:count][::-1], gap_freq.values[:count][::-1])\n",
    "    axs[1].set_title(\" frequency plot of top 100 words in gap\")\n",
    "    axs[1].set_xlabel(\"frequency\")\n",
    "    axs[1].set_ylabel(\"unique words in gap \")\n",
    "    \n",
    "    axs[2].barh( right_context_freq.index[:count][::-1], right_context_freq.values[:count][::-1])\n",
    "    axs[2].set_title(\" frequency plot of top 100 words in right context\")\n",
    "    axs[2].set_xlabel(\"frequency\")\n",
    "    axs[2].set_ylabel(\"unique words in right context \")\n",
    "#     axs[3].set_xscale(\"log\")\n",
    "#     axs[3].set_yscale(\"log\")\n",
    "\n",
    "\n",
    "    return fig\n",
    "# _= plot_freq(edited_tokens_freq_per_group.loc[1], \n",
    "#             left_context_freq_per_group.loc[1], \n",
    "#             right_context_freq_per_group.loc[1],\n",
    "#             change_grouped_by_tokens[\"timestamp\"].get_group(2).values,\n",
    "#             change_grouped_by_tokens[\"relative_position\"].get_group(2).values\n",
    "#             )\n",
    "\n",
    "def display_article_content(index, change_html_series, edited_tokens_freq_per_group, left_context_freq_per_group, right_context_freq_per_group, change_grouped_by_tokens, out):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "    if index in change_html_series.index:\n",
    "        change_html = change_html_series.loc[index]\n",
    "    else:\n",
    "        change_html = \"<p>empty table according to cleanup</p>\"\n",
    "    \n",
    "    if index in left_context_freq_per_group.index:\n",
    "        left_context_freq = left_context_freq_per_group.loc[index]\n",
    "    else:\n",
    "        left_context_freq = pd.Series()\n",
    "    \n",
    "    if index in right_context_freq_per_group.index:\n",
    "        right_context_freq = right_context_freq_per_group.loc[index]\n",
    "    else:\n",
    "        right_context_freq = pd.Series()\n",
    "        \n",
    "    if index in edited_tokens_freq_per_group.index:\n",
    "        edited_tokens_freq = edited_tokens_freq_per_group.loc[index]\n",
    "    else:\n",
    "        edited_tokens_freq = pd.Series()\n",
    "    _ = plot_freq(edited_tokens_freq, left_context_freq, right_context_freq,\n",
    "            change_grouped_by_tokens[\"timestamp\"].get_group(index).values,\n",
    "            change_grouped_by_tokens[\"relative_position\"].get_group(index).values)\n",
    "    with out:\n",
    "#         display(change_html)\n",
    "        display(f\"Word length distribution plot for cluster with {index}\")\n",
    "#         display(fig)\n",
    "        display(HTML(change_html))\n",
    "\n",
    "def rank_clusters(change_dataframe, cluster_by):\n",
    "\n",
    "    rank_by_size = change_dataframe.groupby(cluster_by).size().sort_values(ascending=False)\n",
    "\n",
    "    rank_by_uniq_editor = change_dataframe.reset_index().groupby(cluster_by)[\"editor\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "    rank_by_period = change_dataframe.reset_index().groupby(cluster_by)[\"timestamp\"].apply(lambda x: x.max() - x.min()).sort_values(ascending=False)\n",
    "\n",
    "    rank_by_rate = change_dataframe.reset_index().groupby(cluster_by)[\"timegap\"].apply(lambda x: x.mean()).sort_values(ascending=False)\n",
    "\n",
    "#     rank_by_uniq_gaptoken = edited_tokens_freq_per_group.groupby(level=0).apply(lambda x: len(x)).sort_values(ascending=False)\n",
    "    return (rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate)\n",
    "\n",
    "def get_cluster_wordfreq(change_dataframe, cluster_by, vocab_set):\n",
    "    \n",
    "    edited_tokens_freq_per_group = change_dataframe.set_index(cluster_by)[\"edited_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(cluster_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "    left_context_freq_per_group = change_dataframe.set_index(cluster_by)[\"left_token\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(cluster_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "    right_context_freq_per_group = change_dataframe.set_index(cluster_by)[\"right_token\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(cluster_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "    \n",
    "    return (edited_tokens_freq_per_group, left_context_freq_per_group, right_context_freq_per_group)\n",
    "\n",
    "def create_cluster_html_cleaned(change_dataframe, groupby):\n",
    "    \n",
    "\n",
    "    \n",
    "    zero_gap_mask = ((change_dataframe[\"ins_string\"].str.len() + change_dataframe[\"del_string\"].str.len()) == 0).values\n",
    "\n",
    "    change_dataframe = change_dataframe.reset_index().set_index([\"from revision id\",\"to revision id\",\"timestamp\", \"editor\", \"level_5\"])\n",
    "    html_series =  change_dataframe.loc[~zero_gap_mask,:].groupby(cluster_by)[[\"left_cleaned\", \"del_cleaned\", \"ins_cleaned\", \"right_cleaned\"]].apply(lambda x: x.style.render())\n",
    "    change_dataframe = change_dataframe.reset_index().set_index([\"from revision id\", \"editor\", \"level_5\"])\n",
    "    return html_series\n",
    "\n",
    "def create_cluster_html(change_dataframe, groupby):\n",
    "    \n",
    "    \n",
    "    zero_gap_mask = ((change_dataframe[\"ins_string\"].str.len() + change_dataframe[\"del_string\"].str.len()) == 0).values\n",
    "\n",
    "    change_dataframe = change_dataframe.reset_index().set_index([\"from revision id\",\"to revision id\",\"timestamp\", \"editor\", \"level_5\"])\n",
    "    html_series =  change_dataframe.loc[~zero_gap_mask,:].groupby(cluster_by)[[\"left_string\", \"del_string\", \"ins_string\", \"right_string\"]].apply(lambda x: x.style.render())\n",
    "    change_dataframe = change_dataframe.reset_index().set_index([\"from revision id\", \"editor\", \"level_5\"])\n",
    "    \n",
    "    return html_series    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"Violence_against_Muslims_in_India\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "\n",
    "\n",
    "change_vector_dir = \"../data/change_vector_optimised/\"\n",
    "change_vec_filename = f\"{article_name}_comp_vec.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../wordvectors/vocabs.pkl\", \"rb\") as file:\n",
    "    vocabs = pickle.load(file)\n",
    "with open(\"../../wordvectors/filtered_vocabs.pkl\", \"rb\") as file:\n",
    "    filtered_vocabs = pickle.load(file)\n",
    "vocabs_set = set(vocabs)\n",
    "filtered_vocabs_set = set(filtered_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 892 ms, sys: 224 ms, total: 1.12 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "    \n",
    "change_object_dataframe[\"edited_tokens\"] = change_object_dataframe[\"ins_tokens\"]  + change_object_dataframe[\"del_tokens\"]\n",
    "rev_len_df = pd.read_hdf(len_file_path, key = \"rev_len\")\n",
    "\n",
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    clean_weighted_4 = arrays_dict[\"4_clean_weighted\"]\n",
    "    clean_not_weighted_4 = arrays_dict[\"4_clean_not_weighted\"]\n",
    "    notclean_weighted_4 = arrays_dict[\"4_notclean_weighted\"]\n",
    "    notclean_not_weighted_4 = arrays_dict[\"4_notclean_not_weighted\"]\n",
    "    clean_weighted_10 = arrays_dict[\"10_clean_weighted\"]\n",
    "    clean_not_weighted_10 = arrays_dict[\"10_clean_not_weighted\"]\n",
    "    notclean_weighted_10 = arrays_dict[\"10_notclean_weighted\"]\n",
    "    notclean_not_weighted_10 = arrays_dict[\"10_notclean_not_weighted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make left, ins and delete string for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_object_dataframe[\"left_string\"] = change_object_dataframe[\"left_token\"].str.join(\" \")\n",
    "change_object_dataframe[\"ins_string\"] = change_object_dataframe[\"ins_tokens\"].str.join(\" \")\n",
    "change_object_dataframe[\"del_string\"] = change_object_dataframe[\"del_tokens\"].str.join(\" \")\n",
    "change_object_dataframe[\"right_string\"] = change_object_dataframe[\"right_token\"].str.join(\" \")\n",
    "\n",
    "change_object_dataframe[\"left_cleaned\"] = change_object_dataframe[\"left_token\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "change_object_dataframe[\"del_cleaned\"] = change_object_dataframe[\"ins_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "change_object_dataframe[\"ins_cleaned\"] = change_object_dataframe[\"del_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "change_object_dataframe[\"right_cleaned\"] = change_object_dataframe[\"right_token\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding relative positions of change object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_object_dataframe = change_object_dataframe.reset_index().set_index('from revision id')\n",
    "change_object_dataframe = change_object_dataframe.join(rev_len_df.set_index(\"rev_id\"))\n",
    "change_object_dataframe.index.name = \"from revision id\"\n",
    "\n",
    "change_object_dataframe[\"relative_position\"] =(change_object_dataframe[\"left_neigh\"]+1)/(change_object_dataframe[\"length\"])\n",
    "\n",
    "change_object_dataframe = change_object_dataframe.reset_index().set_index([\"from revision id\",\"timestamp\", \"level_5\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CLustering, Ranking and Visualisation of Different weighting of neighbourhood context vectors.\n",
    "\n",
    "Each change object consists of gap, right context and left context. \n",
    "\n",
    "To cluster similar words together each change object needs to be converted to a vector of fixed dimension. \n",
    "We utilised Word vectors embeddings of 300 dimension to convert change object to change vectors. Each of left and right context is seperately averaged using word embeddings and then concatinated along vector dimension to convert change object as vector of 600 dimension.\n",
    "\n",
    "Vocabulary: Is one million vocabulary from fast text embedding word vectors sorted by their frequency of occurrency in the corpus.\n",
    "\n",
    "**Cleaned** Vocabulary: Is created after cleaning the vocabulary where its top 20 ranked keywords are removed and all the word of size less than 4 is removed.\n",
    "\n",
    "We utlise these two vocabulary and its corresponding vector to average context words and create change vectors. Each word in the context is weighted according to its proximity from gap. Using different weighing mechanism we create 8 word vectors.\n",
    "\n",
    "\n",
    "Following is the description of each of them:-\n",
    "1. clean_weighted_4: Taking four nearby neighbours and intersecting with **Cleaned** vocaboulary. Vectors corresponding to   these context words were averaged after weighting them with inverse of words distance from gap. \n",
    "2. clean_not_weighted_4: Taking four nearby tokens of context and intersecting with **Cleaned** vocabulary.Vectors corresponding to   these context words were averaged without any weighting.\n",
    "3. notclean_weighted_4:   Taking four nearby neighbours and intersecting with vocaboulary. Vectors corresponding to   these context words were averaged after weighting them with inverse of words distance from gap. \n",
    "4. notclean_not_weighted_4:  Taking four nearby tokens of context and intersecting with vocabulary.Vectors corresponding to   these context words were averaged without any weighting. \n",
    "5. clean_weighted_10: Taking ten nearby neighbours and intersecting with **Cleaned** vocaboulary. Vectors corresponding to   these context words were averaged after weighting them with inverse of words distance from gap.\n",
    "6. clean_not_weighted_10: Taking ten nearby tokens of context and intersecting with **Cleaned** vocabulary.Vectors corresponding to   these context words were averaged without any weighting.\n",
    "7. notclean_weighted_10: Taking ten nearby neighbours and intersecting with vocaboulary. Vectors corresponding to   these context words were averaged after weighting them with inverse of words distance from gap.\n",
    "8. notclean_not_weighted_10: Taking ten nearby tokens of context and intersecting with vocabulary.Vectors corresponding to   these context words were averaged without any weighting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#### Ranking\n",
    "###### Clustered groups were then ranked on following parameters.\n",
    "\n",
    "1.  Size of clusters(rank_by_size): This parameter sorts clusters by total number of data points contained in the cluster.\n",
    "2. No of unique editors is clusters(rank_by_uniq_editor): This prameter sorts cluster by number of unique editors involved in the cluster.\n",
    "3. Total period of cluster(rank_by_period): This sorts clusters based on the period  for which cluster was edited. which is the difference between edit timestamp of first cluster and edit timestamp of last cluster.\n",
    "4. Average edit rate of cluster(rank_by_period): Rate is timegap between two edits. This sorts cluster which has higest mean edit rates to lowest mean edit rates\n",
    "\n",
    "widgets.interact has a __**index=**__ parameter which can alter rankingg of clusters following 4 are the name of ranking parameter corresponding to following 4 given above please change them accordingly. It takes a variable named clean_weighted_4_drop_downs which can alter the ranking basis to use different ranking methods use different index of variable. Following list entails which parameter to use for which ranking\n",
    "\n",
    "1. rank_by_size: clean_weighted_4_drop_downs[0] \n",
    "2. rank_by_uniq_editor: clean_weighted_4_drop_downs[1]\n",
    "3. rank_by_period: clean_weighted_4_drop_downs[2]\n",
    "4. rank_by_rate: clean_weighted_4_drop_downs[3]\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "#### clustering\n",
    "\n",
    "##### DBSCAN clustering \n",
    "\n",
    "DBSCAN is a clustering method which do not have a fixed amount of clusters. DBSCAN starts with  core points which has minimu number of samples within epsilon distance then it uses epsilon distance to add new points to the clusters. It has has two main parameters minimum number of samples in a cluster and epsilon distance. Both of them control the growth of the clusters. A bigger number of minimum samples will tend to have few clusters as it will be harder to form a cluster with core samples. A smaller epsilon distance will stop addition of new points to core group thus leading to more clusters.\n",
    "\n",
    "\n",
    "\n",
    "Sickit Learns implementation has two parameters eps and min_samples. which corresponds to the minmum distance between two neighbourhood points to be considered in the same neighbourhood. As eps values will start to to increase more points will start getting added in a cluster thus forming bigger clusters.\n",
    "eps: epsilon \n",
    "min_samples: controls the minimum number of points in cluster.\n",
    "\n",
    "\n",
    "dbscan_params has five config stored as can be seen below 0 th is default. -1 gives biggeer and fewer cluster\n",
    "\n",
    "---\n",
    "---\n",
    "#### Visulaisation\n",
    "\n",
    "We have two kinds of plots first is three frequency plots tokens, namely in gap, left frequency and right frequency another is time invariant plot of relative position of change object with respect to its relative occurrence.\n",
    "\n",
    "As we have two kinds of change object vectors one where vocabulary is cleaned another is un cleaned one. We can have two kinds of frequency two one which only plots cleaned token frequency and another which plots uncleaned token frequency. Function get_cluster_wordfreq(\n",
    "    cluster_df, cluster_by, vocabs_set) has three parameter first one is cluster, second one is parameter on which clustering is done last one is vocab set to be considered for frequency. If this vocab set is changed to filtered_vocabs_set only filtered words frequency will be shown.\n",
    "    \n",
    " __CLuster strings:-__ Apart from this we also show all the elements of clusters down. As these change objects are dervied directly from wiki who they are by default not cleaned but we provide two functions one which shows clusters gap and context without removal of any special characters. Second method removes all the wikiwho tokens which is not alphanumeric to increase readability.\n",
    " Two methods are \n",
    " 1.create_cluster_html(non_neg_cluster_df, cluster_by)\n",
    " 2.create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    " \n",
    " Both have similar function signature first is default used when change vectors are cleaned second one is used when change vector is not cleaned. But as removal of non alphanumeric has not significance for change vector cleaning. All cluster viewing can be replaced by 2. to give more readable change objects in clusters but it also removed few clusters with only  gap which is non alpha numeric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_weighted_4\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eps': 0.5, 'min_samples': 5},\n",
       " {'eps': 1, 'min_samples': 5},\n",
       " {'eps': 1.25, 'min_samples': 5},\n",
       " {'eps': 0.75, 'min_samples': 5},\n",
       " {'eps': 0.1, 'min_samples': 7},\n",
       " {'eps': 0.001, 'min_samples': 10}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan_params =[\n",
    "    { \"eps\": 0.5, \"min_samples\": 5 },\n",
    "    { \"eps\": 1, \"min_samples\": 5 }, \n",
    "    { \"eps\": 1.25, \"min_samples\": 5 },\n",
    "    { \"eps\": .75, \"min_samples\": 5 },  \n",
    "    { \"eps\": .1, \"min_samples\": 7 },\n",
    "    { \"eps\": .001, \"min_samples\": 10 }]\n",
    "dbscan_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 44 ms, total: 10.3 s\n",
      "Wall time: 9.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[0]).fit(clean_weighted_4)\n",
    "cluster_by = \"clean_weighted_4\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "\n",
    "#uncomment the next line and comment the line afer that if you want to see frequency of words without cleaning up\n",
    "# clean_weighted_4_gap_freq, clean_weighted_4_left_context_freq, clean_weighted_4_right_context_freq = get_cluster_wordfreq(non_neg_cluster_df, cluster_by, vocabs_set)\n",
    "clean_weighted_4_gap_freq, clean_weighted_4_left_context_freq, clean_weighted_4_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, filtered_vocabs_set)\n",
    "\n",
    "clean_weighted_4_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "clean_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    "\n",
    "clean_weighted_4_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f6e9e11fff41188d664a5d428d6916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 2), (1, 3), (2, 46), (3, 47), (4, 51), (5, 50…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf3f4fbf378462381382dd8d3b5f244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_clean_weighted_4 = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=clean_weighted_4_drop_downs[2], \n",
    "                   change_html_series= widgets.fixed(clean_weighted_4_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(clean_weighted_4_gap_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(clean_weighted_4_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(clean_weighted_4_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(clean_weighted_4_grouped), out= widgets.fixed(out_clean_weighted_4))\n",
    "out_clean_weighted_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_not_weighted_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 15s, sys: 220 ms, total: 9min 15s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[0], metric=\"minkowski\", p=10).fit(clean_not_weighted_4)\n",
    "\n",
    "cluster_by = \"clean_not_weighted_4\"\n",
    "\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "clean_not_weighted_4_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "clean_not_weighted_4_gap_freq, clean_not_weighted_4_left_context_freq, clean_not_weighted_4_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, filtered_vocabs_set)\n",
    "\n",
    "clean_not_weighted_4_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "\n",
    "# uncomment to see cluster string without cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html(non_neg_cluster_df, cluster_by)\n",
    "clean_not_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041fa30fc34e47c19f65f9dc43f48bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 0), (1, 3), (2, 26), (3, 46), (4, 59), (5, 51…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e975c1770fca4b9bbe24f0867c7ae86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_not_weighted_4_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=clean_not_weighted_4_drop_downs[2], \n",
    "                   change_html_series= widgets.fixed(clean_not_weighted_4_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(clean_not_weighted_4_gap_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(clean_not_weighted_4_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(clean_not_weighted_4_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(clean_not_weighted_4_grouped), out= widgets.fixed(clean_not_weighted_4_outp))\n",
    "clean_not_weighted_4_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notclean_weighted_4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 68 ms, total: 16.1 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[-3]).fit(notclean_weighted_4)\n",
    "\n",
    "cluster_by = \"notclean_weighted_4\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "notclean_weighted_4_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "notclean_weighted_4_gap_freq, notclean_weighted_4_left_context_freq, notclean_weighted_4_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, vocabs_set)\n",
    "\n",
    "notclean_weighted_4_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string after cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    "notclean_weighted_4_repers = create_cluster_html(non_neg_cluster_df, cluster_by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d524244b1af64442a3b461d09e0c7b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 3), (1, 39), (2, 43), (3, 37), (4, 36), (5, 3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e44e91dd1402a9d4af06bbef96fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notclean_weighted_4_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=notclean_weighted_4_drop_downs[2], \n",
    "                   change_html_series= widgets.fixed(notclean_weighted_4_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(notclean_weighted_4_gap_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(notclean_weighted_4_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(notclean_weighted_4_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(notclean_weighted_4_grouped), out= widgets.fixed(notclean_weighted_4_outp))\n",
    "notclean_weighted_4_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notclean_not_weighted_4\n",
    "\n",
    "* eps=0.5, min_samples=5// 47 seems to have more noise than 10 neighbours\n",
    "* eps=1, min_samples=5 //58\n",
    "* eps=1.25, min_samples=5 // \n",
    "* eps=.75, min_samples=5// 52\n",
    "* eps=.1, min_samples=7 //\n",
    "* eps=.001, min_samples=10 // 11 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 s, sys: 0 ns, total: 3.14 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[-2]).fit(notclean_not_weighted_4)\n",
    "\n",
    "cluster_by = \"notclean_not_weighted_4\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "notclean_not_weighted_4_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "notclean_not_weighted_4_gap_freq, notclean_not_weighted_4_left_context_freq, notclean_not_weighted_4_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, vocabs_set)\n",
    "\n",
    "notclean_not_weighted_4_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string after cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    "notclean_not_weighted_4_repers = create_cluster_html(non_neg_cluster_df, cluster_by)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f491622f713498395124ee819be07c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 8), (1, 11), (2, 1), (3, 21), (4, 17), (5, 16…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb4924fe4004f279381da3b7a4f63f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notclean_not_weighted_4_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=notclean_not_weighted_4_drop_downs[1], \n",
    "                   change_html_series= widgets.fixed(notclean_not_weighted_4_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(notclean_not_weighted_4_gap_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(notclean_not_weighted_4_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(notclean_not_weighted_4_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(notclean_not_weighted_4_grouped), out= widgets.fixed(notclean_not_weighted_4_outp))\n",
    "notclean_not_weighted_4_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_weighted_10\n",
    "\n",
    "* eps=0.5, min_samples=5// 41 samples detected few neighbourhood with special character do not get clustered.\n",
    "* eps=1, min_samples=5 \n",
    "* eps=1.25, min_samples=5 // 56 clusters seems to detect better clusters then uncleaned ones. Seems vectors are better divided so for higher values of epsilon also gives better clusters\n",
    "\n",
    "* eps=.75, min_samples=5// 48 samples all were having similar relative position but quite smaller than uncleaned ones, proving the point the neighbourhood is quite seperated in vector space.\n",
    "* eps=.1, min_samples=7 // 10 clusters of exactly similar neighbourhood and relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 4 ms, total: 3.03 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[-2]).fit(clean_weighted_10)\n",
    "\n",
    "cluster_by = \"clean_weighted_10\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "clean_weighted_10_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "clean_weighted_10_gap_freq, clean_weighted_10_left_context_freq, clean_weighted_10_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, filtered_vocabs_set)\n",
    "\n",
    "clean_weighted_10_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string without cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html(non_neg_cluster_df, cluster_by)\n",
    "clean_weighted_10_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc3885be0a49e79fbf36f75ad7e61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 3), (1, 4), (2, 10), (3, 5), (4, 8), (5, 7), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8111ca59964cd8a2d9c1e5f84274ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_weighted_10_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=clean_weighted_10_drop_downs[0], \n",
    "                   change_html_series= widgets.fixed(clean_weighted_10_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(clean_weighted_10_gap_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(clean_weighted_10_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(clean_weighted_10_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(clean_weighted_10_grouped), out= widgets.fixed(clean_weighted_10_outp))\n",
    "clean_weighted_10_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_not_weighted_10\n",
    "\n",
    "\n",
    "* eps=0.5, min_samples=5// 32 samples detected few neighbourhood with special character do not get clustered.\n",
    "* eps=1, min_samples=5 \n",
    "* eps=1.25, min_samples=5 // 56 clusters seems to detect better clusters then uncleaned ones. Seems vectors are better divided so for higher values of epsilon also gives better clusters\n",
    "\n",
    "* eps=.75, min_samples=5// 39 samples all were having similar relative position but quite smaller than uncleaned ones, proving the point the neighbourhood is quite seperated in vector space.\n",
    "* eps=0.01, min_samples=10 // 5 clusters of exactly similar neighbourhood and relative position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 37s, sys: 1min 19s, total: 6min 57s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# clusters = DBSCAN(**dbscan_params[2]).fit(clean_not_weighted_10)\n",
    "\n",
    "# cluster_by = \"clean_not_weighted_10\"\n",
    "\n",
    "clusters = MeanShift(cluster_all=True).fit(clean_not_weighted_10)\n",
    "\n",
    "cluster_by = \"kmeans_clean_not_weighted_10\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "clean_not_weighted_10_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "clean_not_weighted_10_freq, clean_not_weighted_10_left_context_freq, clean_not_weighted_10_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, filtered_vocabs_set)\n",
    "\n",
    "clean_not_weighted_10_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string after cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    "clean_not_weighted_10_repers = create_cluster_html(non_neg_cluster_df, cluster_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9387dc0e9fe4f67af22a01f7bc58747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 0), (1, 4), (2, 16), (3, 3), (4, 2), (5, 6), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d544736088e74d7e890e8cffaf3f7b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_not_weighted_10_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=clean_not_weighted_10_drop_downs[2], \n",
    "                   change_html_series= widgets.fixed(clean_not_weighted_10_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(clean_not_weighted_10_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(clean_not_weighted_10_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(clean_not_weighted_10_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(clean_not_weighted_10_grouped), out= widgets.fixed(clean_not_weighted_10_outp))\n",
    "clean_not_weighted_10_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notclean_weighted_10\n",
    "\n",
    "* eps=0.5, min_samples=5// 28 samples detected few neighbourhood with special character do not get clustered.\n",
    "* eps=1, min_samples=5 //72 quite a bit of noise\n",
    "* eps=1.25, min_samples=5 // 56\n",
    "\n",
    "* eps=.75, min_samples=5// 45 clusters\n",
    "* eps=.1, min_samples=6 // 16 clusters of exactly similar neighbourhood and relative position/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.8 s, sys: 8 ms, total: 2.81 s\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[-2]).fit(notclean_weighted_10)\n",
    "\n",
    "cluster_by = \"notclean_weighted_10\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "notclean_weighted_10_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "notclean_weighted_10_freq, notclean_weighted_10_left_context_freq, notclean_weighted_10_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, vocabs_set)\n",
    "\n",
    "notclean_weighted_10_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string after cleaning\n",
    "# clean_not_weighted_4_repers = create_cluster_html_cleaned(non_neg_cluster_df, cluster_by)\n",
    "notclean_weighted_10_repers = create_cluster_html(non_neg_cluster_df, cluster_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e4504ca4f45268f6b55e930aa4d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 1), (1, 2), (2, 8), (3, 3), (4, 6), (5, 5), (…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512d45e2b6c74a9b9f11cee3b2cb3025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notclean_weighted_10_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=notclean_weighted_10_drop_downs[0], \n",
    "                   change_html_series= widgets.fixed(notclean_weighted_10_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(notclean_weighted_10_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(notclean_weighted_10_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(notclean_weighted_10_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(notclean_weighted_10_grouped), out= widgets.fixed(notclean_weighted_10_outp))\n",
    "notclean_weighted_10_outp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notclean_not_weighted_10\n",
    "\n",
    "\n",
    "* eps=0.5, min_samples=5// 28 samples stable neighbourhood\n",
    "* eps=1, min_samples=5 // 97 clusters as eps is high more noise\n",
    "* eps=.75, min_samples=5// 53 samples all were having similar relative position\n",
    "* eps=0.01, min_samples=10 // 5 clusters of exactly similar neighbourhood and relative position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 4 ms, total: 3.74 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "clusters = DBSCAN(**dbscan_params[-2]).fit(notclean_not_weighted_10)\n",
    "\n",
    "cluster_by = \"notclean_not_weighted_10\"\n",
    "change_object_dataframe[cluster_by] = pd.Series(clusters.labels_, index= change_object_dataframe.index)\n",
    "\n",
    "\n",
    "non_zero_cluster_mask = (change_object_dataframe[cluster_by] != -1)\n",
    "non_neg_cluster_df = change_object_dataframe.loc[non_zero_cluster_mask, :]\n",
    "\n",
    "rank_by_size, rank_by_uniq_editor, rank_by_period, rank_by_rate = rank_clusters(non_neg_cluster_df, cluster_by)\n",
    "not_clean_not_weighted_10_drop_downs = [list(zip(np.arange(rank_by_size.size), rank_by_size.index)), \n",
    "                list(zip(np.arange(rank_by_uniq_editor.size), rank_by_uniq_editor.index)),\n",
    "                list(zip(np.arange(rank_by_period.size), rank_by_period.index)),\n",
    "                list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))]\n",
    "\n",
    "not_clean_not_weighted_10_freq, not_clean_not_weighted_10_left_context_freq, not_clean_not_weighted_10_right_context_freq = get_cluster_wordfreq(\n",
    "    non_neg_cluster_df, cluster_by, vocabs_set)\n",
    "\n",
    "not_clean_not_weighted_10_grouped = non_neg_cluster_df.reset_index().groupby(cluster_by)\n",
    "\n",
    "# uncomment to see cluster string without cleaning and coment line after that\n",
    "# clean_not_weighted_4_repers = create_cluster_html(non_neg_cluster_df, cluster_by)\n",
    "not_clean_not_weighted_10_repers = create_cluster_html(non_neg_cluster_df, cluster_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e6f56dee8c4c949543ec7dff34fc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 0), (1, 4), (2, 5), (3, 1), (4, 3), (5, 2)), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29c65353df646dbb8ab6cf959a5d1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_clean_not_weighted_10_outp = widgets.Output(layout={'r': '1px solid black'})\n",
    "caption = widgets.Label(value='Choose parameters:')\n",
    "_=widgets.interact(display_article_content, index=not_clean_not_weighted_10_drop_downs[2], \n",
    "                   change_html_series= widgets.fixed(not_clean_not_weighted_10_repers),\n",
    "                   edited_tokens_freq_per_group= widgets.fixed(not_clean_not_weighted_10_freq), \n",
    "                   left_context_freq_per_group= widgets.fixed(not_clean_not_weighted_10_left_context_freq), \n",
    "                   right_context_freq_per_group= widgets.fixed(not_clean_not_weighted_10_right_context_freq), \n",
    "                   change_grouped_by_tokens= widgets.fixed(not_clean_not_weighted_10_grouped), out= widgets.fixed(not_clean_not_weighted_10_outp))\n",
    "not_clean_not_weighted_10_outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>clean_weighted_4</th>\n",
       "      <th>clean_not_weighted_4</th>\n",
       "      <th>notclean_weighted_4</th>\n",
       "      <th>notclean_not_weighted_4</th>\n",
       "      <th>clean_weighted_10</th>\n",
       "      <th>clean_not_weighted_10</th>\n",
       "      <th>notclean_weighted_10</th>\n",
       "      <th>notclean_not_weighted_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from revision id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level_5</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558137654</th>\n",
       "      <th>2013-06-03 14:58:33</th>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558137760</th>\n",
       "      <th>2013-06-03 15:00:42</th>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_weighted_4  \\\n",
       "from revision id timestamp           level_5                     \n",
       "558137654        2013-06-03 14:58:33 0                      -1   \n",
       "558137760        2013-06-03 15:00:42 0                      -1   \n",
       "\n",
       "                                              clean_not_weighted_4  \\\n",
       "from revision id timestamp           level_5                         \n",
       "558137654        2013-06-03 14:58:33 0                          -1   \n",
       "558137760        2013-06-03 15:00:42 0                           0   \n",
       "\n",
       "                                              notclean_weighted_4  \\\n",
       "from revision id timestamp           level_5                        \n",
       "558137654        2013-06-03 14:58:33 0                         -1   \n",
       "558137760        2013-06-03 15:00:42 0                         -1   \n",
       "\n",
       "                                              notclean_not_weighted_4  \\\n",
       "from revision id timestamp           level_5                            \n",
       "558137654        2013-06-03 14:58:33 0                             -1   \n",
       "558137760        2013-06-03 15:00:42 0                             -1   \n",
       "\n",
       "                                              clean_weighted_10  \\\n",
       "from revision id timestamp           level_5                      \n",
       "558137654        2013-06-03 14:58:33 0                       -1   \n",
       "558137760        2013-06-03 15:00:42 0                       -1   \n",
       "\n",
       "                                              clean_not_weighted_10  \\\n",
       "from revision id timestamp           level_5                          \n",
       "558137654        2013-06-03 14:58:33 0                           -1   \n",
       "558137760        2013-06-03 15:00:42 0                           -1   \n",
       "\n",
       "                                              notclean_weighted_10  \\\n",
       "from revision id timestamp           level_5                         \n",
       "558137654        2013-06-03 14:58:33 0                          -1   \n",
       "558137760        2013-06-03 15:00:42 0                          -1   \n",
       "\n",
       "                                              notclean_not_weighted_10  \n",
       "from revision id timestamp           level_5                            \n",
       "558137654        2013-06-03 14:58:33 0                              -1  \n",
       "558137760        2013-06-03 15:00:42 0                              -1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_object_dataframe[change_object_dataframe.columns[-8:]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = \"../data/clusters/\"\n",
    "\n",
    "file_name = article_name + \"_dbscan_cluster.h5\"\n",
    "full_file_path = os.path.join(cluster_dir, file_name)\n",
    "with pd.HDFStore(full_file_path, 'w') as store:\n",
    "    store.put(\"cluster\", change_object_dataframe[change_object_dataframe.columns[-8:]], table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
