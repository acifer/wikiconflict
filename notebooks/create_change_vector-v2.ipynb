{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 12.8 s, total: 4min 57s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lenght of vocabulary is 1000000 words'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lenght of vocabulary is 970137 words'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_list = list(wiki_vec.vocab)\n",
    "filtered_vocab = [ t for t in vocab_list[20:] if len(t) > 3]\n",
    "\n",
    "display(f\"lenght of vocabulary is {len(vocab_list)} words\")\n",
    "display(f\"lenght of vocabulary is {len(filtered_vocab)} words\")\n",
    "\n",
    "vocab_list = np.array(vocab_list)\n",
    "filtered_vocab = np.array(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(change_vector_dir\n",
    "# with open(\"../../wordvectors/vocabs.pkl\", \"wb\") as file:\n",
    "# #     print(file)\n",
    "#     pickle.dump(filtered_vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vecs(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[-1] == \"{st@rt}\":\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        print(tokens)\n",
    "        tokens = tokens[:-1]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "#     print(tokens_in_vocab_mask)\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "\n",
    "def weighted_average_word_vec(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[-1] == \"{st@rt}\":\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        tokens = tokens[:-1]\n",
    "    token_len = len(tokens)\n",
    "    if token_len == 0:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "    weights = (token_len-np.arange(token_len))/token_len\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    in_vocab_weight = weights[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], weights=in_vocab_weight, axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"Berlin_Wall\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "# change_file_name = f\"{article_name}.pkl\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19641, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 580 ms, sys: 96 ms, total: 676 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "display(change_object_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 55s, sys: 264 ms, total: 15min 55s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_dir = \"../data/content/\"\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "unique_str = np.unique(all_rev.str)\n",
    "str_in_vocab_mask = np.isin(unique_str, vocab_list, assume_unique=True)\n",
    "str_in_filtered_vocab_mask = np.isin(unique_str, filtered_vocab, assume_unique=True)\n",
    "\n",
    "vocab_masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":str_in_vocab_mask}).set_index(\"str\")\n",
    "filtered_vocab_masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":str_in_filtered_vocab_mask}).set_index(\"str\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vector from change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 30s, sys: 1.2 s, total: 5min 31s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df,  token_set)).values)\n",
    "neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "# weighted vectors\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "weighted_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "weighted_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: get_word_vecs(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "not_filtered_not_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, vocab_masks_df, token_set)).values)\n",
    "not_filtered_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_not_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1] ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set ).apply(lambda token_set: weighted_average_word_vec(wiki_vec, filtered_vocab_masks_df, token_set)).values)\n",
    "filtered_weighted_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving change object vector  to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vector_dir = \"../data/change_vector_optimised/\"\n",
    "change_vec_filename = f\"{article_name}_comp_vec.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "arrays_to_save = {\n",
    "    \"4_clean_weighted\": filtered_weighted_neighbour4_matrix,\n",
    "    \"10_clean_weighted\": filtered_weighted_neighbour10_matrix,\n",
    "\n",
    "    \"4_clean_not_weighted\": filtered_neighbour4_matrix,\n",
    "    \"10_clean_not_weighted\": filtered_neighbour10_matrix,\n",
    "\n",
    "    \"4_notclean_weighted\": weighted_neighbour4_matrix,\n",
    "    \"4_notclean_not_weighted\": neighbour4_matrix,\n",
    "    \"10_notclean_weighted\": weighted_neighbour10_matrix,\n",
    "    \"10_notclean_not_weighted\": neighbour10_matrix,\n",
    "    \"not_filtered_not_weighted_neighbour30_matrix\": not_filtered_not_weighted_neighbour30_matrix,\n",
    "    \"not_filtered_weighted_neighbour30_matrix\": not_filtered_weighted_neighbour30_matrix,\n",
    "    \"filtered_weighted_neighbour30_matrix\": filtered_weighted_neighbour30_matrix,\n",
    "    \"filtered_not_weighted_neighbour30_matrix\": filtered_not_weighted_neighbour30_matrix\n",
    "}\n",
    "with open(change_vector_file, \"wb\") as file:\n",
    "    np.savez(file, **arrays_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
