{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    token_string_df = store.get(\"all_tokens\")\n",
    "    \n",
    "token_string_df = token_string_df.set_index(\"token_id\")[\"str\"]\n",
    "token_string_df[-1] = \"St@rt\"\n",
    "token_string_df[-2] = \"$nd\"\n",
    "change_vector_dir = \"../data/change_vector/\"\n",
    "change_vec_filename = f\"{article_name}.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "content_dir = \"../data/content/\"\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "change_object_dataframe[\"del_string_tokens\"] = change_object_dataframe[\"del_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"ins_string_tokens\"] = change_object_dataframe[\"ins_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"edit_string_tokens\"] = change_object_dataframe[\"ins_string_tokens\"] + change_object_dataframe[\"del_string_tokens\"]\n",
    "\n",
    "\n",
    "# rev_len_df = pd.read_hdf(len_file_path, key = \"rev_len\")\n",
    "vectors ={}\n",
    "\n",
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    vectors[2] = arrays_dict[\"2_clean_not_weighted\"]\n",
    "    vectors[4] = arrays_dict[\"4_clean_not_weighted\"]\n",
    "    vectors[6] = arrays_dict[\"6_clean_not_weighted\"]\n",
    "    vectors[8] = arrays_dict[\"8_clean_not_weighted\"]\n",
    "    vectors[10] = arrays_dict[\"10_clean_not_weighted\"]\n",
    "    vectors[12] = arrays_dict[\"12_clean_not_weighted\"]\n",
    "    vectors[15] = arrays_dict[\"15_clean_not_weighted\"]\n",
    "    vectors[20] = arrays_dict[\"20_clean_not_weighted\"]\n",
    "    vectors[25] = arrays_dict[\"25_clean_not_weighted\"]\n",
    "    vectors[30] = arrays_dict[\"30_clean_not_weighted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = article_name + \"_FULL.csv\"\n",
    "annotation_dir = \"../data/annotation/\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "annotation_df = pd.read_csv(full_file_path)\n",
    "annotation_df = annotation_df[[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\", \"nationality\", \"birth_place\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_names = list(vectors.keys())\n",
    "context_array  = vector_names\n",
    "eps_array = [0.5, 0.75, 1.0,  1.25,1.5,1.75, 2.0]\n",
    "min_samples_array = [2]\n",
    "all_combinations = list(itertools.product(context_array, eps_array,\n",
    "                                          min_samples_array))\n",
    "dbscan_params = list(itertools.product(eps_array,min_samples_array))\n",
    "idx = pd.MultiIndex.from_product([context_array, eps_array,min_samples_array],\n",
    "                                names=[\"context\",\"eps\",\"min_samples\"])\n",
    "cluster_df = pd.DataFrame(columns=idx)\n",
    "\n",
    "evaluation_df = pd.DataFrame(index=idx, columns=[\"rand\", \"entropy\", \"token_entropy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 27 s, total: 2min 4s\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for cluster_by in vector_names:\n",
    "    distances = pairwise_distances(vectors[cluster_by])\n",
    "    for eps, min_samples in dbscan_params:\n",
    "        cluster_df[cluster_by,eps, min_samples] = DBSCAN(eps=eps, min_samples=min_samples, \n",
    "                                                         metric=\"precomputed\").fit(distances).labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.index = change_object_dataframe.index\n",
    "dbscan_results = pd.concat([change_object_dataframe, cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# entropy_series = pd.Series(index=all_combinations)\n",
    "\n",
    "for context, eps, min_samples in all_combinations:\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"token_entropy\"] = weighted_token_entropy(dbscan_results, (context, eps, min_samples))\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropy_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6fdf044cade5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentropy_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'entropy_series' is not defined"
     ]
    }
   ],
   "source": [
    "# entropy_series.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cluster_dir = \"../data/clusters/\"\n",
    "\n",
    "# file_name = article_name + \"_dbscan_cluster_4and10.h5\"\n",
    "# full_file_path = os.path.join(cluster_dir, file_name)\n",
    "# with pd.HDFStore(full_file_path, 'w') as store:\n",
    "#     store.put(\"cluster\", change_object_dataframe[[\"clean_4\", \"clean_10\"]], table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change_object_dataframe[[\"ins_tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting change object to match annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap_df, gap_cluster_df):\n",
    "    context_gap = gap_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    context_cluster = gap_cluster_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    clusters = context_cluster.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x),:].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=gap_cluster_df.columns)\n",
    "    else:\n",
    "        clusters = pd.Series(-10, index=gap_cluster_df.columns)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\", ):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_labels = np.zeros((annotation_df.shape[0]))\n",
    "true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps, min_samples in all_combinations:\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps, min_samples))\n",
    "    evaluation_df.loc[(context, eps, min_samples),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps, min_samples)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.25 15 1.7330592020460496]\n",
      " [1.75 4 1.7475881220106997]\n",
      " [1.75 8 1.7718294347286676]\n",
      " [2.0 4 1.8060909013832354]\n",
      " [1.5 4 1.8137690872541257]\n",
      " [1.5 8 1.815186338360246]\n",
      " [1.25 4 1.8291930083932157]\n",
      " [0.5 4 1.8425632327146717]\n",
      " [0.75 4 1.8425632327146717]\n",
      " [1.0 4 1.842563232714672]\n",
      " [1.75 6 1.8700450817561856]\n",
      " [0.75 30 1.899235724069299]\n",
      " [1.5 12 1.914451024167899]\n",
      " [1.0 20 1.9206889660923419]\n",
      " [1.5 6 1.9252727512842187]\n",
      " [1.75 10 1.9379696940177065]\n",
      " [1.5 10 1.941768652083299]\n",
      " [0.75 20 1.9421001648571565]\n",
      " [1.25 8 1.9445861689142498]\n",
      " [0.75 25 1.9493524462194105]\n",
      " [1.25 6 1.9669072424638143]\n",
      " [2.0 6 1.9672492512095245]\n",
      " [1.25 12 1.987419707892861]\n",
      " [1.5 15 1.9930915013655888]\n",
      " [2.0 8 2.0033920820478905]\n",
      " [1.25 10 2.0097189912520936]\n",
      " [1.0 15 2.0104660278750632]\n",
      " [1.0 30 2.013985187940474]\n",
      " [1.75 12 2.020276231543493]\n",
      " [1.0 8 2.0240333545089304]\n",
      " [1.0 6 2.0556072678551103]\n",
      " [1.0 12 2.080924923767969]\n",
      " [0.5 30 2.1019599958377357]\n",
      " [0.75 6 2.101966880442974]\n",
      " [0.5 6 2.101966880442974]\n",
      " [1.0 10 2.127700844186309]\n",
      " [1.25 20 2.1416736780446763]\n",
      " [1.0 25 2.1743444448973244]\n",
      " [2.0 10 2.1908160649818877]\n",
      " [0.5 25 2.214623106404356]\n",
      " [0.75 8 2.2327602047211004]\n",
      " [0.75 15 2.2416749958637183]\n",
      " [0.75 10 2.2689469949847423]\n",
      " [0.5 8 2.2790346430892923]\n",
      " [0.5 20 2.29684109855368]\n",
      " [0.75 12 2.317107864860763]\n",
      " [2.0 2 2.3691954110196645]\n",
      " [0.5 2 2.3734954449448984]\n",
      " [1.5 2 2.3855207909311456]\n",
      " [1.75 2 2.3855207909311456]\n",
      " [1.25 25 2.3999091544468696]\n",
      " [0.75 2 2.4055697077144247]\n",
      " [1.0 2 2.4055697077144247]\n",
      " [1.25 2 2.4055697077144247]\n",
      " [0.5 10 2.4966817521274747]\n",
      " [0.5 15 2.544416464605449]\n",
      " [0.5 12 2.6279042277549856]\n",
      " [1.5 20 2.999796858303494]\n",
      " [1.25 30 3.586695114482605]\n",
      " [1.75 15 4.6716731908652065]\n",
      " [2.0 12 5.000976034333589]\n",
      " [1.5 25 5.84041690650201]\n",
      " [1.5 30 9.203609665617378]\n",
      " [1.75 20 10.338605679394293]\n",
      " [2.0 15 10.63297269090465]\n",
      " [1.75 25 15.324823512653373]\n",
      " [2.0 20 21.589363904046685]\n",
      " [1.75 30 30.590414413214948]\n",
      " [2.0 25 42.710113295207066]\n",
      " [2.0 30 79.8414129817889]]\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_df.reset_index().set_index([\"min_samples\", \"eps\", \"context\"]).loc[2][\"entropy\"].sort_values().iloc[0:100].reset_index().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps   min_samples\n",
       "15       1.25  2              1.73306\n",
       "4        1.75  2              1.74759\n",
       "8        1.75  2              1.77183\n",
       "4        2.00  2              1.80609\n",
       "         1.50  2              1.81377\n",
       "8        1.50  2              1.81519\n",
       "4        1.25  2              1.82919\n",
       "         0.50  2              1.84256\n",
       "         0.75  2              1.84256\n",
       "         1.00  2              1.84256\n",
       "6        1.75  2              1.87005\n",
       "30       0.75  2              1.89924\n",
       "12       1.50  2              1.91445\n",
       "20       1.00  2              1.92069\n",
       "6        1.50  2              1.92527\n",
       "10       1.75  2              1.93797\n",
       "         1.50  2              1.94177\n",
       "20       0.75  2               1.9421\n",
       "8        1.25  2              1.94459\n",
       "25       0.75  2              1.94935\n",
       "6        1.25  2              1.96691\n",
       "         2.00  2              1.96725\n",
       "12       1.25  2              1.98742\n",
       "15       1.50  2              1.99309\n",
       "8        2.00  2              2.00339\n",
       "10       1.25  2              2.00972\n",
       "15       1.00  2              2.01047\n",
       "30       1.00  2              2.01399\n",
       "12       1.75  2              2.02028\n",
       "8        1.00  2              2.02403\n",
       "6        1.00  2              2.05561\n",
       "12       1.00  2              2.08092\n",
       "30       0.50  2              2.10196\n",
       "6        0.75  2              2.10197\n",
       "         0.50  2              2.10197\n",
       "10       1.00  2               2.1277\n",
       "20       1.25  2              2.14167\n",
       "25       1.00  2              2.17434\n",
       "10       2.00  2              2.19082\n",
       "25       0.50  2              2.21462\n",
       "8        0.75  2              2.23276\n",
       "15       0.75  2              2.24167\n",
       "10       0.75  2              2.26895\n",
       "8        0.50  2              2.27903\n",
       "20       0.50  2              2.29684\n",
       "12       0.75  2              2.31711\n",
       "2        2.00  2               2.3692\n",
       "         0.50  2               2.3735\n",
       "         1.50  2              2.38552\n",
       "         1.75  2              2.38552\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eps</th>\n",
       "      <th>context</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>4</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>8</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.25</th>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.25</th>\n",
       "      <th>8</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.50</th>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>10</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.25</th>\n",
       "      <th>6</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>10</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>6</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>4</th>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>8</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.50</th>\n",
       "      <th>6</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.50</th>\n",
       "      <th>25</th>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.75</th>\n",
       "      <th>6</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>12</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.50</th>\n",
       "      <th>20</th>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>20</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>12</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.75</th>\n",
       "      <th>6</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>15</th>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.50</th>\n",
       "      <th>15</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>10</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>30</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>12</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>25</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>8</th>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.25</th>\n",
       "      <th>20</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>10</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.25</th>\n",
       "      <th>25</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.50</th>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>2</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>2</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.25</th>\n",
       "      <th>2</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>15</th>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>12</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.50</th>\n",
       "      <th>25</th>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>20</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>15</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.50</th>\n",
       "      <th>30</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.00</th>\n",
       "      <th>20</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75</th>\n",
       "      <th>30</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2.00</th>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "eps  context     \n",
       "1.75 4        198\n",
       "1.00 8        196\n",
       "1.25 4        196\n",
       "0.50 4        195\n",
       "1.25 8        195\n",
       "1.00 4        195\n",
       "0.75 4        195\n",
       "1.50 4        195\n",
       "     8        193\n",
       "1.00 10       192\n",
       "1.25 6        192\n",
       "     10       191\n",
       "0.75 10       190\n",
       "1.00 6        190\n",
       "2.00 4        189\n",
       "0.75 8        188\n",
       "1.50 6        188\n",
       "0.50 25       188\n",
       "     6        187\n",
       "0.75 6        187\n",
       "     20       187\n",
       "1.00 12       186\n",
       "0.50 20       186\n",
       "     8        185\n",
       "     30       185\n",
       "1.00 20       185\n",
       "0.75 12       185\n",
       "1.75 6        185\n",
       "     8        184\n",
       "0.75 15       184\n",
       "...           ...\n",
       "1.50 15       173\n",
       "     12       172\n",
       "1.75 10       172\n",
       "1.00 30       169\n",
       "1.75 12       167\n",
       "1.00 25       167\n",
       "2.00 8        167\n",
       "1.25 20       165\n",
       "2.00 10       157\n",
       "1.25 25       156\n",
       "0.50 2        150\n",
       "2.00 2        150\n",
       "1.50 2        149\n",
       "     20       149\n",
       "1.75 2        149\n",
       "0.75 2        148\n",
       "1.00 2        148\n",
       "1.25 2        148\n",
       "     30       137\n",
       "1.75 15       125\n",
       "2.00 12       122\n",
       "1.50 25       105\n",
       "1.75 20        82\n",
       "2.00 15        77\n",
       "1.50 30        75\n",
       "1.75 25        56\n",
       "2.00 20        45\n",
       "1.75 30        33\n",
       "2.00 25        24\n",
       "     30        13\n",
       "\n",
       "[70 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_combination_clusters_df.nunique(axis=0).reset_index().set_index([\"min_samples\", \"eps\", \"context\"]).loc[2].sort_values(0, ascending=False)\n",
    "#[0].sort_values().iloc[0:50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps   min_samples\n",
       "15       1.25  2              182\n",
       "4        1.75  2              198\n",
       "8        1.75  2              184\n",
       "4        2.00  2              189\n",
       "         1.50  2              195\n",
       "8        1.50  2              193\n",
       "4        1.25  2              196\n",
       "         0.50  2              195\n",
       "         0.75  2              195\n",
       "         1.00  2              195\n",
       "6        1.75  2              185\n",
       "30       0.75  2              184\n",
       "12       1.50  2              172\n",
       "20       1.00  2              185\n",
       "6        1.50  2              188\n",
       "10       1.75  2              172\n",
       "         1.50  2              179\n",
       "20       0.75  2              187\n",
       "8        1.25  2              195\n",
       "25       0.75  2              184\n",
       "6        1.25  2              192\n",
       "         2.00  2              177\n",
       "12       1.25  2              177\n",
       "15       1.50  2              173\n",
       "8        2.00  2              167\n",
       "10       1.25  2              191\n",
       "15       1.00  2              181\n",
       "30       1.00  2              169\n",
       "12       1.75  2              167\n",
       "8        1.00  2              196\n",
       "6        1.00  2              190\n",
       "12       1.00  2              186\n",
       "30       0.50  2              185\n",
       "6        0.75  2              187\n",
       "         0.50  2              187\n",
       "10       1.00  2              192\n",
       "20       1.25  2              165\n",
       "25       1.00  2              167\n",
       "10       2.00  2              157\n",
       "25       0.50  2              188\n",
       "8        0.75  2              188\n",
       "15       0.75  2              184\n",
       "10       0.75  2              190\n",
       "8        0.50  2              185\n",
       "20       0.50  2              186\n",
       "12       0.75  2              185\n",
       "2        2.00  2              150\n",
       "         0.50  2              150\n",
       "         1.50  2              149\n",
       "         1.75  2              149\n",
       "25       1.25  2              156\n",
       "2        0.75  2              148\n",
       "         1.00  2              148\n",
       "         1.25  2              148\n",
       "10       0.50  2              180\n",
       "15       0.50  2              182\n",
       "12       0.50  2              175\n",
       "20       1.50  2              149\n",
       "30       1.25  2              137\n",
       "15       1.75  2              125\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al_combination_clusters_df.nunique(axis=0)[evaluation_df[\"entropy\"].sort_values().index]\n",
    "# al_combination_clusters_df.values.shape\n",
    "# al_combination_clusters_df.values[0]\n",
    "# al_combination_clusters_df.head()\n",
    "al_combination_clusters_df.nunique(axis=0)[evaluation_df[\"entropy\"].sort_values().iloc[0:60].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_file_name = f\"{article_name}_evaluation.csv\"\n",
    "# result_file_path = os.path.join(annotation_dir, result_file_name)\n",
    "# annotation_df.to_csv(result_file_path)\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[(\n",
    "#                             true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | \n",
    "#                             (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[ (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_10\"], list(nonoverlaping_clusters)+[-1]),\"cluster_10\"] =-999\n",
    "# annotation_df.loc[annotation_df['cluster_10'] != -999,\"cluster_10\"] = 999\n",
    "\n",
    "# nonoverlaping_clusters = set(annotation_df[\"cluster_4\"].unique()) - set(annotation_df.loc[\n",
    "#                             (true_lable_df[\"nationality\"].str.strip().values == \"Y\") , \"cluster_4\"].unique())\n",
    "\n",
    "# annotation_df.loc[np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =-999\n",
    "# #annotation_df.loc[~np.isin(annotation_df[\"cluster_4\"], list(nonoverlaping_clusters)+[-1]),\"cluster_4\"] =999\n",
    "# annotation_df.loc[annotation_df['cluster_4'] != -999,\"cluster_4\"] = 999\n",
    "\n",
    "# _tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "# _tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]\n",
    "# #_tdf['cluster_4_x'] - _tdf['cluster_10_y']\n",
    "# #_tdf.shape\n",
    "# #_tdf\n",
    "\n",
    "# _tdf = annotation_df.merge(full_df.drop(columns=['cluster_4', 'cluster_10']), how='left', left_on=['context_id', 'rev_id', 'token_id'], right_on=['revid_ctxt', 'rev_id', 'token_id'])\n",
    "# _tdf = _tdf[_tdf['Bulk'].str.strip() == 'N']\n",
    "# _tdf = _tdf[['context_id', 'token_id', 'rev_id','cluster_4', 'cluster_10','true_labels', \"nationality\", \"birth_place\"]]\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "# evaluation_score[\"rand_4\"] = adjusted_rand_score( _tdf[\"cluster_4\"], _tdf['true_labels'])\n",
    "# evaluation_score[\"rand_10\"] = adjusted_rand_score( _tdf[\"cluster_10\"], _tdf['true_labels'])\n",
    "# evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(_tdf['true_labels'], \n",
    "#                                             _tdf[\"cluster_4\"], average_method=\"max\"  )\n",
    "# evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(_tdf['true_labels'],\n",
    "#                                             _tdf[\"cluster_10\"], average_method=\"max\" )\n",
    "# evaluation_score\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand_4\", \"rand_10\", \"mutual_info_4\",  \"mutual_info_10\"])\n",
    "# evaluation_score[\"rand_4\"] = adjusted_rand_score( annotation_df[\"cluster_4\"], true_labels)\n",
    "# evaluation_score[\"rand_10\"] = adjusted_rand_score( annotation_df[\"cluster_10\"], true_labels)\n",
    "# evaluation_score[\"mutual_info_4\"] = adjusted_mutual_info_score(true_labels, \n",
    "#                                             annotation_df[\"cluster_4\"], average_method=\"max\"  )\n",
    "# evaluation_score[\"mutual_info_10\"] = adjusted_mutual_info_score(true_labels,\n",
    "#                                             annotation_df[\"cluster_10\"], average_method=\"max\" )\n",
    "# evaluation_score\n",
    "\n",
    "# normalized_mutual_info_score(true_labels, annotation_df[\"cluster_4\"])\n",
    "\n",
    "# set(annotation_df[\"cluster_10\"].unique()) - set(annotation_df.loc[(true_lable_df[\"birth_place\"].str.strip().values == \"Y\") | (true_lable_df[\"birth_place\"].str.strip().values == \"Y\") , \n",
    "#                   \"cluster_10\"].unique())\n",
    "\n",
    "# annotation_df.loc[annotation_df[\"cluster_10\"] == -1,\"cluster_10\"] =-999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <th>eps</th>\n",
       "      <th>min_samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.50</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0866597</td>\n",
       "      <td>2.3735</td>\n",
       "      <td>18551.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0866624</td>\n",
       "      <td>2.40557</td>\n",
       "      <td>18668.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0866624</td>\n",
       "      <td>2.40557</td>\n",
       "      <td>18720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.25</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0866624</td>\n",
       "      <td>2.40557</td>\n",
       "      <td>18704.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.50</th>\n",
       "      <th>2</th>\n",
       "      <td>0.0863464</td>\n",
       "      <td>2.38552</td>\n",
       "      <td>18698.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               rand  entropy token_entropy\n",
       "context eps  min_samples                                  \n",
       "2       0.50 2            0.0866597   2.3735       18551.7\n",
       "        0.75 2            0.0866624  2.40557       18668.3\n",
       "        1.00 2            0.0866624  2.40557         18720\n",
       "        1.25 2            0.0866624  2.40557       18704.8\n",
       "        1.50 2            0.0863464  2.38552       18698.4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.615029</td>\n",
       "      <td>-0.488641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>-0.615029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_entropy</th>\n",
       "      <td>-0.488641</td>\n",
       "      <td>0.794375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand   entropy  token_entropy\n",
       "rand           1.000000 -0.615029      -0.488641\n",
       "entropy       -0.615029  1.000000       0.794375\n",
       "token_entropy -0.488641  0.794375       1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
