{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to convert change object saved in `./data/change object ` into change vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vecs(wiki_vec, masks, tokens, token_string_df):\n",
    "    if tokens and tokens[-1] == -1:\n",
    "        tokens = tokens[:-1]\n",
    "    if tokens and tokens[-1] == -2:\n",
    "        tokens = tokens[:-1]\n",
    "    if len(tokens) == 0:\n",
    "        return np.zeros( wiki_vec.vector_size)\n",
    "    tokens = token_string_df[np.array(tokens)].values\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "#     print(tokens_in_vocab_mask)\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 38s, sys: 3.47 s, total: 3min 41s\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(wiki_vec.vocab)\n",
    "filtered_vocab = [ t for t in vocab_list[20:] if len(t) > 3]\n",
    "\n",
    "# display(f\"lenght of vocabulary is {len(vocab_list)} words\")\n",
    "# display(f\"lenght of vocabulary is {len(filtered_vocab)} words\")\n",
    "\n",
    "vocab_list = np.array(vocab_list)\n",
    "filtered_vocab = np.array(filtered_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 27.4 ms, total: 184 ms\n",
      "Wall time: 202 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "# change_file_name = f\"{article_name}.pkl\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "# display(change_object_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 1.01 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_dir = \"../data/content/\"\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "unique_str = np.unique(all_rev.str)\n",
    "str_in_filtered_vocab_mask = np.isin(unique_str, filtered_vocab, assume_unique=True)\n",
    "\n",
    "filtered_vocab_masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":str_in_filtered_vocab_mask}).set_index(\"str\")\n",
    "\n",
    "\n",
    "\n",
    "token_string_df = all_rev.set_index(\"token_id\")[\"str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vector from change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:30] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:30] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour30_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:25] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:25] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour25_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:20] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:20] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour20_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:15] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:15] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour15_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:12] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:12] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour12_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:10] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour10_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:8] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:8] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour8_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:6] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:6] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour6_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:4] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour4_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]\n",
    "\n",
    "left_neighbour_matrix = np.stack(change_object_dataframe[\"left_token\"].apply(lambda token_set: token_set[::-1][:2] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "right_neighbour_matrix = np.stack(change_object_dataframe[\"right_token\"].apply(lambda token_set: token_set[:2] ).apply(lambda token_set: get_word_vecs(wiki_vec, filtered_vocab_masks_df, token_set, token_string_df)).values)\n",
    "filtered_neighbour2_matrix = np.c_[ left_neighbour_matrix, right_neighbour_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving change object vector  to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vector_dir = \"../data/change_vector/\"\n",
    "change_vec_filename = f\"{article_name}.npz\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_vec_filename)\n",
    "\n",
    "arrays_to_save = {\n",
    "    \"2_clean_not_weighted\": filtered_neighbour2_matrix,\n",
    "    \"4_clean_not_weighted\": filtered_neighbour4_matrix,\n",
    "    \"6_clean_not_weighted\": filtered_neighbour6_matrix,\n",
    "    \"8_clean_not_weighted\": filtered_neighbour8_matrix,\n",
    "    \"10_clean_not_weighted\": filtered_neighbour10_matrix,\n",
    "    \"12_clean_not_weighted\": filtered_neighbour12_matrix,\n",
    "    \"15_clean_not_weighted\": filtered_neighbour15_matrix,\n",
    "    \"20_clean_not_weighted\": filtered_neighbour20_matrix,\n",
    "    \"25_clean_not_weighted\": filtered_neighbour25_matrix,\n",
    "    \"30_clean_not_weighted\": filtered_neighbour30_matrix,\n",
    "}\n",
    "with open(change_vector_file, \"wb\") as file:\n",
    "    np.savez(file, **arrays_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4913, 600)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_neighbour25_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
