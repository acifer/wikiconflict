{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.stats import entropy\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to take weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\", ):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## reading the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ins_start_pos</th>\n",
       "      <th>ins_end_pos</th>\n",
       "      <th>left_neigh</th>\n",
       "      <th>right_neigh</th>\n",
       "      <th>del_start_pos</th>\n",
       "      <th>del_end_pos</th>\n",
       "      <th>ins_tokens</th>\n",
       "      <th>del_tokens</th>\n",
       "      <th>left_neigh_slice</th>\n",
       "      <th>right_neigh_slice</th>\n",
       "      <th>left_token</th>\n",
       "      <th>right_token</th>\n",
       "      <th>ins_length</th>\n",
       "      <th>del_length</th>\n",
       "      <th>del_string_tokens</th>\n",
       "      <th>ins_string_tokens</th>\n",
       "      <th>edit_string_tokens</th>\n",
       "      <th>left_context</th>\n",
       "      <th>right_context</th>\n",
       "      <th>bykau_cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from revision id</th>\n",
       "      <th>to revision id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timegap</th>\n",
       "      <th>editor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">203693</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">203699</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2002-09-08 14:05:32</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">194 days 22:14:17</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3646</th>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 10, None)</td>\n",
       "      <td>slice(10, 41, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)</td>\n",
       "      <td>(9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' '</td>\n",
       "      <td>of scotland ( [[ university of glasgow ]] ) wa...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(62,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 11, None)</td>\n",
       "      <td>slice(11, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</td>\n",
       "      <td>(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of</td>\n",
       "      <td>scotland ( [[ university of glasgow ]] ) was t...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(63,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 12, None)</td>\n",
       "      <td>slice(12, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</td>\n",
       "      <td>(11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland</td>\n",
       "      <td>( [[ university of glasgow ]] ) was the first ...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>(64, 65, 66, 67, 68, 69)</td>\n",
       "      <td>(26, 27, 28, 29, 30)</td>\n",
       "      <td>slice(0, 27, None)</td>\n",
       "      <td>slice(32, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>(31, 32, 33, 34, 35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>((, which, see, ), in)</td>\n",
       "      <td>(,, a, device, he, presented, to)</td>\n",
       "      <td>(,, a, device, he, presented, to, (, which, se...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland...</td>\n",
       "      <td>the mid 1920s ( [[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>(70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...</td>\n",
       "      <td>(32, 33, 34)</td>\n",
       "      <td>slice(2, 33, None)</td>\n",
       "      <td>slice(36, 42, None)</td>\n",
       "      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>(35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>(mid, 1920s, ()</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>' ' john logie baird ' ' ' of scotland ( [[ un...</td>\n",
       "      <td>[[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                ins_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10.0   \n",
       "                                                                             1           32.0   \n",
       "                                                                             2           34.0   \n",
       "                                                                             3           50.0   \n",
       "                                                                             4           57.0   \n",
       "\n",
       "                                                                                ins_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         30.0   \n",
       "                                                                             1         32.0   \n",
       "                                                                             2         34.0   \n",
       "                                                                             3         55.0   \n",
       "                                                                             4         73.0   \n",
       "\n",
       "                                                                                left_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           9   \n",
       "                                                                             1          10   \n",
       "                                                                             2          11   \n",
       "                                                                             3          26   \n",
       "                                                                             4          32   \n",
       "\n",
       "                                                                                right_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10   \n",
       "                                                                             1           11   \n",
       "                                                                             2           12   \n",
       "                                                                             3           32   \n",
       "                                                                             4           36   \n",
       "\n",
       "                                                                                del_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           -1.0   \n",
       "                                                                             1           -1.0   \n",
       "                                                                             2           -1.0   \n",
       "                                                                             3           27.0   \n",
       "                                                                             4           33.0   \n",
       "\n",
       "                                                                                del_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         -1.0   \n",
       "                                                                             1         -1.0   \n",
       "                                                                             2         -1.0   \n",
       "                                                                             3         31.0   \n",
       "                                                                             4         35.0   \n",
       "\n",
       "                                                                                                                       ins_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...   \n",
       "                                                                             1                                              (62,)   \n",
       "                                                                             2                                              (63,)   \n",
       "                                                                             3                           (64, 65, 66, 67, 68, 69)   \n",
       "                                                                             4  (70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...   \n",
       "\n",
       "                                                                                          del_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                           \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    ()   \n",
       "                                                                             1                    ()   \n",
       "                                                                             2                    ()   \n",
       "                                                                             3  (26, 27, 28, 29, 30)   \n",
       "                                                                             4          (32, 33, 34)   \n",
       "\n",
       "                                                                                  left_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                         \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(0, 10, None)   \n",
       "                                                                             1  slice(0, 11, None)   \n",
       "                                                                             2  slice(0, 12, None)   \n",
       "                                                                             3  slice(0, 27, None)   \n",
       "                                                                             4  slice(2, 33, None)   \n",
       "\n",
       "                                                                                  right_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                          \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(10, 41, None)   \n",
       "                                                                             1  slice(11, 42, None)   \n",
       "                                                                             2  slice(12, 42, None)   \n",
       "                                                                             3  slice(32, 42, None)   \n",
       "                                                                             4  slice(36, 42, None)   \n",
       "\n",
       "                                                                                                                       left_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)   \n",
       "                                                                             1                 (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)   \n",
       "                                                                             2             (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)   \n",
       "                                                                             3  (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "                                                                             4  (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                                                                                                      right_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...   \n",
       "                                                                             1  (10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...   \n",
       "                                                                             2  (11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...   \n",
       "                                                                             3           (31, 32, 33, 34, 35, 36, 37, 38, 39, 40)   \n",
       "                                                                             4                           (35, 36, 37, 38, 39, 40)   \n",
       "\n",
       "                                                                                ins_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0          21   \n",
       "                                                                             1           1   \n",
       "                                                                             2           1   \n",
       "                                                                             3           6   \n",
       "                                                                             4          17   \n",
       "\n",
       "                                                                                del_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           0   \n",
       "                                                                             1           0   \n",
       "                                                                             2           0   \n",
       "                                                                             3           5   \n",
       "                                                                             4           3   \n",
       "\n",
       "                                                                                     del_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                             \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                      ()   \n",
       "                                                                             1                      ()   \n",
       "                                                                             2                      ()   \n",
       "                                                                             3  ((, which, see, ), in)   \n",
       "                                                                             4         (mid, 1920s, ()   \n",
       "\n",
       "                                                                                                                ins_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3                  (,, a, device, he, presented, to)   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                               edit_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3  (,, a, device, he, presented, to, (, which, se...   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                                     left_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                 St@rt ' ' ' john logie baird ' ' '   \n",
       "                                                                             1              St@rt ' ' ' john logie baird ' ' ' of   \n",
       "                                                                             2     St@rt ' ' ' john logie baird ' ' ' of scotland   \n",
       "                                                                             3  St@rt ' ' ' john logie baird ' ' ' of scotland...   \n",
       "                                                                             4  ' ' john logie baird ' ' ' of scotland ( [[ un...   \n",
       "\n",
       "                                                                                                                    right_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  of scotland ( [[ university of glasgow ]] ) wa...   \n",
       "                                                                             1  scotland ( [[ university of glasgow ]] ) was t...   \n",
       "                                                                             2  ( [[ university of glasgow ]] ) was the first ...   \n",
       "                                                                             3                   the mid 1920s ( [[ 1926 ]] ? ) .   \n",
       "                                                                             4                                   [[ 1926 ]] ? ) .   \n",
       "\n",
       "                                                                                bykau_cluster  \n",
       "from revision id to revision id timestamp           timegap           editor                   \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0            -99  \n",
       "                                                                             1            -99  \n",
       "                                                                             2            -99  \n",
       "                                                                             3            -99  \n",
       "                                                                             4            -99  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "content_dir = \"../data/content/\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    token_string_df = store.get(\"all_tokens\")\n",
    "\n",
    "token_string_df = token_string_df.set_index(\"token_id\")[\"str\"]\n",
    "token_string_df[-1] = \"St@rt\"\n",
    "token_string_df[-2] = \"$nd\"\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "change_object_dataframe.shape\n",
    "\n",
    "\n",
    "change_object_dataframe[\"ins_length\"] = change_object_dataframe[\"ins_tokens\"].str.len()\n",
    "change_object_dataframe[\"del_length\"] = change_object_dataframe[\"del_tokens\"].str.len()\n",
    "\n",
    "change_object_dataframe[\"del_string_tokens\"] = change_object_dataframe[\"del_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"ins_string_tokens\"] = change_object_dataframe[\"ins_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "change_object_dataframe[\"edit_string_tokens\"] = change_object_dataframe[\"ins_string_tokens\"] + change_object_dataframe[\"del_string_tokens\"]\n",
    "\n",
    "\n",
    "\n",
    "change_object_dataframe[\"left_context\"] = change_object_dataframe[\"left_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "\n",
    "change_object_dataframe[\"right_context\"] = change_object_dataframe[\"right_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "change_object_dataframe[\"bykau_cluster\"] = pd.Series(-99,index=change_object_dataframe.index)\n",
    "\n",
    "# change_object_dataframe = change_object_dataframe[[\"left_context\",\"del_string_tokens\",\"ins_string_tokens\", \"right_context\", \n",
    "#                                                    \"ins_length\", \"del_length\", \"bykau_cluster\" ]]\n",
    "change_object_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ins_and_del = change_object_dataframe[(change_object_dataframe[\"ins_string_tokens\"]!=()) & (change_object_dataframe[\"del_string_tokens\"]!=())]\n",
    "display(ins_and_del.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing change object with insert or delete token size more than five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_ins_and_del = ins_and_del[~((ins_and_del[\"ins_length\"] >5 ) | (ins_and_del[\"del_length\"] >5) )]\n",
    "reduced_ins_and_del.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ins_string_tokens</th>\n",
       "      <th>del_string_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(2004,)</td>\n",
       "      <td>(1929,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>(2010,)</td>\n",
       "      <td>(2012,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>([[, lomond, school, ]])</td>\n",
       "      <td>(lomond, school)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ins_string_tokens del_string_tokens\n",
       "11                    (2004,)           (1929,)\n",
       "906                   (2010,)           (2012,)\n",
       "511  ([[, lomond, school, ]])  (lomond, school)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_ins_and_del.reset_index(drop=True)[[\"ins_string_tokens\", \"del_string_tokens\"]].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low user support tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_change_object = reduced_ins_and_del.groupby(\"ins_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object = bykau_change_object.groupby(\"del_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = article_name + \"_FULL.csv\"\n",
    "annotation_dir = \"../data/annotation/\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "annotation_df = pd.read_csv(full_file_path)\n",
    "annotation_df = annotation_df[[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\", \"nationality\", \"birth_place\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = article_name + \"_FULL.csv\"\n",
    "# annotation_dir = \"../data/annotation/\"\n",
    "# full_file_path = os.path.join(annotation_dir, file_name)\n",
    "# annotation_df = pd.read_csv(full_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining jaccard similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_distances(dataframe, context = 8):\n",
    "    left_neighbours = dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-context:])\n",
    "    right_neighbours = dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:context])\n",
    "    \n",
    "    neighbour_tokens = left_neighbours + right_neighbours\n",
    "    neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "    \n",
    "    neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "    return pairwise_distances(neighbour_vec, metric=\"jaccard\")\n",
    "#     db = DBSCAN(eps=eps, min_samples=min_samples, metric='jaccard').fit(neighbour_vec)\n",
    "#     return db.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_cluster(distances, eps=0.75, min_samples=5):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit(distances)\n",
    "    return db.labels_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# r_threshold = 8\n",
    "# cutoff_threshold = 0.75\n",
    "# # edit_tokens = change_object_dataframe[\"ins_tokens\"] + change_object_dataframe[\"del_tokens\"]\n",
    "# left_neighbours = change_object_dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-r_threshold:])\n",
    "# right_neighbours = change_object_dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:r_threshold])\n",
    "# neighbour_tokens = left_neighbours + right_neighbours\n",
    "\n",
    "# # bykau_change_object[\"edit_tokens\"] = edit_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "\n",
    "# db = DBSCAN(eps=cutoff_threshold, min_samples=5, metric='jaccard').fit(neighbour_vec)\n",
    "# # change_object_dataframe.loc[change_object_dataframe.index,\"bykau_cluster\"] = db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving change object and its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bykau_dir =  \"../data/bykau_change_object/\"\n",
    "# filename =  f\"{article_name}_without_optimization.h5\"\n",
    "\n",
    "# change_object_file = os.path.join(bykau_dir, filename)\n",
    "# with pd.HDFStore(change_object_file, 'w') as store:\n",
    "#     store.put(\"data\", change_object_dataframe[\"bykau_cluster\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment for seeing the content of each cluster in an ipywidget User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repers_first =  change_object_dataframe.groupby(\"bykau_cluster\")[[\"left_context\",\"del_string_tokens\", \"ins_string_tokens\", \"right_context\"]].apply(lambda x: x.style.render())\n",
    "# @interact( clusters_html=fixed(repers_first), group=change_object_dataframe[\"bykau_cluster\"].unique().tolist())\n",
    "# def display_clusters(clusters_html, group):\n",
    "#      return display(HTML(clusters_html.loc[group]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making annotation file with cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_in_gap(ann, gap):\n",
    "#     context_gap = gap.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "#     clusters = context_gap.loc[ context_gap[\"token_id\"].apply(\n",
    "#             lambda x: ann[\"token_id\"] in x), [\"bykau_cluster\"]].values\n",
    "#     if clusters.size >0:\n",
    "#             clusters = pd.Series(clusters[0],index=[\"bykau_cluster\",])\n",
    "#     else:\n",
    "#         clusters = pd.Series([-10], index=[\"bykau_cluster\",])\n",
    "#     return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap_df, gap_cluster_df):\n",
    "    context_gap = gap_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    context_cluster = gap_cluster_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    clusters = context_cluster.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x),:].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=gap_cluster_df.columns)\n",
    "    else:\n",
    "        clusters = pd.Series(-10, index=gap_cluster_df.columns)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bykau(change_object_dataframe, annotation_df):\n",
    "    ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id', \"bykau_cluster\"]].values\n",
    "\n",
    "    # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "    del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "    gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "    gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "    gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "    \n",
    "    annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "    nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "    nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "    annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "    evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "    evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "    evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "    return evaluation_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = 2 \n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "\n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "\n",
    "bykau_cluster_df = pd.DataFrame(columns=idx)\n",
    "\n",
    "bykau_evaluation_df = pd.DataFrame(index=idx, columns=[\"rand\", \"entropy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for context in context_array:\n",
    "    distances = bykau_distances(bykau_change_object, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_cluster_df[context,eps] = pd.Series(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = bykau_change_object.reset_index().loc[\n",
    "    bykau_change_object[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_cluster_df.loc[\n",
    "    bykau_change_object[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = bykau_change_object.reset_index().loc[\n",
    "    bykau_change_object[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_cluster_df.loc[\n",
    "    bykau_change_object[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "true_labels = np.zeros((annotation_df.shape[0]))\n",
    "true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps in all_combinations:\n",
    "    bykau_evaluation_df.loc[(context, eps),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "    bykau_evaluation_df.loc[(context, eps),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "15       0.001    16.6146\n",
       "         0.025    16.6146\n",
       "         0.100    16.9121\n",
       "30       0.001    17.0722\n",
       "         0.025    17.4926\n",
       "         0.050    17.8787\n",
       "8        0.050    18.3545\n",
       "         0.001    18.3545\n",
       "         0.025    18.3545\n",
       "15       0.050    18.5134\n",
       "8        0.100    18.7685\n",
       "4        0.050    18.7923\n",
       "         0.001    18.7923\n",
       "         0.100    18.7923\n",
       "         0.025    18.7923\n",
       "         0.200    19.9241\n",
       "30       0.100    20.3486\n",
       "2        0.001    21.0485\n",
       "         0.025    21.0485\n",
       "         0.050    21.0485\n",
       "         0.100    21.0485\n",
       "         0.200    21.0485\n",
       "8        0.200    21.0932\n",
       "15       0.200    21.6851\n",
       "30       0.200    24.4529\n",
       "15       0.400    34.5426\n",
       "8        0.400    36.4751\n",
       "30       0.400    37.1242\n",
       "4        0.400    41.6736\n",
       "2        0.400    54.8317\n",
       "15       0.800    181.064\n",
       "2        0.800    183.683\n",
       "4        0.800    184.524\n",
       "8        0.800    370.204\n",
       "30       0.800    370.204\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = [2] \n",
    "# all_combinations = list(itertools.product(context_array, eps_array))\n",
    "all_combinations_without_optimization = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx_without_optimization  = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "# idx_without_optimization = pd.MultiIndex.from_product([context_array, eps_array, min_samples_array],\n",
    "#                                 names=[\"context\",\"eps\", \"min_samples\"])\n",
    "bykau_evaluation_without_optimization = pd.DataFrame(index=idx_without_optimization, \n",
    "                                                     columns=[\"rand\", \"entropy\"])\n",
    "\n",
    "\n",
    "\n",
    "bykau_without_optimization_cluster_df = pd.DataFrame(columns=idx_without_optimization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 10s, sys: 21.6 s, total: 2min 32s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for context in context_array:\n",
    "    distances = bykau_distances(change_object_dataframe, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_without_optimization_cluster_df[context,eps] = pd.Series(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# true_labels = np.zeros((annotation_df.shape[0]))\n",
    "# true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "# annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for context, eps, min_samples in all_combinations_without_optimization:\n",
    "#     print(f\"processing eps {eps}, min samples {min_samples} and context {context}\")\n",
    "#     change_object_dataframe.loc[change_object_dataframe.index,\n",
    "#                             \"bykau_cluster\"] = bykau_cluster(change_object_dataframe, \n",
    "#                                                              context=int(context), eps=eps, min_samples=min_samples)\n",
    "#     bykau_evaluation_without_optimization.loc[context, eps, \n",
    "#                             min_samples] =evaluate_bykau(change_object_dataframe, annotation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eps    context\n",
       "0.100  30         1.70298\n",
       "0.200  15         1.76482\n",
       "0.100  15         1.87732\n",
       "0.001  2          1.90188\n",
       "0.025  2          1.90188\n",
       "0.050  2          1.90188\n",
       "0.100  2          1.90188\n",
       "0.200  2          1.90188\n",
       "       30         1.92995\n",
       "       8          1.95816\n",
       "0.050  30         2.07604\n",
       "0.001  4          2.11921\n",
       "0.025  4          2.11921\n",
       "0.050  4          2.11921\n",
       "0.100  4          2.11921\n",
       "0.200  4          2.17222\n",
       "0.100  8          2.48144\n",
       "0.050  15         2.51455\n",
       "0.025  30         2.57668\n",
       "       8           2.6191\n",
       "0.001  8           2.6191\n",
       "0.050  8           2.6191\n",
       "0.001  30         2.65478\n",
       "       15         2.75388\n",
       "0.025  15         2.75388\n",
       "0.400  8          4.49188\n",
       "       4          5.47147\n",
       "       15         5.85951\n",
       "       30         9.00075\n",
       "       2          9.07192\n",
       "0.800  2          262.476\n",
       "       8          350.297\n",
       "       4          350.394\n",
       "       15         525.609\n",
       "       30         525.609\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:200]\n",
    "\n",
    "bykau_evaluation_without_optimization.reset_index().set_index([\"eps\", \"context\"])[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insert array is always done in to revision so taking it and leaving other change object where \n",
    "# ins_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "#                   [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "\n",
    "# # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "# del_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "#                   [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "# gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "# gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "# gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "# annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "\n",
    "# nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "# evaluation_score\n",
    "\n",
    "# both_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# both_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# both_cluster[annotation_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "# annotation_df[\"both_cluster\"] = both_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\", ])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], both_cluster)\n",
    "\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"both_cluster\")\n",
    "\n",
    "# evaluation_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_cluster_df.index = bykau_change_object.index\n",
    "\n",
    "bykau_optimised = pd.concat([change_object_dataframe, bykau_without_optimization_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_series_with_optimization = pd.Series(index=all_combinations)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_df.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(bykau_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "#     entropy_series_with_optimization[context_eps_tuple]= weighted_token_entropy(bykau_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_with_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_without_optimization_cluster_df.index = change_object_dataframe.index\n",
    "\n",
    "without_optimised = pd.concat([change_object_dataframe, bykau_without_optimization_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization = pd.Series(index=all_combinations_without_optimization)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(without_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "\n",
    "#     entropy_series_without_optimization[context_eps_tuple]= weighted_token_entropy(without_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <th>eps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.0358306</td>\n",
       "      <td>1.90188</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.0358306</td>\n",
       "      <td>1.90188</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0358306</td>\n",
       "      <td>1.90188</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0358306</td>\n",
       "      <td>1.90188</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0358306</td>\n",
       "      <td>1.90188</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rand  entropy  token_entropy\n",
       "context eps                                     \n",
       "2       0.001  0.0358306  1.90188   17480.153219\n",
       "        0.025  0.0358306  1.90188   17480.153219\n",
       "        0.050  0.0358306  1.90188   17480.153219\n",
       "        0.100  0.0358306  1.90188   17480.153219\n",
       "        0.200  0.0358306  1.90188   17480.153219"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <th>eps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.437664</td>\n",
       "      <td>21.0485</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.437664</td>\n",
       "      <td>21.0485</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.437664</td>\n",
       "      <td>21.0485</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.437664</td>\n",
       "      <td>21.0485</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.437664</td>\n",
       "      <td>21.0485</td>\n",
       "      <td>17480.153219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand  entropy  token_entropy\n",
       "context eps                                    \n",
       "2       0.001  0.437664  21.0485   17480.153219\n",
       "        0.025  0.437664  21.0485   17480.153219\n",
       "        0.050  0.437664  21.0485   17480.153219\n",
       "        0.100  0.437664  21.0485   17480.153219\n",
       "        0.200  0.437664  21.0485   17480.153219"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910926</td>\n",
       "      <td>0.924432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.910926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_entropy</th>\n",
       "      <td>0.924432</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand   entropy  token_entropy\n",
       "rand           1.000000  0.910926       0.924432\n",
       "entropy        0.910926  1.000000       0.855127\n",
       "token_entropy  0.924432  0.855127       1.000000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.380560</td>\n",
       "      <td>-0.647538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>-0.380560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_entropy</th>\n",
       "      <td>-0.647538</td>\n",
       "      <td>0.882049</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand   entropy  token_entropy\n",
       "rand           1.000000 -0.380560      -0.647538\n",
       "entropy       -0.380560  1.000000       0.882049\n",
       "token_entropy -0.647538  0.882049       1.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
