{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation for clusters of an article \n",
    " This notebook Visualises and compares change object as clustered according to **Bykau et. al** and using **Word2vec** for an article. Given an **article_name** it shows creation and deletion portion of each clusters along with showing revision_id editor and time as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from string import whitespace\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, HBox\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)\n",
    "# vocab_list = list(wiki_vec.vocab)\n",
    "# filtered_vocab = [ t for t in vocab_list[20:] if len(t) > 3]\n",
    "\n",
    "# display(f\"lenght of vocabulary is {len(vocab_list)} words\")\n",
    "# display(f\"lenght of vocabulary is {len(filtered_vocab)} words\")\n",
    "\n",
    "# vocab_list = np.array(vocab_list)\n",
    "# filtered_vocab = np.array(filtered_vocab)\n",
    "# with open(\"../../wordvectors/vocabs.pkl\", \"wb\") as file:\n",
    "# #     print(file)\n",
    "#     pickle.dump(vocab_list, file)\n",
    "# with open(\"../../wordvectors/filtered_vocabs.pkl\", \"wb\") as file:\n",
    "# #     print(file)\n",
    "#     pickle.dump(filtered_vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_style =     [\n",
    "    {'selector': 'table', 'props': [('border', \"6px double #696969\")]},\n",
    "    {'selector': 'th', 'props': [('border', \"2px solid #D3D3D3\"), (\"font-size\", \"100%\")]},\n",
    "    {\"selector\":\".data\", \"props\":[(\"text-align\", \"justify\"), ('border', \"1px solid #000\"), ('margin', '4px 24px 4px 24px' ), (\"font-size\", \"8pt\")]}\n",
    "] \n",
    "\n",
    "deleted_token_style = {\"color\":\"red\", \"font-weight\": \"bold\",\"font-size\": \"100px\"}\n",
    "inserted_token_style = {\"color\":\"blue\", \"font-weight\": \"bold\",\"font-size\": \"100px\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../wordvectors/vocabs.pkl\", \"rb\") as file:\n",
    "    vocab = pickle.load(file)\n",
    "vocab_set = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting the vlaue of the article to visualise data.\n",
    "article_name = \"Yugoslavia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first read and visualise the clusters done using vectors prepeared by Using pre trained word2vec vector.\n",
    "\n",
    "Word tokens in change objects are converted into vector space by using 300 dimensions of pre-trained fast text pre trained embedding vectors. Size of the vocobulary loaded is 1 million words.\n",
    "\n",
    "Each change object is represented by concatinating  vectors representing left neighbour tokens, insert and delete tokens and right neighbour tokens. Insert and delete tokens vectors is average of fast-text vector representation of each words in token. Left and right neighbours are prepared by taking weighted average of vectors representated by word2vec embeddings. Weights are created by a exponential decay functions whose parameter is  neighbouring token's distance from inserted and deleted word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading change object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11960, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_object_dir = \"../data/change objects/\"\n",
    "content_file = article_name + \"_change.h5\"\n",
    "change_object_path = os.path.join(change_object_dir, content_file)\n",
    "\n",
    "with pd.HDFStore(change_object_path, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    change_df = store.get(\"data\")\n",
    "change_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bigger change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10492, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_df[\"ins_length\"]= change_df[\"ins_tokens\"].apply(lambda x: len(x))\n",
    "change_df[\"del_length\"]= change_df[\"del_tokens\"].apply(lambda x: len(x))\n",
    "\n",
    "optimised_change_object_mask = ((change_df[\"ins_length\"] <= 20 ) & (change_df[\"del_length\"] <= 20))\n",
    "\n",
    "optimised_df = change_df[optimised_change_object_mask].copy()\n",
    "\n",
    "optimised_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df[\"ins_token_len\"] = change_df[\"ins_tokens\"].str.len()\n",
    "change_df[\"del_token_len\"] = change_df[\"del_tokens\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading revision clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dir = \"../data/clusters/\"\n",
    "file_name = article_name + \"_optimised_cluster.h5\"\n",
    "\n",
    "change_dataframe_path = os.path.join(cluster_dir,file_name)\n",
    "\n",
    "with pd.HDFStore(change_dataframe_path, 'r') as store:\n",
    "    optimised_cluster_df = store.get(\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_dir = \"../data/clusters/\"\n",
    "# file_name = f\"{article_name}_cluster.h5\"\n",
    "# change_dataframe_path = os.path.join(cluster_dir,file_name)\n",
    "\n",
    "# with pd.HDFStore(change_dataframe_path, 'r') as store:\n",
    "#     cluster_df = store.get(\"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging change object with its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change_df_with_clusters = pd.concat([change_df, optimised_cluster_df], axis=1)\n",
    "# change_df_with_clusters[\"edited_tokens\"] = change_df_with_clusters[\"ins_tokens\"] + change_df_with_clusters[\"del_tokens\"]\n",
    "\n",
    "\n",
    "optimised_df_with_clusters = pd.concat([optimised_df, optimised_cluster_df], axis=1)\n",
    "optimised_df_with_clusters[\"edited_tokens\"] = optimised_df_with_clusters[\"ins_tokens\"] + optimised_df_with_clusters[\"del_tokens\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make left, ins and delete string for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimised_df_with_clusters[\"left_string\"] = optimised_df_with_clusters[\"left_token\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "# optimised_df_with_clusters[\"del_string\"] = optimised_df_with_clusters[\"ins_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "# optimised_df_with_clusters[\"ins_string\"] = optimised_df_with_clusters[\"del_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "# optimised_df_with_clusters[\"right_string\"] = optimised_df_with_clusters[\"right_token\"].apply(lambda tokens: tuple(token for token in tokens if token.isalnum())).str.join(\" \")\n",
    "\n",
    "optimised_df_with_clusters[\"left_string\"] = optimised_df_with_clusters[\"left_token\"].str.join(\" \")\n",
    "optimised_df_with_clusters[\"ins_string\"] = optimised_df_with_clusters[\"ins_tokens\"].str.join(\" \")\n",
    "optimised_df_with_clusters[\"del_string\"] = optimised_df_with_clusters[\"del_tokens\"].str.join(\" \")\n",
    "optimised_df_with_clusters[\"right_string\"] = optimised_df_with_clusters[\"right_token\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding relative positions of change object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dir = \"../data/content/\"\n",
    "content_file = article_name + \".h5\"\n",
    "content_path = os.path.join(content_dir, content_file)\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "rev_len_df = pd.read_hdf(len_file_path, key = \"rev_len\")\n",
    "\n",
    "\n",
    "\n",
    "# with pd.HDFStore(content_path, 'r') as store:\n",
    "#     #retrieving all rev list and change object from file\n",
    "#     rev_list = store.get(\"rev_list\")[\"id\"].values.tolist()\n",
    "#     keys = [\"r\" +  str(rev) for rev in rev_list]\n",
    "#     rev_len_list = [store.get(key).shape[0] for key in keys]\n",
    "\n",
    "# rev_len_df = pd.DataFrame({\"rev_id\":rev_list[:-1], \"length\": rev_len_list[:-1]})\n",
    "# rev_len_df.to_hdf(len_file_path, \"rev_len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_df_with_clusters = optimised_df_with_clusters.reset_index().set_index('from revision id')\n",
    "optimised_df_with_clusters = optimised_df_with_clusters.join(rev_len_df.set_index(\"rev_id\"))\n",
    "optimised_df_with_clusters.index.name = \"from revision id\"\n",
    "\n",
    "optimised_df_with_clusters[\"relative_position\"] =(optimised_df_with_clusters[\"left_neigh\"]+1)/(optimised_df_with_clusters[\"length\"])\n",
    "\n",
    "\n",
    "# plt.scatter(np.arange(optimised_df_with_clusters[\"relative_position\"].shape[0])+1,  optimised_df_with_clusters[\"relative_position\"], linestyle=\"-\")\n",
    "# plt.xscale(\"log\")\n",
    "# plt.ylim([0,1])\n",
    "# plt.xlim([1,optimised_df_with_clusters[\"relative_position\"].shape[0]+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping clusters and making html of each groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_options =['dbscan_weighted_neighbour', 'dbscan_weighted_all',\n",
    "       'cluster_weighted_neighbour', 'cluster_4', 'cluster_10',\n",
    "       'cluster_4_full', 'cluster_4_weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering_by = \"dbscan_weighted_neighbour\"\n",
    "clustering_by = \"cluster_10\"\n",
    "\n",
    "non_zero_cluster_mask = (optimised_df_with_clusters[clustering_by] != -1)\n",
    "\n",
    "\n",
    "\n",
    "edited_tokens_freq_per_group = optimised_df_with_clusters.set_index(clustering_by)[\"edited_tokens\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(clustering_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "left_context_freq_per_group = optimised_df_with_clusters.set_index(clustering_by)[\"left_token\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(clustering_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "right_context_freq_per_group = optimised_df_with_clusters.set_index(clustering_by)[\"right_token\"].apply(lambda tokens: tuple(token for token in tokens if token in vocab_set)).groupby(clustering_by).apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "\n",
    "optimised_df_with_clusters = optimised_df_with_clusters.reset_index().set_index([\"from revision id\",\"to revision id\",\"timestamp\", \"editor\", \"level_5\"])\n",
    "\n",
    "repers_weighted =  optimised_df_with_clusters.groupby(clustering_by)[[\"relative_position\",\"left_string\", \"del_string\", \"ins_string\", \"right_string\"]].apply(lambda x: x.style.render())\n",
    "optimised_df_with_clusters = optimised_df_with_clusters.reset_index().set_index([\"from revision id\", \"level_5\"])\n",
    "\n",
    "change_grouped_by_tokens = optimised_df_with_clusters.groupby(clustering_by)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking\n",
    "###### Ranking clustered groups on following parameters.\n",
    "1. Size of clusters\n",
    "2. No of unique editors is clusters\n",
    "3. Total period of cluster. i.e difference between start and end date.\n",
    "4. Mean timegap in cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_by_size = change_grouped_by_tokens.size().sort_values(ascending=False)\n",
    "\n",
    "rank_by_uniq_editor = optimised_df_with_clusters.reset_index().groupby(clustering_by)[\"editor\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "rank_by_period = optimised_df_with_clusters.reset_index().groupby(clustering_by)[\"timestamp\"].apply(lambda x: x.max() - x.min()).sort_values(ascending=False)\n",
    "\n",
    "rank_by_rate = optimised_df_with_clusters.reset_index().groupby(clustering_by)[\"timegap\"].apply(lambda x: x.mean()).sort_values(ascending=False)\n",
    "\n",
    "# unique_word_count_per_group = word_freq_per_group.groupby(\"cluster_4_weighted\").apply(lambda x: x.index.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation for change object clustered on neighbour vectors\n",
    "\n",
    "Following visualisation can be used to compare with above visualisation which is using neighbour vectors to cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq(gap_freq, left_context_freq, right_context_freq, timestamp, relative_position, count = 20):\n",
    "    \n",
    "    fig2, ax = plt.subplots(nrows=1, ncols=1,figsize=(30, 7))\n",
    "    ax.scatter( np.arange(relative_position.shape[0])+1, relative_position, c=\"red\",marker=\"D\", label = \"relative position with respect to timestamp\")\n",
    "    ax.set_title(\"Time scale invariant Plot of timestamp with relative position\")\n",
    "    ax.set_xlabel(\"Position with respect to time\")\n",
    "    ax.set_ylabel(\"relative position \")\n",
    "    ax.set_xticklabels(timestamp)\n",
    "    plt.ylim(0, 1)\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3,figsize=(30, 10))\n",
    "        \n",
    "    axs[0].barh( left_context_freq.index[:count][::-1], left_context_freq.values[:count][::-1])\n",
    "    axs[0].set_title(\" frequency plot of top 100 words in left context\")\n",
    "    axs[0].set_xlabel(\"frequency\")\n",
    "    axs[0].set_ylabel(\"unique words in left context \")\n",
    "    \n",
    "    axs[1].barh( gap_freq.index[:count][::-1], gap_freq.values[:count][::-1])\n",
    "    axs[1].set_title(\" frequency plot of top 100 words in gap\")\n",
    "    axs[1].set_xlabel(\"frequency\")\n",
    "    axs[1].set_ylabel(\"unique words in gap \")\n",
    "    \n",
    "    axs[2].barh( right_context_freq.index[:count][::-1], right_context_freq.values[:count][::-1])\n",
    "    axs[2].set_title(\" frequency plot of top 100 words in right context\")\n",
    "    axs[2].set_xlabel(\"frequency\")\n",
    "    axs[2].set_ylabel(\"unique words in right context \")\n",
    "#     axs[3].set_xscale(\"log\")\n",
    "#     axs[3].set_yscale(\"log\")\n",
    "\n",
    "\n",
    "    return fig\n",
    "# _= plot_freq(edited_tokens_freq_per_group.loc[1], \n",
    "#             left_context_freq_per_group.loc[1], \n",
    "#             right_context_freq_per_group.loc[1],\n",
    "#             change_grouped_by_tokens[\"timestamp\"].get_group(2).values,\n",
    "#             change_grouped_by_tokens[\"relative_position\"].get_group(2).values\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_article_content(index, change_html_series, edited_tokens_freq_per_group, left_context_freq_per_group, right_context_freq_per_group, change_grouped_by_tokens, out):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "    if index in change_html_series.index:\n",
    "        change_html = change_html_series.loc[index]\n",
    "    else:\n",
    "        change_html = \"<p>empty table according to cleanup</p>\"\n",
    "    \n",
    "    if index in left_context_freq_per_group.index:\n",
    "        left_context_freq = left_context_freq_per_group.loc[index]\n",
    "    else:\n",
    "        left_context_freq = pd.Series()\n",
    "    \n",
    "    if index in right_context_freq_per_group.index:\n",
    "        right_context_freq = right_context_freq_per_group.loc[index]\n",
    "    else:\n",
    "        right_context_freq = pd.Series()\n",
    "        \n",
    "    if index in edited_tokens_freq_per_group.index:\n",
    "        edited_tokens_freq = edited_tokens_freq_per_group.loc[index]\n",
    "    else:\n",
    "        edited_tokens_freq = pd.Series()\n",
    "    _ = plot_freq(edited_tokens_freq, left_context_freq, right_context_freq,\n",
    "            change_grouped_by_tokens[\"timestamp\"].get_group(index).values,\n",
    "            change_grouped_by_tokens[\"relative_position\"].get_group(index).values)\n",
    "    with out:\n",
    "#         display(change_html)\n",
    "        display(f\"Word length distribution for {index}\")\n",
    "#         display(fig)\n",
    "        display(HTML(change_html))\n",
    "#     return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_down = list(zip(np.arange(rank_by_rate.size), rank_by_rate.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please rerun next cell each time page is reloaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831ef9321fee429194d33a64e089e9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 81), (1, 73), (2, 41), (3, 47), (4, 29), (5, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outp = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "_=widgets.interact(display_article_content, index=drop_down, change_html_series= widgets.fixed(repers_weighted),edited_tokens_freq_per_group= widgets.fixed(edited_tokens_freq_per_group), left_context_freq_per_group= widgets.fixed(left_context_freq_per_group), right_context_freq_per_group= widgets.fixed(right_context_freq_per_group), change_grouped_by_tokens= widgets.fixed(change_grouped_by_tokens), out=widgets.fixed(outp));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b2b1f4d1da4393940e36b7e29bf96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# widgets.Dropdown(\n",
    "#     options=['1', '2', '3'],\n",
    "#     value='2',\n",
    "#     description='Number:',\n",
    "#     disabled=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Bykau Et. al.** change objects and its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331, 22)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_dir =  \"../data/bykau_change_object/\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(bykau_dir, filename)\n",
    "bykau_change_df = pd.read_hdf(change_object_file, key=\"data\")\n",
    "bykau_change_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_change_df = bykau_change_df.reset_index().set_index('from revision id')\n",
    "bykau_change_df = bykau_change_df.join(rev_len_df.set_index(\"rev_id\"))\n",
    "bykau_change_df.index.name = \"from revision id\"\n",
    "bykau_change_df[\"relative_position\"] =(bykau_change_df[\"left_neigh\"]+1)/(bykau_change_df[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_change_df[\"left_string\"] = bykau_change_df[\"left_token\"].str.join(\" \")\n",
    "bykau_change_df[\"ins_string\"] = bykau_change_df[\"ins_tokens\"].str.join(\" \")\n",
    "bykau_change_df[\"del_string\"] = bykau_change_df[\"del_tokens\"].str.join(\" \")\n",
    "bykau_change_df[\"right_string\"] = bykau_change_df[\"right_token\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping and ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_groups = bykau_change_df.groupby(\"reclustered_group\")\n",
    "\n",
    "# bykau_edited_freq = bykau_groups[\"ins_tokens\"].apply(lambda x: find_freq_vocab_words(x, vocab))\n",
    "# bykau_left_context_freq = bykau_groups[\"left_token\"].apply(lambda x: find_freq_vocab_words(x, vocab))\n",
    "# bykau_right_context_freq = bykau_groups[\"right_token\"].apply(lambda x: find_freq_vocab_words(x, vocab))\n",
    "\n",
    "bykau_edited_freq = bykau_groups[\"ins_tokens\"].apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "bykau_left_context_freq = bykau_groups[\"left_token\"].apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n",
    "bykau_right_context_freq = bykau_groups[\"right_token\"].apply(lambda x:  pd.Series(np.concatenate(x.values, axis=0)).value_counts(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_rank_by_size = bykau_groups.size().sort_values(ascending=False)\n",
    "\n",
    "bykau_rank_by_uniq_editor = bykau_change_df.reset_index().groupby(\"reclustered_group\")[\"editor\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "bykau_rank_by_period = bykau_change_df.reset_index().groupby(\"reclustered_group\")[\"timestamp\"].apply(lambda x: x.max() - x.min()).sort_values(ascending=False)\n",
    "\n",
    "bykau_rank_by_rate = bykau_change_df.reset_index().groupby(\"reclustered_group\")[\"timegap\"].apply(lambda x: x.mean()).sort_values(ascending=False)\n",
    "\n",
    "bykau_change_df = bykau_change_df.reset_index().set_index([\"from revision id\", \"level_5\"])\n",
    "\n",
    "\n",
    "# unique_word_count_per_group = word_freq_per_group.groupby(\"cluster_4_weighted\").apply(lambda x: x.index.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bykau Visualisation\n",
    "\n",
    "THis can be used to compare with our visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_drop_down = list(zip(np.arange(bykau_rank_by_period.size), bykau_rank_by_period.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "repers_bykau = bykau_change_df.groupby(\"reclustered_group\")[[\"left_string\", \"del_string\", \"ins_string\", \"right_string\"]].apply(lambda x: x.style.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please rerun next cell each time page is reloaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac7a2c3cb3f4c42b68c62b64efe04ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='index', options=((0, 1.0), (1, 0.0), (2, 2.0), (3, 3.0)), value=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bykau_outp = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "# _=widgets.interact(display_article_content, index=drop_down, change_html_series= widgets.fixed(repers_bykau), out=widgets.fixed(bykau_outp));\n",
    "\n",
    "_=widgets.interact(display_article_content, index=bykau_drop_down, change_html_series= widgets.fixed(repers_bykau),edited_tokens_freq_per_group= widgets.fixed(bykau_edited_freq), left_context_freq_per_group= widgets.fixed(bykau_left_context_freq), right_context_freq_per_group= widgets.fixed(bykau_right_context_freq), change_grouped_by_tokens= widgets.fixed(bykau_change_df.groupby(\"reclustered_group\")), out=widgets.fixed(bykau_outp));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0fbe27ccea47f0b1c2057b43e832e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bykau_outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact( clusters_html=fixed(repers_4_neigh), group=range(groups.ngroups))\n",
    "# def display_clusters(clusters_html, group):\n",
    "#      return display(HTML(clusters_html.iloc[group]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_down = list(zip(rev_list.id, rev_list.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
