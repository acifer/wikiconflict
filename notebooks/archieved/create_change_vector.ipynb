{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 50s, sys: 2min 57s, total: 9min 48s\n",
      "Wall time: 9min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'the',\n",
       " '</s>',\n",
       " 'of',\n",
       " '-',\n",
       " 'in',\n",
       " 'and',\n",
       " \"'\",\n",
       " ')',\n",
       " '(',\n",
       " 'to',\n",
       " 'a',\n",
       " 'is',\n",
       " 'was',\n",
       " 'on',\n",
       " 's',\n",
       " 'for',\n",
       " 'as',\n",
       " 'by']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = list(wiki_vec.vocab)\n",
    "vocab_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= np.array([v.count for v in list(wiki_vec.vocab.values())])\n",
    "# vocab[0]\n",
    "filtered_vocab = [ t for t in vocab_list[20:] if len(t) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lenght of vocabulary is 970137 words'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'with',\n",
       " 'from',\n",
       " 'this',\n",
       " 'talk',\n",
       " 'which',\n",
       " 'also',\n",
       " 'were',\n",
       " 'have',\n",
       " 'first',\n",
       " 'page',\n",
       " 'they',\n",
       " 'article',\n",
       " 'their',\n",
       " 'there',\n",
       " 'been',\n",
       " 'made',\n",
       " 'people',\n",
       " 'after',\n",
       " 'other',\n",
       " 'should',\n",
       " 'score',\n",
       " 'would',\n",
       " 'more',\n",
       " 'about',\n",
       " 'when',\n",
       " 'time',\n",
       " 'team',\n",
       " 'american',\n",
       " 'such']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(f\"lenght of vocabulary is {len(filtered_vocab)} words\")\n",
    "filtered_vocab[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab = np.array(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(change_vector_dir\n",
    "with open(\"../../wordvectors/vocabs.pkl\", \"wb\") as file:\n",
    "#     print(file)\n",
    "    pickle.dump(filtered_vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vecs(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[0] == \"{st@rt}\":\n",
    "        tokens = tokens[1:]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        tokens = tokens[:-1]\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/change objects/Berlin_Wall_change.h5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_name = \"Berlin_Wall\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "# change_file_name = f\"{article_name}.pkl\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "change_object_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 420 ms, sys: 176 ms, total: 596 ms\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 50s, sys: 1.82 s, total: 8min 52s\n",
      "Wall time: 8min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content_dir = \"../data/content/\"\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "unique_str = np.unique(all_rev.str)\n",
    "masks_vec = np.isin(unique_str, filtered_vocab)\n",
    "masks_df = pd.DataFrame({ \"str\":unique_str, \"mask\":masks_vec}).set_index(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19641, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_object_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vector from change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 608 ms, total: 1min 18s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ins_vec_list = []\n",
    "change_object_dataframe[\"ins_tokens\"].apply(lambda token_set: ins_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set)))\n",
    "ins_matrix = np.c_[ins_vec_list]\n",
    "\n",
    "del_vec_list = []\n",
    "change_object_dataframe[\"del_tokens\"].apply(lambda token_set: del_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set)))\n",
    "del_matrix = np.c_[del_vec_list]\n",
    "ins_del_sum_matrix = (ins_matrix + del_matrix)/2\n",
    "\n",
    "del ins_vec_list\n",
    "del del_vec_list\n",
    "del ins_matrix\n",
    "del del_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.3 s, sys: 1 s, total: 50.3 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set[-10:])))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set[:10])))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "neighbour_10_matrix = np.concatenate([ left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    "\n",
    "ins_del_10_sum_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.5 s, sys: 548 ms, total: 44 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set[-4:])))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(get_word_vecs(wiki_vec, masks_df, token_set[:4])))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "neighbour_4_matrix = np.concatenate([left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    " \n",
    "ins_del_4_sum_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_average_word_vec(wiki_vec, masks, tokens):\n",
    "    if tokens and tokens[0] == \"{st@rt}\":\n",
    "        tokens = tokens[1:]\n",
    "    if tokens and tokens[-1] == \"{$nd}\":\n",
    "        tokens = tokens[:-1]\n",
    "    weights = np.exp(-(1/32)*np.arange(len(tokens))*2)\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = masks.loc[tokens, \"mask\"].values\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    in_vocab_weight = weights[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], weights=in_vocab_weight, axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 644 ms, total: 1min 9s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(weighted_average_word_vec(wiki_vec, masks_df, token_set)))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(weighted_average_word_vec(wiki_vec, masks_df, token_set)))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "weighted_neighbour_matrix = np.concatenate([left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    "ins_del_weighted_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving change object vector  to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_vector_dir = \"../data/change_vector_optimised/\"\n",
    "change_vector_file = os.path.join(change_vector_dir, change_object_file_name)\n",
    "\n",
    "arrays_to_save = {\n",
    "    \"neighbour_10\": neighbour_10_matrix,\n",
    "    \"ins_del_10_sum_neighbour\": ins_del_10_sum_neighbour_matrix, \n",
    "    \"neighbour_4\": neighbour_4_matrix,\n",
    "    \"ins_del_4_sum_neighbour\": ins_del_4_sum_neighbour_matrix,\n",
    "    \"weighted_neighbour_matrix\": weighted_neighbour_matrix,\n",
    "    \"ins_del_weighted_neighbour_matrix\": ins_del_weighted_neighbour_matrix\n",
    "}\n",
    "\n",
    "with open(change_vector_file, \"wb\") as file:\n",
    "    np.savez(file, **arrays_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
