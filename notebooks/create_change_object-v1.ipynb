{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scripts.wiki import Wiki,Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki:\n",
    "    '''\n",
    "    MAIN CLASS TO store all revisions for a wiki along with editors and timestamp.\n",
    "    '''\n",
    "    def __init__(self,id,title, revs, all_tokens=[]):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.revisions = revs\n",
    "        self.add_all_token(all_tokens)\n",
    "        \n",
    "\n",
    "           \n",
    "    def add_all_token(self, all_tokens):\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            self.revisions.loc[token[\"o_rev_id\"]].added.add(token[\"token_id\"])\n",
    "            for in_revision in token[\"in\"]:\n",
    "                self.revisions.loc[in_revision].added.add(token[\"token_id\"])\n",
    "            for out_revision in token[\"out\"]:\n",
    "                self.revisions.loc[out_revision].removed.add(token[\"token_id\"])\n",
    "                \n",
    "    def create_change(self, from_rev_id, to_rev_id, to_rev_content, epsilon_size):\n",
    "        try:\n",
    "            from_rev = self.revisions[from_rev_id]\n",
    "            to_rev = self.revisions[to_rev_id]\n",
    "            from_rev.deleted(to_rev)\n",
    "            to_rev.content = to_rev_content\n",
    "            to_rev.inserted_continuous_pos()\n",
    "            to_rev.inserted_neighbours()\n",
    "            from_rev.create_change_object(to_rev)\n",
    "            from_rev.append_neighbour_vec(to_rev, epsilon_size)\n",
    "        except:\n",
    "            print(\"exception occurred in calculating change object\",traceback.format_exc())\n",
    "            print(\"problem in \", to_rev_content.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Revision:\n",
    "    def __init__(self, id, timestamp,editor):\n",
    "        self.id = id\n",
    "        self.timestamp = timestamp\n",
    "        self.editor = editor\n",
    "        self.added = set()\n",
    "        self.removed = set()   \n",
    "        \n",
    "    def deleted(self, to_rev):\n",
    "        self.content[\"removed\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(to_rev.removed), assume_unique= True ))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        start_neighbour = start_pos - 1\n",
    "        end_neighbour = end_pos + 1\n",
    "        self.deleted_object = pd.DataFrame(np.c_[ start_pos, end_pos, start_neighbour, end_neighbour ],\n",
    "                                       columns=[ \"del_start_pos\", \"del_end_pos\", \"left_neigh\", \"right_neigh\",])\n",
    "    \n",
    "    def inserted_continuous_pos(self):\n",
    "        self.content[\"added\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(self.added), assume_unique= True))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        self.added_pos = np.c_[start_pos, end_pos]\n",
    "\n",
    "    def inserted_neighbours(self):\n",
    "        start_token_pos = self.added_pos[:,0] - 1\n",
    "        end_token_pos = self.added_pos[:,1] + 1\n",
    "        self.start_token_id = self.content[\"token_id\"].values[start_token_pos]\n",
    "        self.end_token_id = self.content[\"token_id\"].values[end_token_pos]\n",
    "    \n",
    "    def create_change_object(self, to_rev):\n",
    "        self.ins_left = np.argwhere(np.isin(self.content.token_id.values, to_rev.start_token_id, assume_unique= True))\n",
    "        self.ins_right = np.argwhere(np.isin(self.content.token_id.values, to_rev.end_token_id, assume_unique= True))\n",
    "        self.inserted_object = pd.DataFrame(np.concatenate([to_rev.added_pos, self.ins_left, self.ins_right], axis=1),\n",
    "                                       columns=[\"ins_start_pos\", \"ins_end_pos\", \"left_neigh\", \"right_neigh\" ])\n",
    "\n",
    "        self.change = pd.merge(self.inserted_object, self.deleted_object,how=\"outer\", on=[\"left_neigh\", \"right_neigh\"])\n",
    "        self.change.fillna(-1, inplace=True)\n",
    "        \n",
    "    def append_neighbour_vec(self, to_rev, epsilon_size):\n",
    "        self.content_str_vec = self.content.str.values\n",
    "        del self.content\n",
    "        neighbour_df = self.change.apply(find_tokens, axis=1, args=(self, to_rev, epsilon_size))\n",
    "        neighbour_df.columns= [\"ins_tokens\", \"del_tokens\", \"left_neigh_slice\", \"right_neigh_slice\", \"left_token\", \"right_token\"]\n",
    "        self.change_df = pd.concat([self.change, neighbour_df], sort=False, axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens(change, revision, to_rev, epsilon_size):\n",
    "    start_left = (int(change[\"left_neigh\"]) - epsilon_size)\n",
    "    if start_left <0:\n",
    "        start_left = 0\n",
    "    left_neigh = slice( start_left, int(change[\"left_neigh\"]) + 1)\n",
    "    \n",
    "    end_right = (int(change[\"right_neigh\"]) + epsilon_size+1)\n",
    "    if end_right >= revision.content_str_vec.size:\n",
    "        end_right = revision.content_str_vec.size - 1\n",
    "    right_neigh = slice(int(change[\"right_neigh\"]), end_right )\n",
    "    if(change[\"ins_start_pos\"]==-1):\n",
    "        ins_tokens = []\n",
    "    else:\n",
    "        ins_slice = slice(int(change[\"ins_start_pos\"]), int(change[\"ins_end_pos\"]+1) )\n",
    "        ins_tokens = to_rev.content.str.values[ins_slice]\n",
    "    if(change[\"del_start_pos\"] == -1):\n",
    "        del_tokens = []\n",
    "    else:\n",
    "        del_slice = slice(int(change[\"del_start_pos\"]), int(change[\"del_end_pos\"]+1) )\n",
    "        del_tokens = revision.content_str_vec[del_slice]\n",
    "    left_token = revision.content_str_vec[left_neigh]\n",
    "    right_token = revision.content_str_vec[right_neigh]\n",
    "    return pd.Series([tuple(ins_tokens), tuple(del_tokens), left_neigh, right_neigh, tuple(left_token), tuple(right_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "article_name = \"Violence_against_Muslims_in_India\"\n",
    "filename = article_name + \".h5\"\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "\n",
    "epsilon_size = 30\n",
    "epsilon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    rev_list = store.get(\"rev_list\")\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "    all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "    #making revision objects\n",
    "    revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "    revs.index = rev_list.id\n",
    "    from_rev_id = revs.index[0]\n",
    "    \n",
    "    wiki = Wiki(2345, content, revs, all_tokens)\n",
    "    wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "    for to_rev_id in list(revs.index[1:]):\n",
    "        key=\"r\"+str(to_rev_id)\n",
    "        to_rev_content = store[key]\n",
    "        wiki\n",
    "        wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "        from_rev_id = to_rev_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filepath = os.path.join(change_object_dir, content+\".pkl\")\n",
    "with open(save_filepath, \"wb\") as file:\n",
    "    pickle.dump(wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving change object for all the articles in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_change_object(article_name, content_dir = \"../data/content/\", \n",
    "                            change_object_dir =  \"../data/change objects/\", epsilon_size=30, save=False):\n",
    "    \n",
    "    content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "    change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "    \n",
    "    with pd.HDFStore(content_filepath, 'r') as store:\n",
    "        #retrieving all rev list and change object from file\n",
    "        rev_list = store.get(\"rev_list\")\n",
    "        all_rev = store.get(\"all_tokens\")\n",
    "        all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "        \n",
    "        #making revision objects\n",
    "        revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "        revs.index = rev_list.id\n",
    "        \n",
    "        # Getting first revision object and adding content ot it\n",
    "        from_rev_id = revs.index[0]\n",
    "        wiki = Wiki(2345, article_name, revs, all_tokens)\n",
    "        wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "        # adding content to all other revision and finding change object between them.\n",
    "        \n",
    "        for to_rev_id in list(revs.index[1:]):\n",
    "            key=\"r\"+str(to_rev_id)\n",
    "            to_rev_content = store[key]\n",
    "            wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "            from_rev_id = to_rev_id\n",
    "         \n",
    "    if save:\n",
    "        with open(change_object_filepath, \"wb\") as file:\n",
    "            pickle.dump(wiki, file)\n",
    "        \n",
    "    return wiki\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_series=pd.read_csv(\"../conflicted_article.csv\")[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 s, sys: 10.2 s, total: 1min 4s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki = create_change_object(article_name, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for article in article_series[19:]:\n",
    "#     print(article)\n",
    "#     create_change_object(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving change_object as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "change_objects = []\n",
    "wiki.revisions.iloc[:-1].apply(lambda revision: change_objects.append(revision.change_df))\n",
    "# change_index = [ rev.id for rev in  wiki.revisions[1:].tolist()]\n",
    "# change_df = pd.concat(change_objects, sort=False, keys=change_index, axis=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_s = pd.to_datetime([ rev.timestamp for rev in  wiki.revisions.values.ravel().tolist()])\n",
    "time_gap = pd.to_timedelta(timestamp_s[1:]-timestamp_s[:-1])\n",
    "\n",
    "rev_ids = [ rev.id for rev in  wiki.revisions.tolist()]\n",
    "from_rev_ids = rev_ids[:-1]\n",
    "to_rev_ids= rev_ids[1:]\n",
    "\n",
    "editor_s = [ rev.editor for rev in  wiki.revisions.tolist()]\n",
    "\n",
    "index = list(zip(*[from_rev_ids, to_rev_ids, timestamp_s.tolist()[1:], time_gap, editor_s[1:]]))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=index, names=[\"from revision id\", \"to revision id\", \"timestamp\", \"timegap\", \"editor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 ms ± 199 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "a=change_df.to_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "a=pd.read_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/pandas/core/generic.py:1996: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->['ins_tokens', 'del_tokens', 'left_neigh_slice', 'right_neigh_slice', 'left_token', 'right_token']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223 ms ± 103 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.h5\")\n",
    "change_df.to_hdf(change_dataframe_path, key=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.h5\")\n",
    "\n",
    "\n",
    "a=pd.read_hdf(change_dataframe_path, key=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_df(tokens):\n",
    "    tokens.insert(0, {'token_id':-1, 'str':  \"{st@rt}\"})\n",
    "    tokens.append({'token_id':-2, 'str': \"{$nd}\"})\n",
    "    return pd.DataFrame(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [{'o_rev_id': 558137654,\n",
    "  'str': 'contemporary',\n",
    "  'token_id': 994,\n",
    "  'in': [561887510],\n",
    "  'out': [561887480]},\n",
    " {'o_rev_id': 558137654,\n",
    "  'str': 'india',\n",
    "  'token_id': 995,\n",
    "  'in': [561887510,561887510],\n",
    "  'out': [561887480]},\n",
    " {'o_rev_id': 561887480, 'str': ':', 'token_id': 996, 'in': [561887510], 'out': [561887490]},\n",
    " {'o_rev_id': 561887480, 'str': '|', 'token_id': 8976, 'in': [561887510], 'out': [561887490]},\n",
    " {'o_rev_id': 558137654,\n",
    "  'str': 'hefner',\n",
    "  'token_id': 9876,\n",
    "  'in': [],\n",
    "  'out': []},\n",
    " {'o_rev_id': 558137654, 'str': '_', 'token_id': 1023, 'in': [561887510], 'out': [561887480]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = pd.DataFrame({'id': [558137654, 561887480,561887490,561887510],\n",
    " 'editor': ['14904681', '14904681','14904681', '14904681'],\n",
    " 'timestamp': ['2013-06-03T14:57:37Z', '2013-06-03T15:00:42Z', '2013-06-04T15:00:42Z', '2013-06-05T15:00:42Z']})\n",
    "revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "revs.index = rev_list.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tokens = tokens_to_df( [\n",
    " {'str': 'contemporary','token_id': 994},\n",
    " {'str': 'india','token_id': 995},\n",
    " {'str': 'hefner','token_id': 9876},\n",
    " {'str': '_', 'token_id': 1023}])\n",
    "first_tokens[\"str\"][1:-1].str.cat(sep= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tokens = tokens_to_df( [\n",
    " {'str': ':', 'token_id': 996},\n",
    " {'str': '|', 'token_id': 8976},\n",
    " {'str': 'hefner','token_id': 9876}\n",
    "])\n",
    "second_tokens[\"str\"][1:-1].str.cat(sep= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_tokens = tokens_to_df( [\n",
    " {'str': 'hefner','token_id': 9876}\n",
    "])\n",
    "third_tokens[\"str\"][1:-1].str.cat(sep= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_tokens = tokens_to_df( [\n",
    " {'str': 'contemporary','token_id': 994},\n",
    " {'str': ':', 'token_id': 996},\n",
    " {'str': '|', 'token_id': 8976},\n",
    " {'str': 'india','token_id': 995},\n",
    " {'str': 'hefner','token_id': 9876},\n",
    " {'str': '_', 'token_id': 1023}])\n",
    "fourth_tokens[\"str\"][1:-1].str.cat(sep= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wiki = Wiki(1234, \"test\",revs, all_tokens)\n",
    "\n",
    "test_wiki.revisions.iloc[0].content = first_tokens\n",
    "\n",
    "\n",
    "first_rev_id = revs.index[0]\n",
    "second_rev_id = revs.index[1]\n",
    "\n",
    "test_wiki.create_change(first_rev_id, second_rev_id, second_tokens, vocab, 6)\n",
    "\n",
    "third_rev_id = revs.index[2]\n",
    "\n",
    "test_wiki.create_change(second_rev_id, third_rev_id, third_tokens, vocab, 6)\n",
    "\n",
    "fourth_rev_id = revs.index[3]\n",
    "\n",
    "test_wiki.create_change(third_rev_id, fourth_rev_id, fourth_tokens, vocab, 6)\n",
    "\n",
    "change_objects = []\n",
    "test_wiki.revisions[:-1].apply(lambda revision: change_objects.append(revision.neighbour))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=wiki.revisions.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
