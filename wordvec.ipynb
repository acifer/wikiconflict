{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT_gensim.load('../wordvectors/wiki-news-300d-1M-subword.vec.zip')\n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../wordvectors/wiki-news-300d-1M.vec', binary=False, limit=50000)\n",
    "vocab = set(wiki_vec.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_vec['hello'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "content = \"Mama's_Family\"\n",
    "def _url(url, path):\n",
    "    return url + path\n",
    "revisions_url = _url( baseurl,\"rev_ids/\")\n",
    "mama_revisions_url = _url(revisions_url, content+\"/\")\n",
    "params = {\"editor\": \"true\", \"timestamp\": \"true\"}\n",
    "response = requests.get(mama_revisions_url, params= params)\n",
    "revisons_list = response.json()[\"revisions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_ul = _url(baseurl, \"all_content/\" )\n",
    "mama_all_content_url = _url(all_content_ul, content +\"/\")\n",
    "params = { \"o_rev_id\": \"true\", \"editor\": \"false\", \"token_id\": \"true\", \"in\": \"true\", \"out\": \"true\" }\n",
    "all_rev_data = requests.get(mama_all_content_url, params= params)\n",
    "all_tokens_mama = all_rev_data.json()[\"all_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "HDF5 error back trace\n\n  File \"H5F.c\", line 586, in H5Fopen\n    unable to open file\n  File \"H5Fint.c\", line 1305, in H5F_open\n    unable to lock the file\n  File \"H5FD.c\", line 1839, in H5FD_lock\n    driver lock request failed\n  File \"H5FDsec2.c\", line 940, in H5FD_sec2_lock\n    unable to lock file, errno = 35, error message = 'Resource temporarily unavailable'\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'Mama's_Family.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHDF5ExtError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mHDF5ExtError\u001b[0m: HDF5 error back trace\n\n  File \"H5F.c\", line 586, in H5Fopen\n    unable to open file\n  File \"H5Fint.c\", line 1305, in H5F_open\n    unable to lock the file\n  File \"H5FD.c\", line 1839, in H5FD_lock\n    driver lock request failed\n  File \"H5FDsec2.c\", line 940, in H5FD_sec2_lock\n    unable to lock file, errno = 35, error message = 'Resource temporarily unavailable'\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'Mama's_Family.h5'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-30c22073a8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrevisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrevision\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mrevisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gensim/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;31m# is not part of IOError, make it one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'Unable to open/create file'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: HDF5 error back trace\n\n  File \"H5F.c\", line 586, in H5Fopen\n    unable to open file\n  File \"H5Fint.c\", line 1305, in H5F_open\n    unable to lock the file\n  File \"H5FD.c\", line 1839, in H5FD_lock\n    driver lock request failed\n  File \"H5FDsec2.c\", line 940, in H5FD_sec2_lock\n    unable to lock file, errno = 35, error message = 'Resource temporarily unavailable'\n\nEnd of HDF5 error back trace\n\nUnable to open/create file 'Mama's_Family.h5'"
     ]
    }
   ],
   "source": [
    "filename = content + \".h5\"\n",
    "revisions = []\n",
    "with pd.HDFStore( filename, 'r') as store:\n",
    "    for revision in store.keys():\n",
    "        revisions.append(store.get(revision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Wiki:\n",
    "    def __init__(self,id,title, all_tokens=[]):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.all_tokens = all_tokens\n",
    "        \n",
    "    def init_revisions(self, revisions):\n",
    "#         self.revisions = {revision[\"id\"] : Revision(revision[\"id\"],revision[\"timestamp\"], revision[\"editor\"]) for revision in revisions}\n",
    "          self.revisions = pd.Series( {revision[\"id\"] : \n",
    "                                       Revision(revision[\"id\"],revision[\"timestamp\"], revision[\"editor\"]) for revision in revisions} )\n",
    "\n",
    "\n",
    "    def add_all_token(self, all_tokens):\n",
    "        self.all_tokens = all_tokens\n",
    "        for token in self.all_tokens:\n",
    "            self.revisions.loc[token[\"o_rev_id\"]].added.add(token[\"token_id\"])\n",
    "            for in_revision in token[\"in\"]:\n",
    "                self.revisions.loc[in_revision].added.add(token[\"token_id\"])\n",
    "            for out_revision in token[\"out\"]:\n",
    "                self.revisions.loc[out_revision].removed.add(token[\"token_id\"])\n",
    "class Revision:\n",
    "    def __init__(self, id, timestamp,editor):\n",
    "        self.id = id\n",
    "        self.timestamp = timestamp\n",
    "        self.editor = editor\n",
    "        self.added = set()\n",
    "        self.removed = set()\n",
    "    \n",
    "        \n",
    "        \n",
    "    def deleted(self, to_rev):\n",
    "        self.content[\"removed\"] = self.content[\"token_id\"].isin(to_rev.removed)\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        start_neighbour = start_pos - 1\n",
    "        end_neighbour = end_pos + 1\n",
    "        self.first_last_token_del = self.content[\"removed\"].values[[0,-1]]\n",
    "        if self.first_last_token_del[0]:\n",
    "            start_pos[0] = end_pos[0]\n",
    "            start_neighbour[0] = end_neighbour[0]\n",
    "        if self.first_last_token_del[1]:\n",
    "            end_pos[-1] = start_pos[-1]\n",
    "            start_neighbour[-1] = end_neighbour[-1]\n",
    "        self.deleted_object = pd.DataFrame(np.c_[ start_pos, end_pos, start_neighbour, end_neighbour ],\n",
    "                                       columns=[ \"del_start_pos\", \"del_end_pos\", \"left_neigh\", \"right_neigh\",])\n",
    "    \n",
    "    def inserted_continuous_pos(self):\n",
    "        self.content[\"added\"] = self.content[\"token_id\"].isin(self.added)\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        self.added_pos = np.c_[start_pos, end_pos]\n",
    "\n",
    "    def inserted_neighbours(self):\n",
    "        start_token_pos = self.added_pos[:,0] - 1\n",
    "        end_token_pos = self.added_pos[:,1] + 1\n",
    "        self.first_last_token = self.content[\"added\"].values[[0,-1]]\n",
    "        if self.first_last_token[0]:\n",
    "            start_token_pos[0] = end_token_pos[0]\n",
    "        if self.first_last_token[1]:\n",
    "            end_token_pos[-1] = start_token_pos[-1]\n",
    "        self.start_token_id = self.content[\"token_id\"].values[start_token_pos]\n",
    "        self.end_token_id = self.content[\"token_id\"].values[end_token_pos]\n",
    "    \n",
    "    def create_change_object(self, to_rev):\n",
    "        ins_left = np.argwhere(from_rev.content.token_id.isin(to_rev.start_token_id))\n",
    "        ins_right = np.argwhere(from_rev.content.token_id.isin(to_rev.end_token_id))\n",
    "        self.inserted_object = pd.DataFrame(np.c_[to_rev.added_pos, ins_left, ins_right],\n",
    "                                       columns=[\"ins_start_pos\", \"ins_end_pos\", \"left_neigh\", \"right_neigh\", ])\n",
    "\n",
    "        self.change = pd.merge(self.inserted_object, self.deleted_object,how=\"outer\", on=[\"left_neigh\", \"right_neigh\"])\n",
    "        self.change.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    def append_neighbour_vec(self,wiki_vec, epsilon_size):\n",
    "        neighbour_df = self.change.apply(find_tokens, axis=1, args=(self, wiki_vec, 6))\n",
    "        neighbour_df.columns= [\"ins_tokens\", \"del_tokens\", \"left_neigh\", \"right_neigh\", \"left_token\", \"right_token\"]\n",
    "        self.change = pd.concat([self.change, neighbour_df], sort=False)\n",
    "class Change:\n",
    "    def __init__(self, token, start, end, left_context, right_context):\n",
    "        self.token = token\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.left = left_context\n",
    "        self.right = right_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-Do:Refractor it\n",
    "def find_tokens(change, revision, wiki_vec, epsilon_size):\n",
    "        vocabs_pos = np.argwhere( revision.content[\"invocab\"].values)\n",
    "        left_neigh = vocabs_pos[vocabs_pos <= change[\"left_neigh\"]][-epsilon_size:]\n",
    "        right_neigh = vocabs_pos[vocabs_pos >= change[\"right_neigh\"]][:epsilon_size]\n",
    "        ins_slice = slice(int(change[\"ins_start_pos\"]), int(change[\"ins_end_pos\"]+1) )\n",
    "        del_slice = slice(int(change[\"del_start_pos\"]), int(change[\"del_end_pos\"]+1) )\n",
    "        left_token = from_rev.content.str.values[left_neigh]\n",
    "        right_token = from_rev.content.str.values[right_neigh]\n",
    "        ins_tokens = from_rev.content.str.values[ins_slice]\n",
    "        del_tokens = from_rev.content.str.values[del_slice]\n",
    "        return pd.Series([tuple(ins_tokens), tuple(del_tokens), tuple(left_neigh), tuple(right_neigh), tuple(left_token), tuple(right_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.83 s, sys: 29.6 ms, total: 5.86 s\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_wiki = Wiki(2345, \"a test wiki\", all_tokens=4)\n",
    "test_wiki.init_revisions(revisons_list)\n",
    "test_wiki.add_all_token(all_tokens_mama) \n",
    "epsilon_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 304 ms, sys: 0 ns, total: 304 ms\n",
      "Wall time: 303 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test_wiki.revisions.iloc[0].content = pd.DataFrame(rev_contents[0])\n",
    "for i in range(1, len(rev_contents)):\n",
    "    from_rev = test_wiki.revisions.iloc[i-1]\n",
    "    to_rev = test_wiki.revisions.iloc[i]\n",
    "    from_rev.deleted(to_rev)\n",
    "    from_rev.content[\"invocab\"] = from_rev.content[\"str\"].isin(set(wiki_vec.vocab))\n",
    "    to_rev.content = pd.DataFrame(rev_contents[i])\n",
    "    to_rev.inserted_continuous_pos()\n",
    "    to_rev.inserted_neighbours()\n",
    "    from_rev.create_change_object(to_rev)\n",
    "    from_rev.append_neighbour_vec( wiki_vec, epsilon_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_objects = []\n",
    "test_wiki.revisions[:14].apply(lambda revision: change_objects.append(revision.change))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=test_wiki.revisions.index[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_rev = test_wiki.revisions[revisons.id.tolist()[0]] \n",
    "to_rev = test_wiki.revisions[revisons.id.tolist()[4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vecs(tokens):\n",
    "    in_vocab_tokens = set(tokens) & set(wiki_vec.vocab)\n",
    "    if in_vocab_tokens:\n",
    "        return wiki_vec[in_vocab_tokens].sum(axis=0, keepdims=True)\n",
    "    else:\n",
    "        return np.zeros((1, wiki_vec.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 24.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "change_vecs_list = []\n",
    "change_token_s = change_df[\"ins_tokens\"] + change_df[\"del_tokens\"]\n",
    "change_token_s.apply(lambda token_set: ins_vecs_list.append(get_word_vecs(token_set)))\n",
    "\n",
    "change_matrix = np.concatenate(ins_vecs_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 0 ns, total: 24 ms\n",
      "Wall time: 23.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "neigh_vecs_list = []\n",
    "neighbour_s = change_df['left_token'] + change_df['right_token']\n",
    "neighbour_s.apply(lambda token_set: neigh_vecs_list.append(get_word_vecs(token_set)))\n",
    "\n",
    "neighbour_matrix = np.concatenate(neigh_vecs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neighbours(change, revision, wiki_vec, epsilon_size):\n",
    "#     vocabs_pos = np.argwhere( revision.content[\"invocab\"].values)\n",
    "#     left = vocabs_pos[vocabs_pos < change[\"ins_starting_pos\"]][:epsilon_size]\n",
    "#     right = vocabs_pos[vocabs_pos > change[\"ins_end_pos\"]][:epsilon_size]\n",
    "#     neighbor_tokens = from_rev.content.str.values[np.r_[left,right]]\n",
    "#     neighbour_vec = wiki_vec[neighbor_tokens].sum(axis=0)\n",
    "#     combined_neighbour_vec = np.r_[left, right, neighbor_tokens, neighbour_vec]\n",
    "#     return combined_neighbour_vec  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "added_pos = test_wiki.revisions[3317921].added_pos\n",
    "start_token_id = test_wiki.revisions[3317921].content[\"token_id\"].values[added_pos[:,0]-1]\n",
    "end_token_id = test_wiki.revisions[3317921].content[\"token_id\"].values[added_pos[:,1]+1]\n",
    "first_last_token = test_wiki.revisions[3317921].content[\"added\"].values[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-547-f8df6b6e94d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_wiki\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrevisions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3317921\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'start_pos' is not defined"
     ]
    }
   ],
   "source": [
    "test_wiki.revisions[3317921].content[\"str\"].values[np.s_[start_pos,end_pos[1:]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wiki.revisions[3317921].content[\"str\"].values[start_pos[0][0]:end_pos[1][0]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['an', '80', 'u', 's', '.', 'ian', 'television'], dtype=object)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.compress(test_wiki.revisions[3317921].content[\"removed\"].values, test_wiki.revisions[3317921].content[\"str\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['s', '.'],\n",
       "       ['.', 'show']], dtype=object)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(test_wiki.revisions[3317921].content[\"str\"].values, pos )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
