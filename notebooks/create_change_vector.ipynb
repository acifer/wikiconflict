{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 40s, sys: 10min 1s, total: 15min 42s\n",
      "Wall time: 15min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "wiki_vec = KeyedVectors.load_word2vec_format('../../wordvectors/wiki.en.vec', binary=False, limit=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_vecs(wiki_vec, vocab, tokens):\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = np.isin(tokens, vocab)\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_average_word_vec(wiki_vec, vocab, tokens):\n",
    "    weights = np.exp(-1/32*np.arange(len(tokens))**2)\n",
    "    tokens = np.array(tokens)\n",
    "    tokens_in_vocab_mask = np.isin(tokens, vocab)\n",
    "    in_vocab_tokens = tokens[tokens_in_vocab_mask]\n",
    "    in_vocab_weight = weights[tokens_in_vocab_mask]\n",
    "    if np.any(tokens_in_vocab_mask):\n",
    "        return np.average(wiki_vec[in_vocab_tokens], weights=in_vocab_weight, axis=0)\n",
    "\n",
    "\n",
    "    else:\n",
    "        return np.zeros( wiki_vec.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### reading the change object and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = \"Violence_against_Muslims_in_India\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "change_vector_dir = \"../data/change_vector/\"\n",
    "\n",
    "change_object_file_name = f\"{article_name}_vec.npz\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "change_vector_file = os.path.join(change_vector_dir, change_object_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 4.86 s, total: 6.69 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3933, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_object_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vector from change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(list(wiki_vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ins_vec_list = []\n",
    "change_object_dataframe[\"ins_tokens\"].apply(lambda token_set: ins_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set)))\n",
    "ins_matrix = np.c_[ins_vec_list]\n",
    "\n",
    "del_vec_list = []\n",
    "change_object_dataframe[\"del_tokens\"].apply(lambda token_set: del_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set)))\n",
    "del_matrix = np.c_[del_vec_list]\n",
    "ins_del_sum_matrix = (ins_matrix + del_matrix)/2\n",
    "\n",
    "del ins_vec_list\n",
    "del del_vec_list\n",
    "del ins_matrix\n",
    "del del_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set[-10:])))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set[:10])))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "neighbour_10_matrix = np.concatenate([ left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    "\n",
    "ins_del_10_sum_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set[-4:])))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(get_word_vecs(wiki_vec, vocab, token_set[:4])))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "neighbour_4_matrix = np.concatenate([left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    " \n",
    "ins_del_4_sum_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "left_vec_list = []\n",
    "change_object_dataframe[\"left_token\"].apply(lambda token_set: left_vec_list.append(weighted_average_word_vec(wiki_vec, vocab, token_set)))\n",
    "left_neighbour_matrix = np.c_[left_vec_list]\n",
    "\n",
    "right_vec_list = []\n",
    "change_object_dataframe[\"right_token\"].apply(lambda token_set: right_vec_list.append(weighted_average_word_vec(wiki_vec, vocab, token_set)))\n",
    "right_neighbour_matrix = np.c_[right_vec_list]\n",
    "\n",
    "weighted_neighbour_matrix = np.concatenate([left_neighbour_matrix, right_neighbour_matrix], axis=1)\n",
    " \n",
    "ins_del_weighted_neighbour_matrix = np.concatenate([left_neighbour_matrix, ins_del_sum_matrix, right_neighbour_matrix], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del left_vec_list\n",
    "# del right_vec_list\n",
    "# del right_neighbour_matrix\n",
    "# del left_neighbour_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving change object vector  to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_to_save = {\n",
    "    \"neighbour_10\": neighbour_10_matrix,\n",
    "    \"ins_del_10_sum_neighbour\": ins_del_10_sum_neighbour_matrix, \n",
    "    \"neighbour_4\": neighbour_4_matrix,\n",
    "    \"ins_del_4_sum_neighbour\": ins_del_4_sum_neighbour_matrix,\n",
    "    \"weighted_neighbour_matrix\": weighted_neighbour_matrix,\n",
    "    \"ins_del_weighted_neighbour_matrix\": ins_del_weighted_neighbour_matrix\n",
    "}\n",
    "\n",
    "with open(change_vector_file, \"wb\") as file:\n",
    "    np.savez(file, **arrays_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    neighbour_10_matrix_1 = arrays_dict[\"neighbour_10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(change_vector_file, \"rb\") as file:\n",
    "    arrays_dict = np.load(file)\n",
    "    neighbour_10_matrix = arrays_dict[\"neighbour_10\"]\n",
    "    ins_del_10_sum_neighbour_matrix = arrays_dict[\"ins_del_10_sum_neighbour\"]\n",
    "    neighbour_4_matrix = arrays_dict[\"neighbour_4\"]\n",
    "    ins_del_4_sum_neighbour_matrix = arrays_dict[\"ins_del_4_sum_neighbour\"]\n",
    "    weighted_neighbour_matrix_matrix = arrays_dict[\"weighted_neighbour_matrix\"]\n",
    "    ins_del_weighted_neighbour_matrix = arrays_dict[\"ins_del_weighted_neighbour_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
