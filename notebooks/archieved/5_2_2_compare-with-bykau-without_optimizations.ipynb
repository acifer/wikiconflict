{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.stats import entropy\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to take weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\", ):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## reading the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ins_start_pos</th>\n",
       "      <th>ins_end_pos</th>\n",
       "      <th>left_neigh</th>\n",
       "      <th>right_neigh</th>\n",
       "      <th>del_start_pos</th>\n",
       "      <th>del_end_pos</th>\n",
       "      <th>ins_tokens</th>\n",
       "      <th>del_tokens</th>\n",
       "      <th>left_neigh_slice</th>\n",
       "      <th>right_neigh_slice</th>\n",
       "      <th>left_token</th>\n",
       "      <th>right_token</th>\n",
       "      <th>ins_length</th>\n",
       "      <th>del_length</th>\n",
       "      <th>del_string_tokens</th>\n",
       "      <th>ins_string_tokens</th>\n",
       "      <th>edit_string_tokens</th>\n",
       "      <th>left_context</th>\n",
       "      <th>right_context</th>\n",
       "      <th>bykau_cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from revision id</th>\n",
       "      <th>to revision id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timegap</th>\n",
       "      <th>editor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">203693</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">203699</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2002-09-08 14:05:32</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">194 days 22:14:17</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3646</th>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 10, None)</td>\n",
       "      <td>slice(10, 41, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)</td>\n",
       "      <td>(9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' '</td>\n",
       "      <td>of scotland ( [[ university of glasgow ]] ) wa...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(62,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 11, None)</td>\n",
       "      <td>slice(11, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</td>\n",
       "      <td>(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of</td>\n",
       "      <td>scotland ( [[ university of glasgow ]] ) was t...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(63,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 12, None)</td>\n",
       "      <td>slice(12, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</td>\n",
       "      <td>(11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland</td>\n",
       "      <td>( [[ university of glasgow ]] ) was the first ...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>(64, 65, 66, 67, 68, 69)</td>\n",
       "      <td>(26, 27, 28, 29, 30)</td>\n",
       "      <td>slice(0, 27, None)</td>\n",
       "      <td>slice(32, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>(31, 32, 33, 34, 35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>((, which, see, ), in)</td>\n",
       "      <td>(,, a, device, he, presented, to)</td>\n",
       "      <td>(,, a, device, he, presented, to, (, which, se...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland...</td>\n",
       "      <td>the mid 1920s ( [[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>(70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...</td>\n",
       "      <td>(32, 33, 34)</td>\n",
       "      <td>slice(2, 33, None)</td>\n",
       "      <td>slice(36, 42, None)</td>\n",
       "      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>(35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>(mid, 1920s, ()</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>' ' john logie baird ' ' ' of scotland ( [[ un...</td>\n",
       "      <td>[[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                ins_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10.0   \n",
       "                                                                             1           32.0   \n",
       "                                                                             2           34.0   \n",
       "                                                                             3           50.0   \n",
       "                                                                             4           57.0   \n",
       "\n",
       "                                                                                ins_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         30.0   \n",
       "                                                                             1         32.0   \n",
       "                                                                             2         34.0   \n",
       "                                                                             3         55.0   \n",
       "                                                                             4         73.0   \n",
       "\n",
       "                                                                                left_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           9   \n",
       "                                                                             1          10   \n",
       "                                                                             2          11   \n",
       "                                                                             3          26   \n",
       "                                                                             4          32   \n",
       "\n",
       "                                                                                right_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10   \n",
       "                                                                             1           11   \n",
       "                                                                             2           12   \n",
       "                                                                             3           32   \n",
       "                                                                             4           36   \n",
       "\n",
       "                                                                                del_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           -1.0   \n",
       "                                                                             1           -1.0   \n",
       "                                                                             2           -1.0   \n",
       "                                                                             3           27.0   \n",
       "                                                                             4           33.0   \n",
       "\n",
       "                                                                                del_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         -1.0   \n",
       "                                                                             1         -1.0   \n",
       "                                                                             2         -1.0   \n",
       "                                                                             3         31.0   \n",
       "                                                                             4         35.0   \n",
       "\n",
       "                                                                                                                       ins_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...   \n",
       "                                                                             1                                              (62,)   \n",
       "                                                                             2                                              (63,)   \n",
       "                                                                             3                           (64, 65, 66, 67, 68, 69)   \n",
       "                                                                             4  (70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...   \n",
       "\n",
       "                                                                                          del_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                           \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    ()   \n",
       "                                                                             1                    ()   \n",
       "                                                                             2                    ()   \n",
       "                                                                             3  (26, 27, 28, 29, 30)   \n",
       "                                                                             4          (32, 33, 34)   \n",
       "\n",
       "                                                                                  left_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                         \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(0, 10, None)   \n",
       "                                                                             1  slice(0, 11, None)   \n",
       "                                                                             2  slice(0, 12, None)   \n",
       "                                                                             3  slice(0, 27, None)   \n",
       "                                                                             4  slice(2, 33, None)   \n",
       "\n",
       "                                                                                  right_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                          \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(10, 41, None)   \n",
       "                                                                             1  slice(11, 42, None)   \n",
       "                                                                             2  slice(12, 42, None)   \n",
       "                                                                             3  slice(32, 42, None)   \n",
       "                                                                             4  slice(36, 42, None)   \n",
       "\n",
       "                                                                                                                       left_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)   \n",
       "                                                                             1                 (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)   \n",
       "                                                                             2             (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)   \n",
       "                                                                             3  (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "                                                                             4  (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                                                                                                      right_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...   \n",
       "                                                                             1  (10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...   \n",
       "                                                                             2  (11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...   \n",
       "                                                                             3           (31, 32, 33, 34, 35, 36, 37, 38, 39, 40)   \n",
       "                                                                             4                           (35, 36, 37, 38, 39, 40)   \n",
       "\n",
       "                                                                                ins_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0          21   \n",
       "                                                                             1           1   \n",
       "                                                                             2           1   \n",
       "                                                                             3           6   \n",
       "                                                                             4          17   \n",
       "\n",
       "                                                                                del_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           0   \n",
       "                                                                             1           0   \n",
       "                                                                             2           0   \n",
       "                                                                             3           5   \n",
       "                                                                             4           3   \n",
       "\n",
       "                                                                                     del_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                             \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                      ()   \n",
       "                                                                             1                      ()   \n",
       "                                                                             2                      ()   \n",
       "                                                                             3  ((, which, see, ), in)   \n",
       "                                                                             4         (mid, 1920s, ()   \n",
       "\n",
       "                                                                                                                ins_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3                  (,, a, device, he, presented, to)   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                               edit_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3  (,, a, device, he, presented, to, (, which, se...   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                                     left_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                 St@rt ' ' ' john logie baird ' ' '   \n",
       "                                                                             1              St@rt ' ' ' john logie baird ' ' ' of   \n",
       "                                                                             2     St@rt ' ' ' john logie baird ' ' ' of scotland   \n",
       "                                                                             3  St@rt ' ' ' john logie baird ' ' ' of scotland...   \n",
       "                                                                             4  ' ' john logie baird ' ' ' of scotland ( [[ un...   \n",
       "\n",
       "                                                                                                                    right_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  of scotland ( [[ university of glasgow ]] ) wa...   \n",
       "                                                                             1  scotland ( [[ university of glasgow ]] ) was t...   \n",
       "                                                                             2  ( [[ university of glasgow ]] ) was the first ...   \n",
       "                                                                             3                   the mid 1920s ( [[ 1926 ]] ? ) .   \n",
       "                                                                             4                                   [[ 1926 ]] ? ) .   \n",
       "\n",
       "                                                                                bykau_cluster  \n",
       "from revision id to revision id timestamp           timegap           editor                   \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0            -99  \n",
       "                                                                             1            -99  \n",
       "                                                                             2            -99  \n",
       "                                                                             3            -99  \n",
       "                                                                             4            -99  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "content_dir = \"../data/content/\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    token_string_df = store.get(\"all_tokens\")\n",
    "\n",
    "token_string_df = token_string_df.set_index(\"token_id\")[\"str\"]\n",
    "token_string_df[-1] = \"St@rt\"\n",
    "token_string_df[-2] = \"$nd\"\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "change_object_dataframe.shape\n",
    "\n",
    "\n",
    "change_object_dataframe[\"ins_length\"] = change_object_dataframe[\"ins_tokens\"].str.len()\n",
    "change_object_dataframe[\"del_length\"] = change_object_dataframe[\"del_tokens\"].str.len()\n",
    "\n",
    "change_object_dataframe[\"del_string_tokens\"] = change_object_dataframe[\"del_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"ins_string_tokens\"] = change_object_dataframe[\"ins_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "change_object_dataframe[\"edit_string_tokens\"] = change_object_dataframe[\"ins_string_tokens\"] + change_object_dataframe[\"del_string_tokens\"]\n",
    "\n",
    "\n",
    "\n",
    "change_object_dataframe[\"left_context\"] = change_object_dataframe[\"left_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "\n",
    "change_object_dataframe[\"right_context\"] = change_object_dataframe[\"right_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "change_object_dataframe[\"bykau_cluster\"] = pd.Series(-99,index=change_object_dataframe.index)\n",
    "\n",
    "# change_object_dataframe = change_object_dataframe[[\"left_context\",\"del_string_tokens\",\"ins_string_tokens\", \"right_context\", \n",
    "#                                                    \"ins_length\", \"del_length\", \"bykau_cluster\" ]]\n",
    "change_object_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_and_del = change_object_dataframe[(change_object_dataframe[\"ins_string_tokens\"]!=()) & (change_object_dataframe[\"del_string_tokens\"]!=())]\n",
    "display(ins_and_del.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing change object with insert or delete token size more than five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ins_and_del = ins_and_del[~((ins_and_del[\"ins_length\"] >5 ) | (ins_and_del[\"del_length\"] >5) )]\n",
    "reduced_ins_and_del.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_ins_and_del.reset_index(drop=True)[[\"ins_string_tokens\", \"del_string_tokens\"]].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low user support tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_change_object = reduced_ins_and_del.groupby(\"ins_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object = bykau_change_object.groupby(\"del_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = article_name + \"_FULL.csv\"\n",
    "annotation_dir = \"../data/annotation/\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "annotation_df = pd.read_csv(full_file_path)\n",
    "annotation_df = annotation_df[[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\", \"nationality\", \"birth_place\",\"Bulk\" ]]\n",
    "true_labels = np.zeros((annotation_df.shape[0]))\n",
    "true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "annotation_df[\"nationality\"] = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = article_name + \"_FULL.csv\"\n",
    "# annotation_dir = \"../data/annotation/\"\n",
    "# full_file_path = os.path.join(annotation_dir, file_name)\n",
    "# annotation_df = pd.read_csv(full_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining jaccard similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_distances(dataframe, context = 8):\n",
    "    left_neighbours = dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-context:])\n",
    "    right_neighbours = dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:context])\n",
    "    \n",
    "    neighbour_tokens = left_neighbours + right_neighbours\n",
    "    neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "    \n",
    "    neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "    return pairwise_distances(neighbour_vec, metric=\"jaccard\")\n",
    "#     db = DBSCAN(eps=eps, min_samples=min_samples, metric='jaccard').fit(neighbour_vec)\n",
    "#     return db.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_cluster(distances, eps=0.75, min_samples=5):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit(distances)\n",
    "    return db.labels_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# r_threshold = 8\n",
    "# cutoff_threshold = 0.75\n",
    "# # edit_tokens = change_object_dataframe[\"ins_tokens\"] + change_object_dataframe[\"del_tokens\"]\n",
    "# left_neighbours = change_object_dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-r_threshold:])\n",
    "# right_neighbours = change_object_dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:r_threshold])\n",
    "# neighbour_tokens = left_neighbours + right_neighbours\n",
    "\n",
    "# # bykau_change_object[\"edit_tokens\"] = edit_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "\n",
    "# db = DBSCAN(eps=cutoff_threshold, min_samples=5, metric='jaccard').fit(neighbour_vec)\n",
    "# # change_object_dataframe.loc[change_object_dataframe.index,\"bykau_cluster\"] = db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving change object and its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bykau_dir =  \"../data/bykau_change_object/\"\n",
    "# filename =  f\"{article_name}_without_optimization.h5\"\n",
    "\n",
    "# change_object_file = os.path.join(bykau_dir, filename)\n",
    "# with pd.HDFStore(change_object_file, 'w') as store:\n",
    "#     store.put(\"data\", change_object_dataframe[\"bykau_cluster\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment for seeing the content of each cluster in an ipywidget User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repers_first =  change_object_dataframe.groupby(\"bykau_cluster\")[[\"left_context\",\"del_string_tokens\", \"ins_string_tokens\", \"right_context\"]].apply(lambda x: x.style.render())\n",
    "# @interact( clusters_html=fixed(repers_first), group=change_object_dataframe[\"bykau_cluster\"].unique().tolist())\n",
    "# def display_clusters(clusters_html, group):\n",
    "#      return display(HTML(clusters_html.loc[group]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making annotation file with cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_in_gap(ann, gap):\n",
    "#     context_gap = gap.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "#     clusters = context_gap.loc[ context_gap[\"token_id\"].apply(\n",
    "#             lambda x: ann[\"token_id\"] in x), [\"bykau_cluster\"]].values\n",
    "#     if clusters.size >0:\n",
    "#             clusters = pd.Series(clusters[0],index=[\"bykau_cluster\",])\n",
    "#     else:\n",
    "#         clusters = pd.Series([-10], index=[\"bykau_cluster\",])\n",
    "#     return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap_df, gap_cluster_df):\n",
    "    context_gap = gap_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    context_cluster = gap_cluster_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    clusters = context_cluster.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x),:].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=gap_cluster_df.columns)\n",
    "    else:\n",
    "        clusters = pd.Series(-10, index=gap_cluster_df.columns)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bykau(change_object_dataframe, annotation_df):\n",
    "    ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id', \"bykau_cluster\"]].values\n",
    "\n",
    "    # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "    del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "    gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "    gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "    gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "    \n",
    "    annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "    nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "    nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "    annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "    evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "    evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "    evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "    return evaluation_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = 2 \n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "\n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "\n",
    "bykau_cluster_df =  pd.DataFrame(columns=idx, index= bykau_change_object.index) #pd.DataFrame(columns=idx)\n",
    "\n",
    "bykau_evaluation_df = pd.DataFrame(index=idx, columns=[\"rand\", \"entropy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context in context_array:\n",
    "    distances = bykau_distances(bykau_change_object, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_cluster_df.loc[bykau_change_object.index, (context,eps)] = pd.Series(clusters, index=bykau_change_object.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = bykau_change_object.reset_index().loc[\n",
    "    bykau_change_object[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_cluster_df.loc[\n",
    "    bykau_change_object[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = bykau_change_object.reset_index().loc[\n",
    "    bykau_change_object[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_cluster_df.loc[\n",
    "    bykau_change_object[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "annotation_clusters.head()\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = annotation_df.loc[al_combination_clusters_df.mean(axis=1) > -10]\n",
    "b = al_combination_clusters_df.loc[al_combination_clusters_df.mean(axis=1) > -10]\n",
    "\n",
    "annotation_clusters = pd.concat([a,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gap_df.copy()\n",
    "df2=df2.reset_index()\n",
    "df2['nationality'] = 0\n",
    "\n",
    "aci = annotation_clusters.set_index(['revid_ctxt', 'rev_id']).sort_index()\n",
    "aci = aci[aci['Bulk'] =='N']\n",
    "aci_y=aci[aci['nationality'] == 1]\n",
    "aci_n=aci[aci['nationality'] == 0]\n",
    "\n",
    "counter = 0\n",
    "def nat_val(row):\n",
    "    global counter\n",
    "    val = 0\n",
    "    x=0\n",
    "    y=0\n",
    "    try:\n",
    "        x = int(aci_y.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val + (1 if x > 0 else 0)\n",
    "        \n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    try:\n",
    "        y = int(aci_n.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val - (1 if y > 0 else 0)\n",
    "        counter += y\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "#     if (1 if x > 0 else 0) + (1 if y > 0 else 0) > 1:\n",
    "#         try:\n",
    "#             if not (aci.loc[(row[0],row[1]),['Bulk']] == 'Y').all()[0]:\n",
    "#                 print((aci.loc[(row[0],row[1]),['token_id','nationality','Bulk']])[aci.loc[(row[0],row[1]), 'token_id'].isin(row[2])])\n",
    "#                 print(aci.loc[(row[0],row[1]), ['token_id']].isin(row[2]))\n",
    "#         except:\n",
    "#             import pdb; pdb.set_trace()\n",
    "    return val \n",
    "    \n",
    "df2['nationality'] = df2[['revid_ctxt', 'rev_id', 'token_id']].apply(nat_val, axis=1)\n",
    "df3 = pd.concat([df2, gap_cluster_df.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "print(\"Without bulks\")\n",
    "for context, eps in all_combinations:\n",
    "    print(str((context, eps)) + \": \" + str(weighted_entropy(df3, entropy_column=\"nationality\", group_columns=(context, eps))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = annotation_clusters[annotation_clusters['Bulk']=='N']\n",
    "for context, eps in all_combinations:\n",
    "    bykau_evaluation_df.loc[(context, eps),\"entropy\"] = weighted_entropy(df4, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_clusters['Bulk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps in all_combinations:\n",
    "    bykau_evaluation_df.loc[(context, eps),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "#     bykau_evaluation_df.loc[(context, eps),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "#                                                                                                      eps)], \n",
    "#                                                                                 true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = [2] \n",
    "# all_combinations = list(itertools.product(context_array, eps_array))\n",
    "all_combinations_without_optimization = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx_without_optimization  = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "# idx_without_optimization = pd.MultiIndex.from_product([context_array, eps_array, min_samples_array],\n",
    "#                                 names=[\"context\",\"eps\", \"min_samples\"])\n",
    "bykau_evaluation_without_optimization = pd.DataFrame(index=idx_without_optimization, \n",
    "                                                     columns=[\"rand\", \"entropy\"])\n",
    "\n",
    "\n",
    "\n",
    "bykau_without_optimization_cluster_df = pd.DataFrame(columns=idx_without_optimization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 12.6 s, total: 2min 7s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for context in context_array:\n",
    "    distances = bykau_distances(change_object_dataframe, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_without_optimization_cluster_df[context,eps] = pd.Series(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# true_labels = np.zeros((annotation_df.shape[0]))\n",
    "# true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "# annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the entropy of the Annotated change objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = gap_df.copy()\n",
    "df2=df2.reset_index()\n",
    "df2['nationality'] = 0\n",
    "\n",
    "aci = annotation_clusters.set_index(['revid_ctxt', 'rev_id']).sort_index()\n",
    "aci = aci[aci['Bulk'] =='N']\n",
    "aci_y=aci[aci['nationality'] == 1]\n",
    "aci_n=aci[aci['nationality'] == 0]\n",
    "\n",
    "counter = 0\n",
    "def nat_val(row):\n",
    "    global counter\n",
    "    val = 0\n",
    "    x=0\n",
    "    y=0\n",
    "    try:\n",
    "        x = int(aci_y.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val + (1 if x > 0 else 0)\n",
    "        \n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    try:\n",
    "        y = int(aci_n.loc[(row[0],row[1]), ['token_id']].isin(row[2]).sum())\n",
    "        val = val - (1 if y > 0 else 0)\n",
    "        counter += y\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "#     if (1 if x > 0 else 0) + (1 if y > 0 else 0) > 1:\n",
    "#         try:\n",
    "#             if not (aci.loc[(row[0],row[1]),['Bulk']] == 'Y').all()[0]:\n",
    "#                 print((aci.loc[(row[0],row[1]),['token_id','nationality','Bulk']])[aci.loc[(row[0],row[1]), 'token_id'].isin(row[2])])\n",
    "#                 print(aci.loc[(row[0],row[1]), ['token_id']].isin(row[2]))\n",
    "#         except:\n",
    "#             import pdb; pdb.set_trace()\n",
    "    return val \n",
    "    \n",
    "df2['nationality'] = df2[['revid_ctxt', 'rev_id', 'token_id']].apply(nat_val, axis=1)\n",
    "df3 = pd.concat([df2, gap_cluster_df.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without bulks\n",
      "(2, 0.001): 0.9687533115695315\n",
      "(2, 0.025): 0.9687533115695315\n",
      "(2, 0.05): 0.9687533115695315\n",
      "(2, 0.1): 0.9687533115695315\n",
      "(2, 0.2): 0.9687533115695315\n",
      "(2, 0.4): 3.9587710425096807\n",
      "(2, 0.8): 156.75197195239275\n",
      "(4, 0.001): 1.220476592052727\n",
      "(4, 0.025): 1.220476592052727\n",
      "(4, 0.05): 1.220476592052727\n",
      "(4, 0.1): 1.220476592052727\n",
      "(4, 0.2): 1.1617986579589623\n",
      "(4, 0.4): 2.532822003972604\n",
      "(4, 0.8): 445.4838719899523\n",
      "(8, 0.001): 1.4693867125070161\n",
      "(8, 0.025): 1.4693867125070161\n",
      "(8, 0.05): 1.4693867125070161\n",
      "(8, 0.1): 1.2946428525961646\n",
      "(8, 0.2): 0.9752636186278807\n",
      "(8, 0.4): 2.7238185125280636\n",
      "(8, 0.8): 534.6634030369431\n",
      "(15, 0.001): 1.5796493907206288\n",
      "(15, 0.025): 1.5796493907206288\n",
      "(15, 0.05): 1.414065576153409\n",
      "(15, 0.1): 1.05857028142598\n",
      "(15, 0.2): 1.092392932008907\n",
      "(15, 0.4): 4.296414081718749\n",
      "(15, 0.8): 668.5617760943499\n",
      "(30, 0.001): 1.6106862502595019\n",
      "(30, 0.025): 1.433799901850248\n",
      "(30, 0.05): 1.13415447609199\n",
      "(30, 0.1): 1.0388522931176771\n",
      "(30, 0.2): 1.7923024966010208\n",
      "(30, 0.4): 10.199653956820198\n",
      "(30, 0.8): 2675.072779602147\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "print(\"Without bulks\")\n",
    "for context, eps in all_combinations_without_optimization:\n",
    "    print(str((context, eps)) + \": \" + str(weighted_entropy(df3, entropy_column=\"nationality\", group_columns=(context, eps))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the entropy of the Annotated cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = annotation_df.loc[al_combination_clusters_df.mean(axis=1) > -10]\n",
    "b = al_combination_clusters_df.loc[al_combination_clusters_df.mean(axis=1) > -10]\n",
    "\n",
    "annotation_clusters = pd.concat([a,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "2        0.001     1.1264\n",
       "         0.025     1.1264\n",
       "         0.050     1.1264\n",
       "         0.100     1.1264\n",
       "         0.200     1.1264\n",
       "30       0.100    1.13915\n",
       "15       0.100    1.22499\n",
       "         0.200     1.3654\n",
       "4        0.100    1.36815\n",
       "         0.025    1.36815\n",
       "         0.001    1.36815\n",
       "         0.050    1.36815\n",
       "30       0.050    1.41643\n",
       "4        0.200    1.49722\n",
       "8        0.200    1.49789\n",
       "30       0.025    1.69439\n",
       "         0.001    1.71708\n",
       "15       0.050    1.81844\n",
       "8        0.100    1.82507\n",
       "         0.050    1.91669\n",
       "         0.025    1.91669\n",
       "         0.001    1.91669\n",
       "30       0.200    1.93159\n",
       "15       0.001     1.9481\n",
       "         0.025     1.9481\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = annotation_clusters[annotation_clusters['Bulk']=='N']\n",
    "for context, eps in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"entropy\"] = weighted_entropy(df4, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"rand\"] = adjusted_rand_score(df4[(context, \n",
    "                                                                                                     eps)], \n",
    "                                                                                df4['nationality'])\n",
    "\n",
    "bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **@Aadil**, the rankindex here is wrong because all the nationality clusters hould be mark as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <th>eps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">2</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.0344953</td>\n",
       "      <td>1.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.0344953</td>\n",
       "      <td>1.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0344953</td>\n",
       "      <td>1.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0344953</td>\n",
       "      <td>1.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0344953</td>\n",
       "      <td>1.1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>0.0660082</td>\n",
       "      <td>15.8781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>0</td>\n",
       "      <td>509.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">4</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.0233593</td>\n",
       "      <td>1.36815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.0233593</td>\n",
       "      <td>1.36815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0233593</td>\n",
       "      <td>1.36815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0233593</td>\n",
       "      <td>1.36815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0293159</td>\n",
       "      <td>1.49722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>0.127709</td>\n",
       "      <td>7.89078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>0</td>\n",
       "      <td>509.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">8</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.0155554</td>\n",
       "      <td>1.91669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.0155554</td>\n",
       "      <td>1.91669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0155554</td>\n",
       "      <td>1.91669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0168361</td>\n",
       "      <td>1.82507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0448379</td>\n",
       "      <td>1.49789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>0.0843053</td>\n",
       "      <td>6.29846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>0</td>\n",
       "      <td>509.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">15</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.00679509</td>\n",
       "      <td>1.9481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.00679509</td>\n",
       "      <td>1.9481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0130613</td>\n",
       "      <td>1.81844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0343277</td>\n",
       "      <td>1.22499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0627298</td>\n",
       "      <td>1.3654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>0.0199065</td>\n",
       "      <td>9.90127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>0</td>\n",
       "      <td>509.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">30</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.00629528</td>\n",
       "      <td>1.71708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.0102674</td>\n",
       "      <td>1.69439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.0289889</td>\n",
       "      <td>1.41643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.0530047</td>\n",
       "      <td>1.13915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.0592763</td>\n",
       "      <td>1.93159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.400</th>\n",
       "      <td>0.0214034</td>\n",
       "      <td>17.5753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.800</th>\n",
       "      <td>0</td>\n",
       "      <td>509.252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rand  entropy\n",
       "context eps                       \n",
       "2       0.001   0.0344953   1.1264\n",
       "        0.025   0.0344953   1.1264\n",
       "        0.050   0.0344953   1.1264\n",
       "        0.100   0.0344953   1.1264\n",
       "        0.200   0.0344953   1.1264\n",
       "        0.400   0.0660082  15.8781\n",
       "        0.800           0  509.252\n",
       "4       0.001   0.0233593  1.36815\n",
       "        0.025   0.0233593  1.36815\n",
       "        0.050   0.0233593  1.36815\n",
       "        0.100   0.0233593  1.36815\n",
       "        0.200   0.0293159  1.49722\n",
       "        0.400    0.127709  7.89078\n",
       "        0.800           0  509.252\n",
       "8       0.001   0.0155554  1.91669\n",
       "        0.025   0.0155554  1.91669\n",
       "        0.050   0.0155554  1.91669\n",
       "        0.100   0.0168361  1.82507\n",
       "        0.200   0.0448379  1.49789\n",
       "        0.400   0.0843053  6.29846\n",
       "        0.800           0  509.252\n",
       "15      0.001  0.00679509   1.9481\n",
       "        0.025  0.00679509   1.9481\n",
       "        0.050   0.0130613  1.81844\n",
       "        0.100   0.0343277  1.22499\n",
       "        0.200   0.0627298   1.3654\n",
       "        0.400   0.0199065  9.90127\n",
       "        0.800           0  509.252\n",
       "30      0.001  0.00629528  1.71708\n",
       "        0.025   0.0102674  1.69439\n",
       "        0.050   0.0289889  1.41643\n",
       "        0.100   0.0530047  1.13915\n",
       "        0.200   0.0592763  1.93159\n",
       "        0.400   0.0214034  17.5753\n",
       "        0.800           0  509.252"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for context, eps, min_samples in all_combinations_without_optimization:\n",
    "#     print(f\"processing eps {eps}, min samples {min_samples} and context {context}\")\n",
    "#     change_object_dataframe.loc[change_object_dataframe.index,\n",
    "#                             \"bykau_cluster\"] = bykau_cluster(change_object_dataframe, \n",
    "#                                                              context=int(context), eps=eps, min_samples=min_samples)\n",
    "#     bykau_evaluation_without_optimization.loc[context, eps, \n",
    "#                             min_samples] =evaluate_bykau(change_object_dataframe, annotation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:200]\n",
    "\n",
    "bykau_evaluation_without_optimization.reset_index().set_index([\"eps\", \"context\"])[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insert array is always done in to revision so taking it and leaving other change object where \n",
    "# ins_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "#                   [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "\n",
    "# # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "# del_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "#                   [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "# gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "# gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "# gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "# annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "\n",
    "# nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "# evaluation_score\n",
    "\n",
    "# both_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# both_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# both_cluster[annotation_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "# annotation_df[\"both_cluster\"] = both_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\", ])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], both_cluster)\n",
    "\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"both_cluster\")\n",
    "\n",
    "# evaluation_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_cluster_df.index = change_object_dataframe.index\n",
    "\n",
    "bykau_optimised = pd.concat([change_object_dataframe, bykau_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_series_with_optimization = pd.Series(index=all_combinations)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_df.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(bykau_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "#     entropy_series_with_optimization[context_eps_tuple]= weighted_token_entropy(bykau_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_with_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_without_optimization_cluster_df.index = change_object_dataframe.index\n",
    "\n",
    "without_optimised = pd.concat([change_object_dataframe, bykau_without_optimization_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization = pd.Series(index=all_combinations_without_optimization)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(without_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "\n",
    "#     entropy_series_without_optimization[context_eps_tuple]= weighted_token_entropy(without_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_without_optimization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df[\"token_entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_without_optimization[\"token_entropy\"].sort_values().iloc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df.reset_index().set_index([\"eps\", \"context\"])[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_df.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_without_optimization.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
