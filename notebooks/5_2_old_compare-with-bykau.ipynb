{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from scipy import stats\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.stats import entropy\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import pairwise_distances  \n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to take weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(dataframe, entropy_column, group_columns=\"cluster\", ):\n",
    "    group_size = dataframe.groupby(group_columns).size()\n",
    "    group_entropy = dataframe.groupby(group_columns)[entropy_column].apply(lambda x: entropy(x.value_counts().values))\n",
    "    weighted_entropy = (group_size * group_entropy).mean()\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## reading the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ins_start_pos</th>\n",
       "      <th>ins_end_pos</th>\n",
       "      <th>left_neigh</th>\n",
       "      <th>right_neigh</th>\n",
       "      <th>del_start_pos</th>\n",
       "      <th>del_end_pos</th>\n",
       "      <th>ins_tokens</th>\n",
       "      <th>del_tokens</th>\n",
       "      <th>left_neigh_slice</th>\n",
       "      <th>right_neigh_slice</th>\n",
       "      <th>left_token</th>\n",
       "      <th>right_token</th>\n",
       "      <th>ins_length</th>\n",
       "      <th>del_length</th>\n",
       "      <th>del_string_tokens</th>\n",
       "      <th>ins_string_tokens</th>\n",
       "      <th>edit_string_tokens</th>\n",
       "      <th>left_context</th>\n",
       "      <th>right_context</th>\n",
       "      <th>bykau_cluster</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from revision id</th>\n",
       "      <th>to revision id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timegap</th>\n",
       "      <th>editor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">203693</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">203699</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2002-09-08 14:05:32</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">194 days 22:14:17</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3646</th>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 10, None)</td>\n",
       "      <td>slice(10, 41, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)</td>\n",
       "      <td>(9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' '</td>\n",
       "      <td>of scotland ( [[ university of glasgow ]] ) wa...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(62,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 11, None)</td>\n",
       "      <td>slice(11, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</td>\n",
       "      <td>(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>([[,)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of</td>\n",
       "      <td>scotland ( [[ university of glasgow ]] ) was t...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(63,)</td>\n",
       "      <td>()</td>\n",
       "      <td>slice(0, 12, None)</td>\n",
       "      <td>slice(12, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</td>\n",
       "      <td>(11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>()</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>(]],)</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland</td>\n",
       "      <td>( [[ university of glasgow ]] ) was the first ...</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>(64, 65, 66, 67, 68, 69)</td>\n",
       "      <td>(26, 27, 28, 29, 30)</td>\n",
       "      <td>slice(0, 27, None)</td>\n",
       "      <td>slice(32, 42, None)</td>\n",
       "      <td>(-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...</td>\n",
       "      <td>(31, 32, 33, 34, 35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>((, which, see, ), in)</td>\n",
       "      <td>(,, a, device, he, presented, to)</td>\n",
       "      <td>(,, a, device, he, presented, to, (, which, se...</td>\n",
       "      <td>St@rt ' ' ' john logie baird ' ' ' of scotland...</td>\n",
       "      <td>the mid 1920s ( [[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>(70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...</td>\n",
       "      <td>(32, 33, 34)</td>\n",
       "      <td>slice(2, 33, None)</td>\n",
       "      <td>slice(36, 42, None)</td>\n",
       "      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>(35, 36, 37, 38, 39, 40)</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>(mid, 1920s, ()</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>([[, royal, institute, ]], and, a, reporter, f...</td>\n",
       "      <td>' ' john logie baird ' ' ' of scotland ( [[ un...</td>\n",
       "      <td>[[ 1926 ]] ? ) .</td>\n",
       "      <td>-99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                ins_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10.0   \n",
       "                                                                             1           32.0   \n",
       "                                                                             2           34.0   \n",
       "                                                                             3           50.0   \n",
       "                                                                             4           57.0   \n",
       "\n",
       "                                                                                ins_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         30.0   \n",
       "                                                                             1         32.0   \n",
       "                                                                             2         34.0   \n",
       "                                                                             3         55.0   \n",
       "                                                                             4         73.0   \n",
       "\n",
       "                                                                                left_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           9   \n",
       "                                                                             1          10   \n",
       "                                                                             2          11   \n",
       "                                                                             3          26   \n",
       "                                                                             4          32   \n",
       "\n",
       "                                                                                right_neigh  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           10   \n",
       "                                                                             1           11   \n",
       "                                                                             2           12   \n",
       "                                                                             3           32   \n",
       "                                                                             4           36   \n",
       "\n",
       "                                                                                del_start_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                    \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           -1.0   \n",
       "                                                                             1           -1.0   \n",
       "                                                                             2           -1.0   \n",
       "                                                                             3           27.0   \n",
       "                                                                             4           33.0   \n",
       "\n",
       "                                                                                del_end_pos  \\\n",
       "from revision id to revision id timestamp           timegap           editor                  \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0         -1.0   \n",
       "                                                                             1         -1.0   \n",
       "                                                                             2         -1.0   \n",
       "                                                                             3         31.0   \n",
       "                                                                             4         35.0   \n",
       "\n",
       "                                                                                                                       ins_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 5...   \n",
       "                                                                             1                                              (62,)   \n",
       "                                                                             2                                              (63,)   \n",
       "                                                                             3                           (64, 65, 66, 67, 68, 69)   \n",
       "                                                                             4  (70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 8...   \n",
       "\n",
       "                                                                                          del_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                           \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    ()   \n",
       "                                                                             1                    ()   \n",
       "                                                                             2                    ()   \n",
       "                                                                             3  (26, 27, 28, 29, 30)   \n",
       "                                                                             4          (32, 33, 34)   \n",
       "\n",
       "                                                                                  left_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                         \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(0, 10, None)   \n",
       "                                                                             1  slice(0, 11, None)   \n",
       "                                                                             2  slice(0, 12, None)   \n",
       "                                                                             3  slice(0, 27, None)   \n",
       "                                                                             4  slice(2, 33, None)   \n",
       "\n",
       "                                                                                  right_neigh_slice  \\\n",
       "from revision id to revision id timestamp           timegap           editor                          \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  slice(10, 41, None)   \n",
       "                                                                             1  slice(11, 42, None)   \n",
       "                                                                             2  slice(12, 42, None)   \n",
       "                                                                             3  slice(32, 42, None)   \n",
       "                                                                             4  slice(36, 42, None)   \n",
       "\n",
       "                                                                                                                       left_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                    (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8)   \n",
       "                                                                             1                 (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)   \n",
       "                                                                             2             (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)   \n",
       "                                                                             3  (-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,...   \n",
       "                                                                             4  (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                                                                                                      right_token  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  (9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20...   \n",
       "                                                                             1  (10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2...   \n",
       "                                                                             2  (11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...   \n",
       "                                                                             3           (31, 32, 33, 34, 35, 36, 37, 38, 39, 40)   \n",
       "                                                                             4                           (35, 36, 37, 38, 39, 40)   \n",
       "\n",
       "                                                                                ins_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0          21   \n",
       "                                                                             1           1   \n",
       "                                                                             2           1   \n",
       "                                                                             3           6   \n",
       "                                                                             4          17   \n",
       "\n",
       "                                                                                del_length  \\\n",
       "from revision id to revision id timestamp           timegap           editor                 \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0           0   \n",
       "                                                                             1           0   \n",
       "                                                                             2           0   \n",
       "                                                                             3           5   \n",
       "                                                                             4           3   \n",
       "\n",
       "                                                                                     del_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                             \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                      ()   \n",
       "                                                                             1                      ()   \n",
       "                                                                             2                      ()   \n",
       "                                                                             3  ((, which, see, ), in)   \n",
       "                                                                             4         (mid, 1920s, ()   \n",
       "\n",
       "                                                                                                                ins_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3                  (,, a, device, he, presented, to)   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                               edit_string_tokens  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  ((, b, ., [[, august, 14, ]], [[, 1888, ]], ,,...   \n",
       "                                                                             1                                              ([[,)   \n",
       "                                                                             2                                              (]],)   \n",
       "                                                                             3  (,, a, device, he, presented, to, (, which, se...   \n",
       "                                                                             4  ([[, royal, institute, ]], and, a, reporter, f...   \n",
       "\n",
       "                                                                                                                     left_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0                 St@rt ' ' ' john logie baird ' ' '   \n",
       "                                                                             1              St@rt ' ' ' john logie baird ' ' ' of   \n",
       "                                                                             2     St@rt ' ' ' john logie baird ' ' ' of scotland   \n",
       "                                                                             3  St@rt ' ' ' john logie baird ' ' ' of scotland...   \n",
       "                                                                             4  ' ' john logie baird ' ' ' of scotland ( [[ un...   \n",
       "\n",
       "                                                                                                                    right_context  \\\n",
       "from revision id to revision id timestamp           timegap           editor                                                        \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0  of scotland ( [[ university of glasgow ]] ) wa...   \n",
       "                                                                             1  scotland ( [[ university of glasgow ]] ) was t...   \n",
       "                                                                             2  ( [[ university of glasgow ]] ) was the first ...   \n",
       "                                                                             3                   the mid 1920s ( [[ 1926 ]] ? ) .   \n",
       "                                                                             4                                   [[ 1926 ]] ? ) .   \n",
       "\n",
       "                                                                                bykau_cluster  \n",
       "from revision id to revision id timestamp           timegap           editor                   \n",
       "203693           203699         2002-09-08 14:05:32 194 days 22:14:17 3646   0            -99  \n",
       "                                                                             1            -99  \n",
       "                                                                             2            -99  \n",
       "                                                                             3            -99  \n",
       "                                                                             4            -99  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_name = \"John_Logie_Baird\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "content_dir = \"../data/content/\"\n",
    "filename =  f\"{article_name}_change.h5\"\n",
    "change_object_file = os.path.join(change_object_dir, filename)\n",
    "filename = article_name + \".h5\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    token_string_df = store.get(\"all_tokens\")\n",
    "\n",
    "token_string_df = token_string_df.set_index(\"token_id\")[\"str\"]\n",
    "token_string_df[-1] = \"St@rt\"\n",
    "token_string_df[-2] = \"$nd\"\n",
    "\n",
    "\n",
    "if os.path.exists(change_object_file):\n",
    "    with pd.HDFStore(change_object_file, 'r') as store:\n",
    "        change_object_dataframe = store.get(\"data\")\n",
    "else:\n",
    "    print(\"file do not exist\")\n",
    "change_object_dataframe.shape\n",
    "\n",
    "\n",
    "change_object_dataframe[\"ins_length\"] = change_object_dataframe[\"ins_tokens\"].str.len()\n",
    "change_object_dataframe[\"del_length\"] = change_object_dataframe[\"del_tokens\"].str.len()\n",
    "\n",
    "change_object_dataframe[\"del_string_tokens\"] = change_object_dataframe[\"del_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "\n",
    "change_object_dataframe[\"ins_string_tokens\"] = change_object_dataframe[\"ins_tokens\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist()))\n",
    "change_object_dataframe[\"edit_string_tokens\"] = change_object_dataframe[\"ins_string_tokens\"] + change_object_dataframe[\"del_string_tokens\"]\n",
    "\n",
    "\n",
    "\n",
    "change_object_dataframe[\"left_context\"] = change_object_dataframe[\"left_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "\n",
    "change_object_dataframe[\"right_context\"] = change_object_dataframe[\"right_token\"].apply(\n",
    "    lambda x:  tuple(token_string_df[np.array(x)].tolist())).str.join(\" \")\n",
    "\n",
    "change_object_dataframe[\"bykau_cluster\"] = pd.Series(-99,index=change_object_dataframe.index)\n",
    "\n",
    "# change_object_dataframe = change_object_dataframe[[\"left_context\",\"del_string_tokens\",\"ins_string_tokens\", \"right_context\", \n",
    "#                                                    \"ins_length\", \"del_length\", \"bykau_cluster\" ]]\n",
    "change_object_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the change object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ins_and_del = change_object_dataframe[(change_object_dataframe[\"ins_string_tokens\"]!=()) & (change_object_dataframe[\"del_string_tokens\"]!=())]\n",
    "display(ins_and_del.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing change object with insert or delete token size more than five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_ins_and_del = ins_and_del[~((ins_and_del[\"ins_length\"] >5 ) | (ins_and_del[\"del_length\"] >5) )]\n",
    "reduced_ins_and_del.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ins_string_tokens</th>\n",
       "      <th>del_string_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>(partly,)</td>\n",
       "      <td>(partially,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>(argylle,)</td>\n",
       "      <td>(argyll,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>(shagedso,)</td>\n",
       "      <td>(james, w)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ins_string_tokens del_string_tokens\n",
       "179         (partly,)      (partially,)\n",
       "874        (argylle,)         (argyll,)\n",
       "555       (shagedso,)        (james, w)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_ins_and_del.reset_index(drop=True)[[\"ins_string_tokens\", \"del_string_tokens\"]].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low user support tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_change_object = reduced_ins_and_del.groupby(\"ins_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object = bykau_change_object.groupby(\"del_string_tokens\").filter(lambda x : x.index.get_level_values(\"editor\").nunique()>=2)\n",
    "bykau_change_object.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = article_name + \"_FULL.csv\"\n",
    "annotation_dir = \"../data/annotation/\"\n",
    "full_file_path = os.path.join(annotation_dir, file_name)\n",
    "annotation_df = pd.read_csv(full_file_path)\n",
    "annotation_df = annotation_df[[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\", \"nationality\", \"birth_place\" ]]\n",
    "true_labels = np.zeros((annotation_df.shape[0]))\n",
    "true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "annotation_df[\"nationality\"] = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = article_name + \"_FULL.csv\"\n",
    "# annotation_dir = \"../data/annotation/\"\n",
    "# full_file_path = os.path.join(annotation_dir, file_name)\n",
    "# annotation_df = pd.read_csv(full_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for weighted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining jaccard similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_distances(dataframe, context = 8):\n",
    "    left_neighbours = dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-context:])\n",
    "    right_neighbours = dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:context])\n",
    "    \n",
    "    neighbour_tokens = left_neighbours + right_neighbours\n",
    "    neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "    \n",
    "    neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "    return pairwise_distances(neighbour_vec, metric=\"jaccard\")\n",
    "#     db = DBSCAN(eps=eps, min_samples=min_samples, metric='jaccard').fit(neighbour_vec)\n",
    "#     return db.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bykau_cluster(distances, eps=0.75, min_samples=5):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed').fit(distances)\n",
    "    return db.labels_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# r_threshold = 8\n",
    "# cutoff_threshold = 0.75\n",
    "# # edit_tokens = change_object_dataframe[\"ins_tokens\"] + change_object_dataframe[\"del_tokens\"]\n",
    "# left_neighbours = change_object_dataframe[\"left_context\"].apply(lambda x: x.split(\" \")[-r_threshold:])\n",
    "# right_neighbours = change_object_dataframe[\"right_context\"].apply(lambda x: x.split(\" \")[:r_threshold])\n",
    "# neighbour_tokens = left_neighbours + right_neighbours\n",
    "\n",
    "# # bykau_change_object[\"edit_tokens\"] = edit_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_tokens_set = neighbour_tokens.apply(lambda x: np.unique(x))\n",
    "# neighbour_vec = MultiLabelBinarizer().fit_transform(neighbour_tokens_set)\n",
    "\n",
    "# db = DBSCAN(eps=cutoff_threshold, min_samples=5, metric='jaccard').fit(neighbour_vec)\n",
    "# # change_object_dataframe.loc[change_object_dataframe.index,\"bykau_cluster\"] = db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving change object and its clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bykau_dir =  \"../data/bykau_change_object/\"\n",
    "# filename =  f\"{article_name}_without_optimization.h5\"\n",
    "\n",
    "# change_object_file = os.path.join(bykau_dir, filename)\n",
    "# with pd.HDFStore(change_object_file, 'w') as store:\n",
    "#     store.put(\"data\", change_object_dataframe[\"bykau_cluster\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment for seeing the content of each cluster in an ipywidget User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repers_first =  change_object_dataframe.groupby(\"bykau_cluster\")[[\"left_context\",\"del_string_tokens\", \"ins_string_tokens\", \"right_context\"]].apply(lambda x: x.style.render())\n",
    "# @interact( clusters_html=fixed(repers_first), group=change_object_dataframe[\"bykau_cluster\"].unique().tolist())\n",
    "# def display_clusters(clusters_html, group):\n",
    "#      return display(HTML(clusters_html.loc[group]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making annotation file with cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_in_gap(ann, gap):\n",
    "#     context_gap = gap.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "#     clusters = context_gap.loc[ context_gap[\"token_id\"].apply(\n",
    "#             lambda x: ann[\"token_id\"] in x), [\"bykau_cluster\"]].values\n",
    "#     if clusters.size >0:\n",
    "#             clusters = pd.Series(clusters[0],index=[\"bykau_cluster\",])\n",
    "#     else:\n",
    "#         clusters = pd.Series([-10], index=[\"bykau_cluster\",])\n",
    "#     return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_in_gap(ann, gap_df, gap_cluster_df):\n",
    "    context_gap = gap_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    context_cluster = gap_cluster_df.loc[ann[['revid_ctxt', 'rev_id']]]\n",
    "    clusters = context_cluster.loc[ context_gap[\"token_id\"].apply(\n",
    "            lambda x: ann[\"token_id\"] in x),:].values\n",
    "    if clusters.size >0:\n",
    "            clusters = pd.Series(clusters[0],index=gap_cluster_df.columns)\n",
    "    else:\n",
    "        clusters = pd.Series(-10, index=gap_cluster_df.columns)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bykau(change_object_dataframe, annotation_df):\n",
    "    ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id', \"bykau_cluster\"]].values\n",
    "\n",
    "    # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "    del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "    gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "    gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "    gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "    \n",
    "    annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "    nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "    nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "    annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "    evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "    evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "    evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "    return evaluation_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = 2 \n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "\n",
    "all_combinations = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "\n",
    "bykau_cluster_df =  pd.DataFrame(-99, columns=idx, index= change_object_dataframe.index) #pd.DataFrame(columns=idx)\n",
    "\n",
    "bykau_evaluation_df = pd.DataFrame(index=idx, columns=[\"rand\", \"entropy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "for context in context_array:\n",
    "    distances = bykau_distances(bykau_change_object, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_cluster_df.loc[bykau_change_object.index, (context,eps)] = pd.Series(clusters, index=bykau_change_object.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps in all_combinations:\n",
    "    bykau_evaluation_df.loc[(context, eps),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "    bykau_evaluation_df.loc[(context, eps),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "15       0.100    16.7485\n",
       "         0.025    16.8837\n",
       "         0.001    16.8837\n",
       "30       0.001    16.9028\n",
       "         0.025    17.3155\n",
       "         0.050    17.6908\n",
       "15       0.050    18.2909\n",
       "8        0.100    18.5302\n",
       "4        0.100    18.5567\n",
       "         0.050    18.5567\n",
       "         0.001    18.5567\n",
       "         0.025    18.5567\n",
       "8        0.001    18.6544\n",
       "         0.025    18.6544\n",
       "         0.050    18.6544\n",
       "4        0.200     19.639\n",
       "30       0.100    20.0447\n",
       "8        0.200     20.752\n",
       "15       0.200    21.3194\n",
       "2        0.001    21.4152\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_array  = np.array([2, 4, 8, 15, 30])\n",
    "eps_array =[0.001, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "\n",
    "min_samples_array = [2] \n",
    "# all_combinations = list(itertools.product(context_array, eps_array))\n",
    "all_combinations_without_optimization = list(itertools.product(context_array, eps_array))\n",
    "\n",
    "idx_without_optimization  = pd.MultiIndex.from_product([context_array, eps_array],\n",
    "                                names=[\"context\",\"eps\"])\n",
    "\n",
    "# idx_without_optimization = pd.MultiIndex.from_product([context_array, eps_array, min_samples_array],\n",
    "#                                 names=[\"context\",\"eps\", \"min_samples\"])\n",
    "bykau_evaluation_without_optimization = pd.DataFrame(index=idx_without_optimization, \n",
    "                                                     columns=[\"rand\", \"entropy\"])\n",
    "\n",
    "\n",
    "\n",
    "bykau_without_optimization_cluster_df = pd.DataFrame(columns=idx_without_optimization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/ubuntu/anaconda3/envs/wrod2vec/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to bool by check_pairwise_arrays.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 1s, sys: 1min 23s, total: 3min 25s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for context in context_array:\n",
    "    distances = bykau_distances(change_object_dataframe, context = context)\n",
    "    for eps in eps_array:\n",
    "        clusters = bykau_cluster(distances, eps=eps, min_samples=2)\n",
    "        \n",
    "        bykau_without_optimization_cluster_df[context,eps] = pd.Series(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert array is always done in to revision so taking it and leaving other change object where \n",
    "ins_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "                  [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "ins_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"ins_start_pos\"].values != -1, :]\n",
    "\n",
    "# delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "del_array = change_object_dataframe.reset_index().loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "                  [\"from revision id\",\"del_tokens\", 'to revision id']].values\n",
    "del_cluster = bykau_without_optimization_cluster_df.loc[\n",
    "    change_object_dataframe[\"del_start_pos\"].values != -1, :]\n",
    "\n",
    "gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "gap_df = pd.DataFrame(gap_array,columns=[\"revid_ctxt\", \"token_id\",\n",
    "                               \"rev_id\"])\n",
    "\n",
    "gap_cluster= pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "gap_cluster_df = pd.concat([ins_cluster, del_cluster], axis=0)\n",
    "\n",
    "gap_cluster_df.index=gap_df.index\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "al_combination_clusters_df = annotation_df.apply(token_in_gap, axis=1, args=(gap_df, gap_cluster_df))\n",
    "\n",
    "annotation_clusters = pd.concat([annotation_df, al_combination_clusters_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# true_labels = np.zeros((annotation_df.shape[0]))\n",
    "# true_labels[(annotation_df[\"nationality\"].str.strip() == \"Y\").values] = 1\n",
    "# annotation_df[\"nationality\"] = true_labels\n",
    "#true_labels[true_lable_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context, eps in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"entropy\"] = weighted_entropy(annotation_clusters, \n",
    "                                                                                entropy_column=\"nationality\", \n",
    "                                                                                group_columns=(context, eps))\n",
    "    bykau_evaluation_without_optimization.loc[(context, eps),\"rand\"] = adjusted_rand_score(annotation_clusters[(context, \n",
    "                                                                                                     eps)], \n",
    "                                                                                true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for context, eps, min_samples in all_combinations_without_optimization:\n",
    "#     print(f\"processing eps {eps}, min samples {min_samples} and context {context}\")\n",
    "#     change_object_dataframe.loc[change_object_dataframe.index,\n",
    "#                             \"bykau_cluster\"] = bykau_cluster(change_object_dataframe, \n",
    "#                                                              context=int(context), eps=eps, min_samples=min_samples)\n",
    "#     bykau_evaluation_without_optimization.loc[context, eps, \n",
    "#                             min_samples] =evaluate_bykau(change_object_dataframe, annotation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eps    context\n",
       "0.100  30         1.67847\n",
       "0.200  15         1.74138\n",
       "0.100  15         1.85413\n",
       "0.001  2          1.88174\n",
       "0.025  2          1.88174\n",
       "0.050  2          1.88174\n",
       "0.100  2          1.88174\n",
       "0.200  2          1.88174\n",
       "       30         1.90516\n",
       "       8          1.93331\n",
       "0.050  30         2.05271\n",
       "0.001  4          2.09995\n",
       "0.025  4          2.09995\n",
       "0.050  4          2.09995\n",
       "0.100  4          2.09995\n",
       "0.200  4          2.15148\n",
       "0.100  8          2.46163\n",
       "0.050  15         2.49224\n",
       "0.025  30         2.55369\n",
       "       8          2.59872\n",
       "0.001  8          2.59872\n",
       "0.050  8          2.59872\n",
       "0.001  30         2.63508\n",
       "       15         2.73362\n",
       "0.025  15         2.73362\n",
       "0.400  8          4.48308\n",
       "       4          5.46712\n",
       "       15         5.86475\n",
       "       30         8.94614\n",
       "       2          9.07203\n",
       "0.800  2          262.712\n",
       "       8          350.612\n",
       "       4          350.709\n",
       "       15         526.082\n",
       "       30         526.082\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:200]\n",
    "\n",
    "bykau_evaluation_without_optimization.reset_index().set_index([\"eps\", \"context\"])[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insert array is always done in to revision so taking it and leaving other change object where \n",
    "# ins_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"ins_start_pos\"].values != -1, \n",
    "#                   [\"to revision id\",\"ins_tokens\", 'to revision id']].values\n",
    "\n",
    "# # delete array is always done in from revision so taking it and leaving other change object where delete does not come.\n",
    "# del_array = change_object_dataframe.reset_index().loc[\n",
    "#     change_object_dataframe[\"del_start_pos\"].values != -1, \n",
    "#                   [\"from revision id\",\"del_tokens\", 'to revision id',\"bykau_cluster\"]].values\n",
    "\n",
    "# gap_array = np.concatenate([ins_array,del_array], axis=0)\n",
    "# gap_df = pd.DataFrame(gap_array,columns= [\"revid_ctxt\", \"token_id\", \"rev_id\",\"bykau_cluster\"])\n",
    "# gap_df = gap_df.set_index(['revid_ctxt', 'rev_id'])\n",
    "\n",
    "# Finding the tokens who were in the gap.\n",
    "# annotation_df[\"bykau_cluster\"] = annotation_df.apply(token_in_gap, axis=1, args=(gap_df,))\n",
    "\n",
    "# nationality_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# nationality_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# annotation_df[\"nationality_cluster\"] = nationality_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\",])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], nationality_cluster)\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"nationality_cluster\")\n",
    "# evaluation_score\n",
    "\n",
    "# both_cluster = np.zeros((annotation_df.shape[0]))\n",
    "# both_cluster[annotation_df[\"nationality\"].str.strip() == \"Y\"] = 1\n",
    "# both_cluster[annotation_df[\"birth_place\"].str.strip() == \"Y\"] = 2\n",
    "# annotation_df[\"both_cluster\"] = both_cluster\n",
    "\n",
    "# evaluation_score = pd.Series(index=[\"rand\", \"entropy\", ])\n",
    "# evaluation_score[\"rand\"] = adjusted_rand_score( annotation_df[\"bykau_cluster\"], both_cluster)\n",
    "\n",
    "# evaluation_score[\"entropy\"] = weighted_entropy(annotation_df, group_columns=\"bykau_cluster\", entropy_column=\"both_cluster\")\n",
    "\n",
    "# evaluation_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_token_entropy(dataframe, group_by):\n",
    "    cluster_sizes = dataframe.groupby(group_by).size()\n",
    "    token_entropy_clusters = dataframe.groupby(group_by)[\"edit_string_tokens\"].apply(\n",
    "                    lambda token_tuples: entropy(pd.Series(\n",
    "                    [token for token_tuple in token_tuples.tolist() for token in token_tuple]\n",
    "                    ).value_counts().values))\n",
    "    cluster_entropy = (cluster_sizes * token_entropy_clusters).sum()\n",
    "    return cluster_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_cluster_df.index = change_object_dataframe.index\n",
    "\n",
    "bykau_optimised = pd.concat([change_object_dataframe, bykau_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_series_with_optimization = pd.Series(index=all_combinations)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_df.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(bykau_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "#     entropy_series_with_optimization[context_eps_tuple]= weighted_token_entropy(bykau_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_with_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_without_optimization_cluster_df.index = change_object_dataframe.index\n",
    "\n",
    "without_optimised = pd.concat([change_object_dataframe, bykau_without_optimization_cluster_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization = pd.Series(index=all_combinations_without_optimization)\n",
    "\n",
    "for context_eps_tuple in all_combinations_without_optimization:\n",
    "    bykau_evaluation_without_optimization.loc[context_eps_tuple,\n",
    "                                              \"token_entropy\"] = weighted_token_entropy(without_optimised, \n",
    "                                                                                        context_eps_tuple)\n",
    "\n",
    "#     entropy_series_without_optimization[context_eps_tuple]= weighted_token_entropy(without_optimised, context_eps_tuple)\n",
    "# all_combinations_without_optimization[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_series_without_optimization.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bykau_evaluation_without_optimization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context</th>\n",
       "      <th>eps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.426345</td>\n",
       "      <td>21.4152</td>\n",
       "      <td>26877.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.025</th>\n",
       "      <td>0.426345</td>\n",
       "      <td>21.4152</td>\n",
       "      <td>26877.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.050</th>\n",
       "      <td>0.426345</td>\n",
       "      <td>21.4152</td>\n",
       "      <td>26877.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.426345</td>\n",
       "      <td>21.4152</td>\n",
       "      <td>26877.174444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.200</th>\n",
       "      <td>0.426345</td>\n",
       "      <td>21.4152</td>\n",
       "      <td>26877.174444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand  entropy  token_entropy\n",
       "context eps                                    \n",
       "2       0.001  0.426345  21.4152   26877.174444\n",
       "        0.025  0.426345  21.4152   26877.174444\n",
       "        0.050  0.426345  21.4152   26877.174444\n",
       "        0.100  0.426345  21.4152   26877.174444\n",
       "        0.200  0.426345  21.4152   26877.174444"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "4        0.400    26874.436158\n",
       "2        0.400    26875.815040\n",
       "         0.001    26877.174444\n",
       "         0.025    26877.174444\n",
       "         0.050    26877.174444\n",
       "         0.100    26877.174444\n",
       "         0.200    26877.174444\n",
       "15       0.200    26910.837172\n",
       "8        0.200    26914.454455\n",
       "         0.400    26924.666833\n",
       "4        0.200    26943.566006\n",
       "         0.100    26967.565965\n",
       "         0.001    26967.565965\n",
       "         0.025    26967.565965\n",
       "         0.050    26967.565965\n",
       "30       0.200    26992.280139\n",
       "15       0.100    27000.216721\n",
       "30       0.100    27001.257637\n",
       "15       0.400    27011.050698\n",
       "8        0.100    27038.588944\n",
       "Name: token_entropy, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df[\"token_entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "4        0.050    18704.316957\n",
       "         0.001    18704.316957\n",
       "8        0.400    18844.693674\n",
       "4        0.400    19039.001054\n",
       "15       0.050    19168.102247\n",
       "30       0.025    19436.504145\n",
       "8        0.001    19487.139834\n",
       "         0.025    19487.139834\n",
       "         0.050    19487.139834\n",
       "15       0.025    19974.770145\n",
       "         0.001    19974.770145\n",
       "30       0.001    20342.635778\n",
       "15       0.400    20994.077577\n",
       "2        0.400    21861.792609\n",
       "30       0.400    24836.714619\n",
       "2        0.800    28873.776904\n",
       "8        0.800    29056.285917\n",
       "4        0.800    29064.499357\n",
       "15       0.800    29074.114543\n",
       "30       0.800    29102.748768\n",
       "Name: token_entropy, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization[\"token_entropy\"].sort_values().iloc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "30       0.100    1.67847\n",
       "15       0.200    1.74138\n",
       "         0.100    1.85413\n",
       "2        0.001    1.88174\n",
       "         0.025    1.88174\n",
       "         0.050    1.88174\n",
       "         0.100    1.88174\n",
       "         0.200    1.88174\n",
       "30       0.200    1.90516\n",
       "8        0.200    1.93331\n",
       "30       0.050    2.05271\n",
       "4        0.001    2.09995\n",
       "         0.025    2.09995\n",
       "         0.050    2.09995\n",
       "         0.100    2.09995\n",
       "         0.200    2.15148\n",
       "8        0.100    2.46163\n",
       "15       0.050    2.49224\n",
       "30       0.025    2.55369\n",
       "8        0.025    2.59872\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context  eps  \n",
       "15       0.100    16.7485\n",
       "         0.025    16.8837\n",
       "         0.001    16.8837\n",
       "30       0.001    16.9028\n",
       "         0.025    17.3155\n",
       "         0.050    17.6908\n",
       "15       0.050    18.2909\n",
       "8        0.100    18.5302\n",
       "4        0.100    18.5567\n",
       "         0.050    18.5567\n",
       "         0.001    18.5567\n",
       "         0.025    18.5567\n",
       "8        0.001    18.6544\n",
       "         0.025    18.6544\n",
       "         0.050    18.6544\n",
       "4        0.200     19.639\n",
       "30       0.100    20.0447\n",
       "8        0.200     20.752\n",
       "15       0.200    21.3194\n",
       "2        0.001    21.4152\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df[\"entropy\"].sort_values().iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eps    context\n",
       "0.100  15         16.7485\n",
       "0.025  15         16.8837\n",
       "0.001  15         16.8837\n",
       "       30         16.9028\n",
       "0.025  30         17.3155\n",
       "0.050  30         17.6908\n",
       "       15         18.2909\n",
       "0.100  8          18.5302\n",
       "       4          18.5567\n",
       "0.050  4          18.5567\n",
       "0.001  4          18.5567\n",
       "0.025  4          18.5567\n",
       "0.001  8          18.6544\n",
       "0.025  8          18.6544\n",
       "0.050  8          18.6544\n",
       "0.200  4           19.639\n",
       "0.100  30         20.0447\n",
       "0.200  8           20.752\n",
       "       15         21.3194\n",
       "0.001  2          21.4152\n",
       "0.200  2          21.4152\n",
       "0.100  2          21.4152\n",
       "0.050  2          21.4152\n",
       "0.025  2          21.4152\n",
       "0.200  30          23.862\n",
       "0.400  15         33.1424\n",
       "       8          34.9366\n",
       "       30         35.5944\n",
       "       4          39.7934\n",
       "       2          51.4184\n",
       "0.800  15         145.998\n",
       "       2          147.451\n",
       "       4          148.002\n",
       "       8          247.479\n",
       "       30         247.479\n",
       "Name: entropy, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df.reset_index().set_index([\"eps\", \"context\"])[\"entropy\"].sort_values().iloc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945280</td>\n",
       "      <td>0.897814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.945280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.889817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_entropy</th>\n",
       "      <td>0.897814</td>\n",
       "      <td>0.889817</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand   entropy  token_entropy\n",
       "rand           1.000000  0.945280       0.897814\n",
       "entropy        0.945280  1.000000       0.889817\n",
       "token_entropy  0.897814  0.889817       1.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_df.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rand</th>\n",
       "      <th>entropy</th>\n",
       "      <th>token_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rand</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.383619</td>\n",
       "      <td>-0.649833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>-0.383619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_entropy</th>\n",
       "      <td>-0.649833</td>\n",
       "      <td>0.882047</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   rand   entropy  token_entropy\n",
       "rand           1.000000 -0.383619      -0.649833\n",
       "entropy       -0.383619  1.000000       0.882047\n",
       "token_entropy -0.649833  0.882047       1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bykau_evaluation_without_optimization.astype(np.float64).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
